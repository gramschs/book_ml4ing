---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.14.7
kernelspec:
  display_name: Python 3.9.12 ('python39')
  language: python
  name: python3
---

# Das Bias-Varianz-Dilemma

Mit der Aufteilung der Daten in Test- und Trainingsdaten k√∂nnen wir untersuchen,
ob ein Modell prinzipiell die Trainingsdaten gut erkl√§rt. Wie gut es neue Daten
prognostizieren kann, d.h. wie gut es verallgemeinert, beurteilen wir mit den
Testdaten. Was ist aber, wenn wir mit dem gew√§hlten Modell nicht zufrieden sind
und mit anderen Modellen vergleichen wollen? Nicht nur die Modellauswahl selbst
ist schwierig. Jedes Modell hat selbst noch Finetuning-Optionen. Die Parameter,
die zu einem Modell geh√∂ren und nichts mit den Daten zu tun haben, werden
**Hyperparameter** genannt. Vielleicht reicht das gew√§hlte Modell ja doch, aber
die Hyperparameter m√ºssen noch optimiert werden. Wie gehen wir da vor?

Die Wahl des Modells ist eine schwierige Kunst und h√§ngt vor allem von der
Qualit√§t der vorliegenden Daten ab. Im Folgenden besch√§ftigen wir uns mit dem
sogenannten **Bias-Varianz-Dilemma**, das bei der Modellauswahl auftritt.

+++

## Lernziele 

```{admonition} Lernziele
:class: important
* Sie wissen, was ein **Hyperparameter** ist.
* Sie k√∂nnen das **Bias-Varianz-Dilemma** erkl√§ren.
* Sie wissen, was **Overfitting** und **Underfitting** bedeutet.
* Sie k√∂nnen erkl√§ren, wie mit Hilfe von **Validierungskurven** ein geeignetes Modell ausgew√§hlt wird. 
```

+++

## Was ist das Bias-Varianz-Dilemma?

Im letzten Abschnitt hatten wir k√ºnstlich generierte Messdaten. Auswendiglernen
ist kein sinnvolles ML-Modell, probieren wir es mit linearer Regression.

```{code-cell} ipython3
import matplotlib.pylab as plt
import numpy as np
from sklearn.linear_model import LinearRegression

# styling of plots
plt.style.use('bmh')

# artificial data: f(x) = ‚àí2ùë•^2 + 8ùë• + 15 + error
X = np.array([-1, 0, 1, 2, 3, 4, 5]).reshape(-1,1)
y = np.array([5.4384, 14.3252, 19.2451, 23.3703, 18.2885, 13.8978, 3.7586])

# preprocessing and training linear regression
model = LinearRegression()
model.fit(X, y)
print('R2-score Trainingsdaten: {:.2f}'.format(model.score(X, y)))

# prediction
X_prediction = np.linspace(-1, 5).reshape(-1,1)
y_prediction = model.predict(X_prediction)

# visualization
fig, ax = plt.subplots()
ax.scatter(X, y, color='k', label='Trainingsdaten')
ax.plot(X_prediction, y_prediction, label='Modell')
ax.set_xlabel('Ursache')
ax.set_ylabel('Wirkung')
ax.set_title('K√ºnstlich generierte Messdaten')
ax.legend(bbox_to_anchor=(1.04, 1), loc='upper left');
```

Schon die Visualisierung zeigt, dass lineare Regression bei diesen Daten keine
gute Idee ist. Der R¬≤-Score ist auch bei 0.

Au√üer der linearen Modellfunktion gibt es ja noch Polynome h√∂heren Grades, also
quadratische Funktion oder kubische Funktionen. Wenn Sie in der Dokumentation
von Scikit-Learn nun nach einer Funktion zur polynomialen Regression suchen,
werden Sie nicht f√ºndig werden. Tats√§chlich brauchen wir auch keine
eigenst√§ndige Funktion, sondern k√∂nnen uns mit einem Trick weiterhelfen. 

Das lineare Regressionsmodell, das wir eben ausprobiert haben, lautet
mathematisch formuliert folgenderma√üen:

$$y^{(i)} = \omega_0 + \omega_1 \cdot x^{(i)}$$

mit nur einem Input $x$.

Wenn wir eine quadratische Funktion als Modellfunktion w√§hlen m√∂chten, erzeugen
wir einfach eine zweite Eigenschaft. Wir nennen die bisherigen x-Werte $x^{(i)}$ jetzt $x_1^{(i)}$ und f√ºgen als zweiten Input die neue Eigenschaft

$$x_2^{(i)} = \left( x_1^{(i)} \right)^2$$

hinzu.  

Dieser Trick wird auch bei anderen ML-Verfahren angewandt. Aus einem Input, aus
einer Eigenschaft werden jetzt eine neue Eigenschaften erzeugt. Aus einem
eindimensionalen Input wird ein zweidimensionaler Input. Mathematisch gesehen
haben wir die Input-Daten in einen h√∂herdimensionalen Bereich projiziert. Diese
Methode nennt man **Kernel-Trick**. Es ist auch m√∂glich, andere Funktionen zu
benutzen, um die Daten in einen h√∂herdimensionalen Raum zu projizieren, z.B.
radiale Gau√üsche Basisfunktionen. Das nennt man dann **Kernel-Methoden**.  

In dieser Vorlesung bleiben wir aber bei den Polynomen als Basisfunktion.
Scikit-Learn stellt auch hier passende Methoden bereit, siehe [Dokumentation
Scikit-Learn ‚Üí
PolynomialFeature](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html).
Wir erzeugen das PolynomialFeature-Objekt mit der Option `degree=2`, um die
Quadrate hinzuzuf√ºgen. Dann transformieren wir die Input-Daten, indem wir die
`fit_transform()`-Methode auf den Input anwenden.
 

```{code-cell} ipython3
from sklearn.preprocessing import PolynomialFeatures

print('Original X:\n', X)

# lade die Polynom-Transformator 
polynom_transformator = PolynomialFeatures(degree = 2)

# transformiere X
X_transformiert =  polynom_transformator.fit_transform(X)
print('transformiertes X:\n', X_transformiert)
```

Aus dem Spaltenvektor macht Scikit-Learn eine Matrix, bei der in der 1. Spalte nur Einsen stehen, die 2. Spalte enth√§lt die urspr√ºnglichen x-Werte und die 3. Spalte nun die Quadrate der urspr√ºnglichen x-Werte.

Damit k√∂nnen wir nun ein multiples lineares Regressionsmodell trainieren, also die
Koeffizienten suchen, so dass 

$$y^{(i)} = \omega_0 + \omega_1 \cdot x_1^{(i)} + \omega_2 \cdot x_2^{(i)}$$ 

mit m√∂glichst kleinen Fehlern gilt. Hier der komplette Code inklusive Visualisierung:

```{code-cell} ipython3
# project data
X2 = PolynomialFeatures(degree = 2).fit_transform(X)

# preprocessing and training linear regression
model = LinearRegression()
model.fit(X2, y)
print('R2-score Trainingsdaten: {:.2f}'.format(model.score(X2, y)))

# prediction
X_prediction = PolynomialFeatures(degree = 2).fit_transform(np.linspace(-1, 5).reshape(-1,1))
y_prediction = model.predict(X_prediction)

# visualization
fig, ax = plt.subplots()
ax.scatter(X2[:,1], y, color='k', label='Trainingsdaten')
ax.plot(X_prediction[:,1], y_prediction, label='Modell')
ax.set_xlabel('Ursache')
ax.set_ylabel('Wirkung')
ax.set_title('K√ºnstlich generierte Messdaten')
ax.legend(bbox_to_anchor=(1.04, 1), loc='upper left');
```

W√§re eine kubische Modellfunktion noch besser? Wir f√ºgen noch eine dritte Eigenschaft hinzu, n√§mlich

$$x_3^{(i)} = \left( x_1^{(i)} \right)^{3}.$$

Nat√ºrlich lassen wir auch hier Scikit-Learn f√ºr uns arbeiten und sezten die Option `degree` diesmal auf den Wert 3.

```{code-cell} ipython3
# project data
X3 = PolynomialFeatures(degree = 3).fit_transform(X)

# preprocessing and training linear regression
model = LinearRegression()
model.fit(X3, y)
print('R2-score Trainingsdaten: {:.2f}'.format(model.score(X3, y)))

# prediction
X_prediction = PolynomialFeatures(degree = 3).fit_transform(np.linspace(-1, 5).reshape(-1,1))
y_prediction = model.predict(X_prediction)

# visualization
fig, ax = plt.subplots()
ax.scatter(X3[:,1], y, color='k', label='Trainingsdaten')
ax.plot(X_prediction[:,1], y_prediction, label='Modell')
ax.set_xlabel('Ursache')
ax.set_ylabel('Wirkung')
ax.set_title('K√ºnstlich generierte Messdaten')
ax.legend(bbox_to_anchor=(1.04, 1), loc='upper left');
```

K√∂nnte tats√§chlich besser sein. Und ist vielleicht ein Polynom 4. Grades noch
besser? Wir betrachten den Polynomgrad als Hyperparameter des Modells
"Regressionspolynom" und gehen einfach mal alle Regressionspolynome bis Grad 7
durch.

```{code-cell} ipython3

model = LinearRegression()

# visualization
fig, ax = plt.subplots()
ax.scatter(X, y, color='k', label='Trainingsdaten')
ax.set_xlabel('Ursache')
ax.set_ylabel('Wirkung')
ax.set_title('K√ºnstlich generierte Messdaten')

for d in range(4,8):
    X_transformiert = PolynomialFeatures(degree = d).fit_transform(X)
    model.fit(X_transformiert, y)
    print('R2-score f√ºr Grad {0}: {1:.12f}'.format(d, model.score(X_transformiert, y)))

    # prediction
    X_prediction = PolynomialFeatures(degree = d).fit_transform(np.linspace(-1, 5, 200).reshape(-1,1))
    y_prediction = model.predict(X_prediction)

    label_string = 'Polynomgrad {}'.format(d)
    ax.plot(X_prediction[:,1], y_prediction, label=label_string)
ax.legend(bbox_to_anchor=(1.04, 1), loc='upper left');

```

Ab Grad 6 kann keine Verbesserung des R¬≤-Scores mehr erzielt werden (warum?).
Auch scheinen die Polynom mit Grad 6 und Grad 7 in den Intervallen $[-1,0]$ und
$[4,5]$ etwas schwankend. Vielleicht m√ºssen wir doch noch h√∂her gehen. Schauen
wir uns Grad 8 an:

```{code-cell} ipython3
# visualization
fig, ax = plt.subplots()
ax.scatter(X, y, color='k', label='Trainingsdaten')
ax.set_xlabel('Ursache')
ax.set_ylabel('Wirkung')
ax.set_title('K√ºnstlich generierte Messdaten')

d = 8
X_transformiert = PolynomialFeatures(degree = d).fit_transform(X)
model.fit(X_transformiert, y)
print('R2-score f√ºr Grad {0}: {1:.6f}'.format(d, model.score(X_transformiert, y)))

# prediction
X_prediction = PolynomialFeatures(degree = d).fit_transform(np.linspace(-1, 5, 200).reshape(-1,1))
y_prediction = model.predict(X_prediction)

label_string = 'Polynomgrad {}'.format(d)
ax.plot(X_prediction[:,1], y_prediction, label=label_string)
ax.legend(bbox_to_anchor=(1.04, 1), loc='upper left');
```

Tats√§chlich werden dadurch die Schwankungen im Intervall $[4,5]$ noch verst√§rkt.
F√ºr Grad 9 wird es noch schlimmer, doch diesmal geht es in die andere Richtung.

```{code-cell} ipython3
# visualization
fig, ax = plt.subplots()
ax.scatter(X, y, color='k', label='Trainingsdaten')
ax.set_xlabel('Ursache')
ax.set_ylabel('Wirkung')
ax.set_title('K√ºnstlich generierte Messdaten')

d = 9
X_transformiert = PolynomialFeatures(degree = d).fit_transform(X)
model.fit(X_transformiert, y)
print('R2-score f√ºr Grad {0}: {1:.6f}'.format(d, model.score(X_transformiert, y)))

# prediction
X_prediction = PolynomialFeatures(degree = d).fit_transform(np.linspace(-1, 5, 200).reshape(-1,1))
y_prediction = model.predict(X_prediction)

label_string = 'Polynomgrad {}'.format(d)
ax.plot(X_prediction[:,1], y_prediction, label=label_string)
ax.legend(bbox_to_anchor=(1.04, 1), loc='upper left');
```

Die Regression mit Polynomen unerschiedlichen Grades verdeutlicht das sogenannte
**Bias-Varianz-Dilemma**. Das lineare Regressionsmodell hat zwei Parameter,
Steigung und y-Achsenabschnitt. Scheinbar sind das zuwenige Modellparameter, um
die Messdaten gut zu erkl√§ren und zu prognostizieren. Wir sind sozusagen mit dem
Vorurteil in unsere Modellierung gestartet, dass diese zwei Modellparameter
ausreichen werden, um die Daten gut zu erkl√§ren. In der ML-Community sagt man,
dass ein gro√üer **Bias** vorliegt. Das Modell ist zu wenig an die Daten
angepasst oder anders ausgedr√ºckt, es liegt eine Unteranpassung an die Daten
vor. Daf√ºr ist auch in der deutschsprachigen Literatur das englische Wort
**Underfitting** sehr gebr√§uchlich.

Bei dem Polynom mit Grad 9 haben wir 10 Modellparameter. Offensichtlich sind
das zuviele Parameter (es liegen ja nur sieben Messwerte vor). Die **Varianz**
ist zu gro√ü. Bereits bei der gefundenen Modellfunktion beobachten wir gro√üe
Schwankungen, insbesondere im Intervall [4,5]. Aber auch durch die Hinzunahme
eines einzigen Datenpunktes w√ºrden wir eine komplett andere Modellfunktion
erhalten. Das Modell reagiert sehr sensibel selbst auf kleine √Ñnderungen. Das
nennen wir √úberanpassung oder **Overfitting**. 

Bei der Modellwahl stehen wir stets vor dem Dilemma, ein Modell mit zu wenigen
Parametern oder eines mit zu vielen Parametern zu w√§hlen. 

+++

## Mit Validierungskurven Modell w√§hlen

Wenn wir in der gl√ºcklichen Situation sind, die Komplexit√§t des Modells w√§hlen
zu k√∂nnen, hilft uns eine Analyse der sogenannten **Validierungskurven** weiter, um
dem Bias-Varianz-Dilemma zu begegnen.

Dazu starten wir bei dem einfachsten Modell (hier in unserem Beispiel mit
Polynomgrad 1) und berechnen den R¬≤-Score sowohl f√ºr die Test- als auch die
Trainingsdaten. Wenn das Modell zu einfach ist, werden beide Scores schlecht
ausfallen. Da wir das Modell mit den Trainingsdaten trainieren, sollte jedoch
der R¬≤-Score f√ºr die Trainingsdaten besser sein als f√ºr die Testdaten.

Nehmen wir mehr Modellparameter hinzu, sollten sich sowohl der Score f√ºr die
Trainingsdaten als auch der Score f√ºr die Testdaten verbessern. Je mehr
Modellparameter hinzukommen, desto besser wird der Score f√ºr die Trainingsdaten,
bis es irgendwann einfach nicht mehr besser geht. Bei unseren k√ºnstlich
generierten Daten haben wir bereits ab Polynomgrad 6 keine Verbesserung mehr
gesehen. Der R¬≤-Score lag ab da bei 1.

Anders hingegen verh√§lt es sich bei dem Score der Testdaten. W√ºrden wir
beispielsweise unser Modell mit Polynomgrad 9 im Intervall $[4,5]$ auswerten, so
w√ºrden negative Werte prognostiziert werden. Da wir hier mit k√ºnstlich erzeugten
Daten arbeiten, wissen wir aber, dass diese mit einer quadratischen Funktion
erzeugt wurden und in diesem Bereich positiv sein m√ºssen. Wir h√§tten daher einen
gro√üen Fehler und einen schlechten R¬≤-Score f√ºr die Testdaten.  

Wenn wir nun den Score (das gilt nicht nur f√ºr den R¬≤-Score, sondern auch f√ºr
andere Bewertungskriterien, die wir noch kennenlernen werden) abh√§ngig von der
Komplexit√§t eines Modells auftragen, nennen wir die entstehende Kurve
**Validierungskurve**. Prinzipiell sehen die Validierungskurve f√ºr die
Trainings- und Testdaten folgenderma√üen aus:

+++

```{figure} pics/validierungskurven_annotated.pdf
---
width: 600px
name: fig_validierungskurven
---
Schematische Darstellung von typischen Validierungskurven der Test- und Trainingsdaten zur Wahl der geeigneten Modellkomplexit√§t
```

+++

## Zusammenfassung 

In diesem Abschnitt haben wir das Bias-Varianz-Dilemma kennengelernt, das vor
allem bei der Auswahl eines geeigneten Modells auftritt. Wir haben auch gesehen,
wie prinzipiell anhand von Validierungskurven bewertet werden kann, ob das
Modell Overfitting oder Underfitting zeigt.
