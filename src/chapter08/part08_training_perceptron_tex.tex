\documentclass[11pt, aspectratio=169, t]{beamer}

% Layout
\usetheme{frankfurt}
\usepackage[no-math]{fontspec}
%\renewcommand{\logofooter}{}


\usepackage{unicode-math}
\usepackage{libertinus}

\useinnertheme{default}
%\useinnertheme{circles}
%\useinnertheme{rectangles}
%\useinnertheme{rounded}
%\useinnertheme{inmargin}

% Sonstige Erweiterungne
\usepackage{booktabs}
\usepackage{nicefrac}

\usepackage{pgf,xcolor}
\definecolor{itwm_blue_04}{HTML}{005A94}
\definecolor{itwm_red}{HTML}{C00000}

\usepackage{tikz}
\usetikzlibrary{shapes.misc, shadows, decorations}
\usetikzlibrary{backgrounds}
\usetikzlibrary{calc}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{fillbetween}
\usepackage{tikzpagenodes}

\usepackage{polyglossia}
\setdefaultlanguage[spelling=new, babelshorthands=true]{german}
\usepackage{csquotes}


\setbeamertemplate{frametitle}[default][center]


\begin{document}

% 1. Iteration
\begin{frame}{Beispiel: Training eines Perzeptrons für das logische ODER}
\begin{small}
\begin{columns}
\begin{column}{0.25\textwidth}
\hspace{0.5cm}
\begin{footnotesize}
\begin{tabular}{cccc} \toprule
$x_0$ & $x_1$ & $x_2$ & y \\ \midrule
1 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
1 & 1 & 1 & 1 \\ \bottomrule
\end{tabular} \end{footnotesize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{tabular}{ll}
aktueller Input: & $\mathbf{x}^{T} = (1, 0, 0)$ \\
aktueller Output: & $y=0$ \\
\end{tabular}
\end{column}
\begin{column}{0.4\textwidth}
aktuelle Gewichte: \alert{ $\boldsymbol{\omega} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}$ }
\end{column}
\end{columns}
prognostizierter Output:
\[\hat{y}^{aktuell} = \Phi(\mathbf{x}^{T}\boldsymbol{\omega}) = \Phi\left( \begin{pmatrix} 1, 0, 0 \end{pmatrix}\cdot \alert{\begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}} \right) = \Phi(0) = 1 \qquad \neq \qquad y = 0\]
Aktualisierung notwendig:
\begin{align*}
\omega_0^{\text{neu}} &= 0 + 1\cdot (0-1)\cdot 1 = -1\\
\omega_1^{\text{neu}} &= 0 + 1\cdot (0-1)\cdot 0 = 0\\
\omega_2^{\text{neu}} &= 0 + 1\cdot (0-1)\cdot 0 = 0
\end{align*}
\end{small}
\end{frame}

% 2. Iteration
\begin{frame}{Beispiel: Training eines Perzeptrons für das logische ODER}
\begin{small}
\begin{columns}
\begin{column}{0.25\textwidth}
\hspace{0.5cm}
\begin{footnotesize}
\begin{tabular}{cccc} \toprule
$x_0$ & $x_1$ & $x_2$ & y \\ \midrule
1 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
1 & 1 & 1 & 1 \\ \bottomrule
\end{tabular} \end{footnotesize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{tabular}{ll}
aktueller Input: & $\mathbf{x}^{T} = (1, 0, 1)$ \\
aktueller Output: & $y=1$ \\
\end{tabular}
\end{column}
\begin{column}{0.4\textwidth}
aktuelle Gewichte: \alert{ $\boldsymbol{\omega} = \begin{pmatrix} -1 \\ 0 \\ 0 \end{pmatrix}$ }
\end{column}
\end{columns}
prognostizierter Output:
\[\hat{y}^{aktuell} = \Phi(\mathbf{x}^{T}\boldsymbol{\omega}) = \Phi\left( \begin{pmatrix} 1, 0, 1 \end{pmatrix}\cdot \alert{\begin{pmatrix} -1 \\ 0 \\ 0 \end{pmatrix}} \right) = \Phi(-1) = 0 \qquad \neq \qquad y = 1\]
Aktualisierung notwendig:
\begin{align*}
\omega_0^{\text{neu}} &= -1 + 1\cdot (1-0)\cdot 1 = 0 \\
\omega_1^{\text{neu}} &=  0 + 1\cdot (1-0)\cdot 0 = 0 \\
\omega_2^{\text{neu}} &=  0 + 1\cdot (1-0)\cdot 1 = 1
\end{align*}
\end{small}
\end{frame}

% 3. Iteration
\begin{frame}{Beispiel: Training eines Perzeptrons für das logische ODER}
\begin{small}
\begin{columns}
\begin{column}{0.25\textwidth}
\hspace{0.5cm}
\begin{footnotesize}
\begin{tabular}{cccc} \toprule
$x_0$ & $x_1$ & $x_2$ & y \\ \midrule
1 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
1 & 1 & 1 & 1 \\ \bottomrule
\end{tabular} \end{footnotesize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{tabular}{ll}
aktueller Input: & $\mathbf{x}^{T} = (1, 1, 0)$ \\
aktueller Output: & $y=1$ \\
\end{tabular}
\end{column}
\begin{column}{0.4\textwidth}
aktuelle Gewichte: \alert{ $\boldsymbol{\omega} = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}$ }
\end{column}
\end{columns}
prognostizierter Output:
\[\hat{y}^{aktuell} = \Phi(\mathbf{x}^{T}\boldsymbol{\omega}) = \Phi\left( \begin{pmatrix} 1, 1, 0 \end{pmatrix}\cdot \alert{\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}} \right) = \Phi(0) = 1 \qquad = \qquad y = 1\]
keine Aktualisierung notwendig!
\begin{align*} % dummy for horizontal space
\phantom{\omega_0^{\text{neu}}} & \\
\phantom{\omega_1^{\text{neu}}} & \\
\phantom{\omega_2^{\text{neu}}} &
\end{align*}
\end{small}
\end{frame}

% 4. Iteration
\begin{frame}{Beispiel: Training eines Perzeptrons für das logische ODER}
\begin{small}
\begin{columns}
\begin{column}{0.25\textwidth}
\hspace{0.5cm}
\begin{footnotesize}
\begin{tabular}{cccc} \toprule
$x_0$ & $x_1$ & $x_2$ & y \\ \midrule
1 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
1 & 1 & 1 & 1 \\ \bottomrule
\end{tabular} \end{footnotesize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{tabular}{ll}
aktueller Input: & $\mathbf{x}^{T} = (1, 1, 1)$ \\
aktueller Output: & $y=1$ \\
\end{tabular}
\end{column}
\begin{column}{0.4\textwidth}
aktuelle Gewichte: \alert{ $\boldsymbol{\omega} = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}$ }
\end{column}
\end{columns}
prognostizierter Output:
\[\hat{y}^{aktuell} = \Phi(\mathbf{x}^{T}\boldsymbol{\omega}) = \Phi\left( \begin{pmatrix} 1, 1, 1 \end{pmatrix}\cdot \alert{\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}} \right) = \Phi(1) = 1 \qquad = \qquad y = 1\]
keine Aktualisierung notwendig!
\begin{align*} % dummy for horizontal space
\phantom{\omega_0^{\text{neu}}} & \\
\phantom{\omega_1^{\text{neu}}} & \\
\phantom{\omega_2^{\text{neu}}} &
\end{align*}
\end{small}
\end{frame}

% 5. Iteration
\begin{frame}{Beispiel: Training eines Perzeptrons für das logische ODER}
\begin{small}
\begin{columns}
\begin{column}{0.25\textwidth}
\hspace{0.5cm}
\begin{footnotesize}
\begin{tabular}{cccc} \toprule
$x_0$ & $x_1$ & $x_2$ & y \\ \midrule
1 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
1 & 1 & 1 & 1 \\ \bottomrule
\end{tabular} \end{footnotesize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{tabular}{ll}
aktueller Input: & $\mathbf{x}^{T} = (1, 0, 0)$ \\
aktueller Output: & $y=0$ \\
\end{tabular}
\end{column}
\begin{column}{0.4\textwidth}
aktuelle Gewichte: \alert{ $\boldsymbol{\omega} = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}$ }
\end{column}
\end{columns}
prognostizierter Output:
\[\hat{y}^{aktuell} = \Phi(\mathbf{x}^{T}\boldsymbol{\omega}) = \Phi\left( \begin{pmatrix} 1, 0, 0 \end{pmatrix}\cdot \alert{\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}} \right) = \Phi(0) = 1 \qquad \neq \qquad y = 0\]
Aktualisierung notwendig:
\begin{align*}
\omega_0^{\text{neu}} &=  0 + 1\cdot (0-1)\cdot 1 = -1 \\
\omega_1^{\text{neu}} &=  0 + 1\cdot (0-1)\cdot 0 = 0 \\
\omega_2^{\text{neu}} &=  0 + 1\cdot (0-1)\cdot 0 = 1
\end{align*}
\end{small}
\end{frame}

% 6. Iteration
\begin{frame}{Beispiel: Training eines Perzeptrons für das logische ODER}
\begin{small}
\begin{columns}
\begin{column}{0.25\textwidth}
\hspace{0.5cm}
\begin{footnotesize}
\begin{tabular}{cccc} \toprule
$x_0$ & $x_1$ & $x_2$ & y \\ \midrule
1 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
1 & 1 & 1 & 1 \\ \bottomrule
\end{tabular} \end{footnotesize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{tabular}{ll}
aktueller Input: & $\mathbf{x}^{T} = (1, 0, 1)$ \\
aktueller Output: & $y=1$ \\
\end{tabular}
\end{column}
\begin{column}{0.4\textwidth}
aktuelle Gewichte: \alert{ $\boldsymbol{\omega} = \begin{pmatrix} -1 \\ 0 \\ 1 \end{pmatrix}$ }
\end{column}
\end{columns}
prognostizierter Output:
\[\hat{y}^{aktuell} = \Phi(\mathbf{x}^{T}\boldsymbol{\omega}) = \Phi\left( \begin{pmatrix} 1, 0, 1 \end{pmatrix}\cdot \alert{\begin{pmatrix} -1 \\ 0 \\ 1 \end{pmatrix}} \right) = \Phi(0) = 1 \qquad = \qquad y = 1\]
keine Aktualisierung notwendig!
\begin{align*} % dummy for horizontal space
\phantom{\omega_0^{\text{neu}}} & \\
\phantom{\omega_1^{\text{neu}}} & \\
\phantom{\omega_2^{\text{neu}}} &
\end{align*}
\end{small}
\end{frame}

% 7. Iteration
\begin{frame}{Beispiel: Training eines Perzeptrons für das logische ODER}
\begin{small}
\begin{columns}
\begin{column}{0.25\textwidth}
\hspace{0.5cm}
\begin{footnotesize}
\begin{tabular}{cccc} \toprule
$x_0$ & $x_1$ & $x_2$ & y \\ \midrule
1 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
1 & 1 & 1 & 1 \\ \bottomrule
\end{tabular} \end{footnotesize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{tabular}{ll}
aktueller Input: & $\mathbf{x}^{T} = (1, 1, 0)$ \\
aktueller Output: & $y=1$ \\
\end{tabular}
\end{column}
\begin{column}{0.4\textwidth}
aktuelle Gewichte: \alert{ $\boldsymbol{\omega} = \begin{pmatrix} -1 \\ 0 \\ 1 \end{pmatrix}$ }
\end{column}
\end{columns}
prognostizierter Output:
\[\hat{y}^{aktuell} = \Phi(\mathbf{x}^{T}\boldsymbol{\omega}) = \Phi\left( \begin{pmatrix} 1, 1, 0 \end{pmatrix}\cdot \alert{\begin{pmatrix} -1 \\ 0 \\ 1 \end{pmatrix}} \right) = \Phi(-1) = 0 \qquad \neq \qquad y = 1\]
Aktualisierung notwendig:
\begin{align*}
\omega_0^{\text{neu}} &= -1 + 1\cdot (1-0)\cdot 1 = 0 \\
\omega_1^{\text{neu}} &=  0 + 1\cdot (1-0)\cdot 1 = 1 \\
\omega_2^{\text{neu}} &=  1 + 1\cdot (1-0)\cdot 0 = 1
\end{align*}
\end{small}
\end{frame}

% 8. Iteration
\begin{frame}{Beispiel: Training eines Perzeptrons für das logische ODER}
\begin{small}
\begin{columns}
\begin{column}{0.25\textwidth}
\hspace{0.5cm}
\begin{footnotesize}
\begin{tabular}{cccc} \toprule
$x_0$ & $x_1$ & $x_2$ & y \\ \midrule
1 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
1 & 1 & 1 & 1 \\ \bottomrule
\end{tabular} \end{footnotesize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{tabular}{ll}
aktueller Input: & $\mathbf{x}^{T} = (1, 1, 1)$ \\
aktueller Output: & $y=1$ \\
\end{tabular}
\end{column}
\begin{column}{0.4\textwidth}
aktuelle Gewichte: \alert{ $\boldsymbol{\omega} = \begin{pmatrix} 0 \\ 1 \\ 1 \end{pmatrix}$ }
\end{column}
\end{columns}
prognostizierter Output:
\[\hat{y}^{aktuell} = \Phi(\mathbf{x}^{T}\boldsymbol{\omega}) = \Phi\left( \begin{pmatrix} 1, 1, 1 \end{pmatrix}\cdot \alert{\begin{pmatrix} 0 \\ 1 \\ 1 \end{pmatrix}} \right) = \Phi(2) = 1 \qquad = \qquad y = 1\]
keine Aktualisierung notwendig!
\begin{align*} % dummy for horizontal space
\phantom{\omega_0^{\text{neu}}} & \\
\phantom{\omega_1^{\text{neu}}} & \\
\phantom{\omega_2^{\text{neu}}} &
\end{align*}
\end{small}
\end{frame}

% 9. Iteration
\begin{frame}{Beispiel: Training eines Perzeptrons für das logische ODER}
\begin{small}
\begin{columns}
\begin{column}{0.25\textwidth}
\hspace{0.5cm}
\begin{footnotesize}
\begin{tabular}{cccc} \toprule
$x_0$ & $x_1$ & $x_2$ & y \\ \midrule
1 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
1 & 1 & 1 & 1 \\ \bottomrule
\end{tabular} \end{footnotesize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{tabular}{ll}
aktueller Input: & $\mathbf{x}^{T} = (1, 0, 0)$ \\
aktueller Output: & $y=0$ \\
\end{tabular}
\end{column}
\begin{column}{0.4\textwidth}
aktuelle Gewichte: \alert{ $\boldsymbol{\omega} = \begin{pmatrix} 0 \\ 1 \\ 1 \end{pmatrix}$ }
\end{column}
\end{columns}
prognostizierter Output:
\[\hat{y}^{aktuell} = \Phi(\mathbf{x}^{T}\boldsymbol{\omega}) = \Phi\left( \begin{pmatrix} 1, 0, 0 \end{pmatrix}\cdot \alert{\begin{pmatrix} 0 \\ 1 \\ 1 \end{pmatrix}} \right) = \Phi(0) = 1 \qquad \neq \qquad y = 0\]
Aktualisierung notwendig:
\begin{align*}
\omega_0^{\text{neu}} &=  0 + 1\cdot (0-1)\cdot 1 = -1 \\
\omega_1^{\text{neu}} &=  1 + 1\cdot (0-1)\cdot 0 = 1 \\
\omega_2^{\text{neu}} &=  1 + 1\cdot (0-1)\cdot 0 = 1
\end{align*}
\end{small}
\end{frame}

% 10. Iteration
\begin{frame}{Beispiel: Training eines Perzeptrons für das logische ODER}
\begin{small}
\begin{columns}
\begin{column}{0.25\textwidth}
\hspace{0.5cm}
\begin{footnotesize}
\begin{tabular}{cccc} \toprule
$x_0$ & $x_1$ & $x_2$ & y \\ \midrule
1 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
1 & 1 & 1 & 1 \\ \bottomrule
\end{tabular} \end{footnotesize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{tabular}{ll}
aktueller Input: & $\mathbf{x}^{T} = (1, 0, 1)$ \\
aktueller Output: & $y=1$ \\
\end{tabular}
\end{column}
\begin{column}{0.4\textwidth}
aktuelle Gewichte: \alert{ $\boldsymbol{\omega} = \begin{pmatrix} -1 \\ 1 \\ 1 \end{pmatrix}$ }
\end{column}
\end{columns}
prognostizierter Output:
\[\hat{y}^{aktuell} = \Phi(\mathbf{x}^{T}\boldsymbol{\omega}) = \Phi\left( \begin{pmatrix} 1, 0, 1 \end{pmatrix}\cdot \alert{\begin{pmatrix} -1 \\ 1 \\ 1 \end{pmatrix}} \right) = \Phi(0) = 1 \qquad = \qquad y = 1\]
keine Aktualisierung notwendig!
\begin{align*} % dummy for horizontal space
\phantom{\omega_0^{\text{neu}}} & \\
\phantom{\omega_1^{\text{neu}}} & \\
\phantom{\omega_2^{\text{neu}}} &
\end{align*}
\end{small}
\end{frame}

% 11. Iteration
\begin{frame}{Beispiel: Training eines Perzeptrons für das logische ODER}
\begin{small}
\begin{columns}
\begin{column}{0.25\textwidth}
\hspace{0.5cm}
\begin{footnotesize}
\begin{tabular}{cccc} \toprule
$x_0$ & $x_1$ & $x_2$ & y \\ \midrule
1 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
1 & 1 & 1 & 1 \\ \bottomrule
\end{tabular} \end{footnotesize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{tabular}{ll}
aktueller Input: & $\mathbf{x}^{T} = (1, 1, 0)$ \\
aktueller Output: & $y=1$ \\
\end{tabular}
\end{column}
\begin{column}{0.4\textwidth}
aktuelle Gewichte: \alert{ $\boldsymbol{\omega} = \begin{pmatrix} -1 \\ 1 \\ 1 \end{pmatrix}$ }
\end{column}
\end{columns}
prognostizierter Output:
\[\hat{y}^{aktuell} = \Phi(\mathbf{x}^{T}\boldsymbol{\omega}) = \Phi\left( \begin{pmatrix} 1, 1, 0 \end{pmatrix}\cdot \alert{\begin{pmatrix} -1 \\ 1 \\ 1 \end{pmatrix}} \right) = \Phi(0) = 1 \qquad = \qquad y = 1\]
keine Aktualisierung notwendig!
\begin{align*} % dummy for horizontal space
\phantom{\omega_0^{\text{neu}}} & \\
\phantom{\omega_1^{\text{neu}}} & \\
\phantom{\omega_2^{\text{neu}}} &
\end{align*}
\end{small}
\end{frame}

% 12. Iteration
\begin{frame}{Beispiel: Training eines Perzeptrons für das logische ODER}
\begin{small}
\begin{columns}
\begin{column}{0.25\textwidth}
\hspace{0.5cm}
\begin{footnotesize}
\begin{tabular}{cccc} \toprule
$x_0$ & $x_1$ & $x_2$ & y \\ \midrule
1 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
1 & 1 & 1 & 1 \\ \bottomrule
\end{tabular} \end{footnotesize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{tabular}{ll}
aktueller Input: & $\mathbf{x}^{T} = (1, 1, 1)$ \\
aktueller Output: & $y=1$ \\
\end{tabular}
\end{column}
\begin{column}{0.4\textwidth}
aktuelle Gewichte: \alert{ $\boldsymbol{\omega} = \begin{pmatrix} -1 \\ 1 \\ 1 \end{pmatrix}$ }
\end{column}
\end{columns}
prognostizierter Output:
\[\hat{y}^{aktuell} = \Phi(\mathbf{x}^{T}\boldsymbol{\omega}) = \Phi\left( \begin{pmatrix} 1, 1, 1 \end{pmatrix}\cdot \alert{\begin{pmatrix} -1 \\ 1 \\ 1 \end{pmatrix}} \right) = \Phi(1) = 1 \qquad = \qquad y = 1\]
keine Aktualisierung notwendig!
\begin{align*} % dummy for horizontal space
\phantom{\omega_0^{\text{neu}}} & \\
\phantom{\omega_1^{\text{neu}}} & \\
\phantom{\omega_2^{\text{neu}}} &
\end{align*}
\end{small}
\end{frame}

% 13. Iteration
\begin{frame}{Beispiel: Training eines Perzeptrons für das logische ODER}
\begin{small}
\begin{columns}
\begin{column}{0.25\textwidth}
\hspace{0.5cm}
\begin{footnotesize}
\begin{tabular}{cccc} \toprule
$x_0$ & $x_1$ & $x_2$ & y \\ \midrule
1 & 0 & 0 & 0 \\
1 & 0 & 1 & 1 \\
1 & 1 & 0 & 1 \\
1 & 1 & 1 & 1 \\ \bottomrule
\end{tabular} \end{footnotesize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{tabular}{ll}
aktueller Input: & $\mathbf{x}^{T} = (1, 0, 0)$ \\
aktueller Output: & $y=0$ \\
\end{tabular}
\end{column}
\begin{column}{0.4\textwidth}
aktuelle Gewichte: \alert{ $\boldsymbol{\omega} = \begin{pmatrix} -1 \\ 1 \\ 1 \end{pmatrix}$ }
\end{column}
\end{columns}
prognostizierter Output:
\[\hat{y}^{aktuell} = \Phi(\mathbf{x}^{T}\boldsymbol{\omega}) = \Phi\left( \begin{pmatrix} 1, 0, 0 \end{pmatrix}\cdot \alert{\begin{pmatrix} -1 \\ 1 \\ 1 \end{pmatrix}} \right) = \Phi(-1) = 0 \qquad = \qquad y = 1\]
keine Aktualisierung notwendig!
\begin{align*} % dummy for horizontal space
\phantom{\omega_0^{\text{neu}}} & \\
\phantom{\omega_1^{\text{neu}}} & \\
\phantom{\omega_2^{\text{neu}}} &
\end{align*}
\end{small}
\end{frame}

\end{document}
