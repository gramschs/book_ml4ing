{"version":"1","records":[{"hierarchy":{"lvl1":"1.1 Was ist maschinelles Lernen?"},"type":"lvl1","url":"/chapter01-sec01","position":0},{"hierarchy":{"lvl1":"1.1 Was ist maschinelles Lernen?"},"content":"KI (Künstliche Intelligenz) ist in aller Munde. Etwas seltener wird der Begriff\nmaschinelles Lernen verwendet. Maschinelles Lernen, oft auch Machine\nLearning genannt, ist ein Teilgebiet der Künstlichen Intelligenz.\n\nWir kürzen in dieser Vorlesung maschinelles Lernen oft mit ML ab. Damit\numgehen wir die Diskussion, warum Künstliche Intelligenz mit einem\nGroßbuchstaben beginnt und maschinelles Lernen mit einem Kleinbuchstaben.\nGleichzeitig ist das auch die gängige Abkürzung im englischen Sprachgebrauch.\nDieses Kapitel klärt, was maschinelles Lernen ist und führt in die\ngrundlegenenden Bestandteile eines ML-Systems ein.","type":"content","url":"/chapter01-sec01","position":1},{"hierarchy":{"lvl1":"1.1 Was ist maschinelles Lernen?","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter01-sec01#lernziele","position":2},{"hierarchy":{"lvl1":"1.1 Was ist maschinelles Lernen?","lvl2":"Lernziele"},"content":"Lernziele\n\nSie wissen, wie langes es das Forschungsgebiet maschinelles Lernen gibt\nund warum es sich in den letzten beiden Jahrzehnten so stark entwickelt hat.\n\nSie kennen die Bestandteile eines ML-Systems: Daten, Algorithmus und\nModell.","type":"content","url":"/chapter01-sec01#lernziele","position":3},{"hierarchy":{"lvl1":"1.1 Was ist maschinelles Lernen?","lvl2":"Ein wenig Geschichte"},"type":"lvl2","url":"/chapter01-sec01#ein-wenig-geschichte","position":4},{"hierarchy":{"lvl1":"1.1 Was ist maschinelles Lernen?","lvl2":"Ein wenig Geschichte"},"content":"Viele glauben, dass die Forschungsgebiete Künstliche Intelligenz und\nmaschinelles Lernen Neuentwicklungen des 21. Jahrhunderts sind. Doch tatsächlich\nhat Arthur L. Samuel bereits 1959 maschinelles Lernen wie folgt definiert:\n\n»... ein Forschungsgebiet, das Computer in die Lage versetzen soll, zu lernen,\nohne explizit darauf programmiert zu sein.«\n\nArthur L. Samuel, 1959\n\nWikipedia → Maschinelles Lernen\nbietet eine weitere Definition:\n\n»Maschinelles Lernen (ML) ist ein Oberbegriff für die „künstliche“ Generierung\nvon Wissen aus Erfahrung: Ein künstliches System lernt aus Beispielen und kann\ndiese nach Beendigung der Lernphase verallgemeinern.«\n\nAuch hier wird der Aspekt betont, dass das künstliche System selbst lernt. Aber\nwas ist mit selbst Lernen gemeint? Ein Kind lernt beispielsweise selbst das\nLaufen. Auch wenn Eltern präzise beschreiben könnten, welcher Muskel zu welchem\nZeitpunkt kontrahiert werden muss und mit welcher Geschwindigkeit in welche\nRichtung das Bein bewegt werden soll, würde das Kind die Anweisungen nicht\nverstehen können. Ein Kind lernt selbst durch Versuch und Irrtum. In der\nAnfangszeit der Robotik versuchten Forscherinnen und Forscher, Roboter durch\nexplizite Befehle zu steuern. Doch bei unvorhergesehenen Hindernissen stießen\nsolche Roboter an ihre Grenzen. Aus der Notwendigkeit, dass Roboter ähnlich wie\nMenschen lernen, entwickelte sich das Teilgebiet maschinelles Lernen innerhalb\nder Künstlichen Intelligenz.\n\nDoch nicht nur in der Robotik spielt maschinelles Lernen eine wichtige Rolle.\nAuch bei der Datenanalyse kann es hilfreich sein, wenn ein Computersystem\neigenständig Muster in den Daten erkennt. Ein Klassiker dafür ist die\nSpam-Erkennung. Natürlich ist es möglich, den Spam-Filter mit expliziten Regeln\nzu programmieren. Schon nach kurzer Zeit ändern jedoch Spammer die E-Mail-Texte\nund schon greifen die expliziten Regeln nicht mehr. Hier helfen maschinell\ngelernte Regeln. Durch die Markierung von E-Mails als Spam lernt das\nE-Mail-Programm nach und nach selbst Regeln, um Spam-E-Mails zu identifizieren.\n\nDie exponentiell wachsende Datenmenge der letzten beiden Jahrzehnte hat das\nInteresse an maschinellen Lernverfahren stark erhöht. Es gibt aber auch noch\nandere Gründe, die zum aktuellen Boom des maschinellen Lernens beigetragen\nhaben.\n\nMini-Übung\n\nSchauen Sie sich das folgende Video an. Welche drei Gründe werden dort genannt,\nwarum maschinelles Lernen zuletzt so stark nachgefragt wurde?\n\nLösung\n\nJannik nennt bei Zeitindex 2:44 min drei Gründe, warum in den letzten zwei Jahrzehnten maschinelles Lernen an Bedeutung gewonnen hat:\n\nmehr Daten\n\nschnellere Computer\n\nForschung\n\nEs gibt einige Gründe, warum ML in den letzten zwei Jahrzehnten so stark an\nBedeutung gewonnen hat. Im Folgenden werden die wichtigsten Gründe für den\nBedeutungsgewinn erläutert.\n\nDatenverfügbarkeit: Die Produktion von Daten hat mit der Digitalisierung\nmassiv zugenommen. Mit der Einführung der Smartphones ist auch die Anzahl der\nBilder und Videos deutlich gewachsen, die täglich aufgenommen werden. Das\nKaufverhalten von Kunden in Online-Shops wird beobachtet, Fußballspiele werden\nstatistisch analysiert oder Maschinen mit Messsensoren bestückt. Das\nBeratungsunternehmen IDC (International Data Corporation) prognostiziert, dass\nsich die Datenmenge von 33 Zettabytes im Jahr 2018 auf 175 Zettebytes im Jahr\n2025 mehr als verfünffachen wird \n\nReinsel et al. (2020).\n\nRechenleistung: Der rasante Fortschritt in der Computertechnologie hat die\nRechenleistung, die für maschinelles Lernen erforderlich ist, drastisch erhöht.\nSpeziell für die Entwicklung sogenannter neuronaler Netze werden sogar häufig\nGrafikkarten (GPUs) anstatt eines Prozessors (CPU) bevorzugt.\n\nAlgorithmen und Modelle: Natürlich ist auch die Erforschung neuer\nAlgorithmen und Modelle nicht stehengeblieben. Ein Durchbruch in der Forschung\nwar dabei die Entwicklung von den sogenannten Deep-Learning-Modellen, einer\nVariante der neuronalen Netze.\n\nSoftware und Tools: Es gibt jetzt eine Vielzahl von Softwarebibliotheken und\nTools (z.B. \n\nTensorFlow,\n\n\nPyTorch,\n\n\nScikit-learn usw.), die es\nermöglichen, auch ohne tiefergehende Mathematik- und Programmierkenntnissen\nmaschinelles Lernen in der Praxis einzusetzen. Daher werden immer mehr\nAnwendungen mit maschinellem Lernen analysiert und optimiert.\n\nBisher haben wir nicht besprochen, was es mit Künstlicher Intelligenz und Deep\nLearning auf sich hat. Beide Begriffe werden oft in einem Atemzug mit ML\ngenannt. Das folgende Video gibt eine Einführung dazu.\n\nVideo zu “ML Tutorial - #1 Einführung in ML” von CodingWithMagga","type":"content","url":"/chapter01-sec01#ein-wenig-geschichte","position":5},{"hierarchy":{"lvl1":"1.1 Was ist maschinelles Lernen?","lvl2":"Was sind Algorithmen und Modelle?"},"type":"lvl2","url":"/chapter01-sec01#was-sind-algorithmen-und-modelle","position":6},{"hierarchy":{"lvl1":"1.1 Was ist maschinelles Lernen?","lvl2":"Was sind Algorithmen und Modelle?"},"content":"Ein notwendiger Baustein des maschinellen Lernens sind Daten, am besten ganz,\nganz viele! Aber selbst ein riesiger Haufen an Daten ist alleine wertlos. Erst\ndurch Algorithmen, die in diesen Daten Muster finden, gewinnen wir neues Wissen,\nkönnen Prozesse analysieren und Entscheidungen treffen.\n\nAber was ist nun ein Algorithmus? In dem folgenden Video wird zuerst erklärt,\nwas ein Algorithmus ist. Danach werden die ersten grundlegenden Fachbegriffe des\nmaschinellen Lernens eingeführt.\n\nMini-Übung\n\nSchauen Sie sich das folgende Video an und beantworten Sie die folgenden Fragen:\n\nWelche drei Beispiele für Algorithmen werden aufgezählt?\n\nIn dem Video werden die Fachbegriff »Feature« und »Label« eingeführt. Was bedeuten die\nBegriffe?\n\nWas bedeutet »überwachtes Lernen«?\n\nLösung\n\nBei Zeitindex 0:28 min wird erklärt, dass ein Algorithmus eine exakte\nHandlungsvorschrift zum Lösen eines Problems ist. Als erstes Beispiel wird\ndie Anweisung, wie ein mathematisches Problem zu lösen ist, genannt. Als\nweitere Beispiele werden ein Kochrezept oder eine Spieleanleitung aufgezählt.\nAb Zeitindex 0:38 min wird Tic-Tac-Toe als Algorithmus formuliert.\n\nAb Zeitindex 3:19 min werden die Begriffe Feature und Label eingeführt. Die\nEingangsdaten (z.B. Bilder von Hunden und Katzen), die wir dem Computer zum\nLernen geben, werden Features genannt. Daraus soll der Computer lernen, Hunde\nvon Katzen zu unterscheiden. Dazu müssen zuerst die Bilder, die einen Hund\nzeigen, als »Hundebild« gekennzeichnet werden und die anderen als\n»Katzenbild«. Die Kennzeichnung wird Label genannt. Die Zielgröße bzw. Ziel ist die\nEinordnung Hund/Katze.\n\nÜberwachtes Lernen ist das Lernen aus Beispielen (Zeitindex 4:03 min). Bevor\nder Computer lernt, auf Bildern Hunde und Katzen zu unterscheiden, müssen die\nBilder mit den Labels Hund/Katze versehen werden.\n\nEin Algorithmus ist also eine Anleitung, wie ein Problem zu lösen ist.\nTypisch für einen Algorithmus ist, das in sehr kleinen Schritten detailliert\nAnweisungen formuliert werden, um das Problem zu lösen. In der Informatik sind\nAlgorithmen besonders wichtig, da durch sie festgelegt wird, wie der Computer\nDaten verarbeiten und ein Problem lösen soll. Jeder einzelne Schritt zur\nProblemlösung muss eindeutig und konkret beschrieben werden. Wird der\nAlgorithmus in einer Programmiersprache formuliert, verwendet man den Begriff\nComputerprogramm.\n\nWas ist ... ein Algorithmus?\n\nEin Algorithmus ist eine spezifische Anleitung mit einzelnen Anweisungen, wie ein\nbestimmtes Problem gelöst werden soll.\n\nDer Begriff Modell hat viele verschiedenen Bedeutungen (siehe \n\nWikipedia →\nModell\n(Begriffsklärung)).\nZunächst einmal bedeutet Modell, das ein Original vereinfacht beschrieben wird.\nVereinfacht heißt, dass beispielsweise Details weggelassen werden oder die\nAbmessungen geändert werden. In der Architektur wird beispielsweise ein Haus in\nkleinem Maßstab gebaut, um potentiellen Kunden durch das Modell einen besseren\nEindruck zu vermitteln, wie das Haus in echt aussehen wird. Im Maschinenbau wird\ndas Modell eines Flugzeugs in einen Windkanal gehalten, um die Flugzeuggeometrie\nzu optimieren. In manchen naturwissenschaftlichen Museen gibt es begehbare\nModelle von Organen wie beispielsweise dem Darm, um den Aufbau des Darms\nbegreifbar zu machen. Es gibt auch auch virtuelle Modelle wie  das\n\n\nVier-Ohren-Modell des\nKommunikationswissenschaftlers Friedemann Schulz von Thun, das das\nKommunikationsverhalten von Menschen beschreibt.\n\nIn der Welt des maschinellen Lernens bezieht sich der begriff Modell darauf,\nDaten zu interpretieren und basierend auf diesen Daten Vorhersagen oder\nEntscheidungen zu treffen.\n\nWas ist ... ein Modell?\n\nEin Modell ist ein vereinfachtes Abbild der Wirklichkeit. Im Kontext das\nmaschinellen Lernens ist das ML-Modell eine abstrake Beschreibung eines Systems,\ndas unbekannte Daten interpretieren kann oder basierend auf diesen Daten\nVorhersagen oder Entscheidungen treffen kann.","type":"content","url":"/chapter01-sec01#was-sind-algorithmen-und-modelle","position":7},{"hierarchy":{"lvl1":"1.1 Was ist maschinelles Lernen?","lvl2":"Maschinelles Lernen ist wie Kuchenbacken"},"type":"lvl2","url":"/chapter01-sec01#maschinelles-lernen-ist-wie-kuchenbacken","position":8},{"hierarchy":{"lvl1":"1.1 Was ist maschinelles Lernen?","lvl2":"Maschinelles Lernen ist wie Kuchenbacken"},"content":"Damit kommen beim maschinellen Lernen drei Dinge zusammen: Daten, Algorithmus\nund Modell. Um das Verhältnis zwischen den drei Konzepten zu verdeutlichen,\nvergleichen wir die Konzepte mit dem Kuchenbacken. Die Daten sind die Zutaten,\naus denen ein Kuchen gebacken werden soll. Der Algorithmus ist das Rezept mit\neiner detaillierten Schritt-für-Schritt-Anleitung, wie der Kuchen gebacken\nwerden soll. Das Modell hingegen ist der fertige Kuchen, der aus dem Prozess\nherauskommt. Es ist somit das Endprodukt, das erstellt wird, indem man den\nAnweisungen des Rezepts (dem Algorithmus) folgt und die Zutaten zusammenfügt.\n\nSobald das Modell bzw. der Kuchen fertig ist, wird dieses Modell\ndann verwendet, um Vorhersagen zu treffen oder Entscheidungen zu treffen, genau\nwie man einen Kuchen essen würde, nachdem er gebacken ist.\n\n\n\nFigure 1:Analogie zwischen dem ML-Workflow und dem Kuchenbacken (Quelle: eigene Darstellung)\n\nAllerdings ist es damit nicht getan. Je nachdem, wie viele und welche Gäste\nerwartet werden, benötigen wir einen anderen Kuchen. Bei einer großen\nGeburtstagsparty brauchen wir einen Blechkuchen, damit jeder Gast ein Stückchen\nKuchen bekommt. Haben wir Diabetiker eingeladen, sollten wir keine\nSchokoladentorte anbieten. Auch beim maschinellen Lernen ist es daher sehr\nwichtig, je nach Einsatzzweck das passende Modell bzw. den passenden Algorithmus\nzu wählen.\n\nNatürlich hängt die Wahl des Kuchens auch von den vorhandenen Zutaten ab. Fehlt\ndie Schokolade, so kann ich keinen Schokoladenkuchen backen. Entweder backen wir\ndann einen anderen Kuchen oder wir entscheiden uns, noch schnell zum Supermarkt\nzu gehen und Schokolade einzukaufen. Vielleicht stellen wir auch fest, dass die\nMilch abgelaufen ist und nicht mehr genießbar ist. Dann ist unser Plan nicht\ndurchführbar. Und auch hier können wir uns entscheiden, einen anderen Kuchen zu\nbacken oder die Zutaten zu erneuern. Diese Analogie passt auch zu maschinellem\nLernen. Fehlen Daten oder sind die Daten nicht qualititativ hochwertig, können\nwir die Datenlage verbessern, indem wir beispielsweise mehr Experimente\ndurchführen oder offensichtlich schiefgelaufene Experimente wiederholen. Diese\nPhase des maschinellen Lernen wird auch Datenerkundung oder\nDatenexploration genannt. Sollten wir die Daten jedoch nicht verbessern\nkönnen (oder wollen, weil zu teuer oder die Abgabefrist der Bachelorarbeit\nansteht), dann müssen wir die Auswahl des Modells an die vorhandenen Daten\nanpassen.\n\nZutaten komplett, Rezept ausgewählt, Kuchen gebacken, der Gast beißt in den\nKuchen und verzieht das Gesicht ... Zucker und Salz verwechselt. Hätten wir den\nKuchen lieber einmal vor dem Servieren probiert. Auch beim maschinellen Lernen\nist es mit dem “Backen” des Modells nicht getan. Ist ein Modell erstellt, so\nmuss es auch bewertet werden. Der Prozess des maschinellen Lernens wird mit der\nValidierung abgeschlossen, bevor das Modell dann produktiv eingesetzt wird.\nDie Erstellung und Verwendung von Modellen im maschinellen Lernen ist ein\nfortlaufender Prozess. Modelle werden oft mehrfach getestet und angepasst, um\nihre Leistung zu verbessern. Beim Kuchenbacken könnte der Bäcker auf die Idee\nkommen, den Kuchen nicht bei 160 °C, sondern bei 162 °C zu backen, weil dann der\nKuchen noch besser schmeckt. Solche Parameter zum Finetunen eines Modells werden\nHyperparameter genannt. Hyperparameter haben nichts mit den vorhanden Daten\nzu tun, sondern gehören zum ML-Modell. Aber auch wenn sich Daten verändern und\nneue Daten hinzukommen, muss das Modell aktualisiert werden, um mit den sich\nändernden Bedingungen zurechtzukommen.\n\nDie folgende Skizze zeigt den schematischen Ablauf eines typischen ML-Projektes.\nDabei benutzen wir das sogenannte QUA3CK-Modell nach einem Vorschlag von\n\n\nStock et al. (2020). Das QUA3CK-Modell zeigt den typischen Ablauf eines\nML-Projektes von der wissenschaftlichen Fragestellung (Q -- Question) bis zu\nderen Beantwortung (K -- Knowledge Transfer). Dazu gehört das Sammeln und\nErkunden der Daten (U -- Understanding the data), mit Hilfe derer die Frage\nbeantwortet werden soll. Die Phase der ML-Modellbildung wird mehrfach\ndurchlaufen und besteht aus der Auswahl und dem Training des Algorithmus bzw.\ndes Modells (A -- Algorithm selection and training), dazu passend der Auswahl\nund Anpassung der Daten (A -- Adaption of the data) sowie der Anpassung der\nHyperparamter (A -- Adjustement of the hyperparameter). Die Modelle, die durch\ndiese Schleife erstellt werden, werden letztendlich miteinander verglichen und\nbewertet (C -- Comparison and Conclusion), bevor sie produktiv eingesetzt\nwerden.\n\n\n\nFigure 2:Typischer Ablauf eines ML-Projektes als QUA3CK-Prozess dargestellt\n(Quelle: in Anlehnung an \n\nStock et al. (2020))\n\nDas folgende Video erklärt den ML-Workflow etwas detaillierter, als wir es mit\nder Kuchenbacken-Analogie getan haben. Als Ausblick auf die weitere Vorlesung\nbietet dieses Video dennoch eine sehr gute Übersicht über die Vorgehenweise in\neinem ML-Projekt und ist daher sehr empfehlenswert.\n\nVideo zu “ML Tutorial - #2 ML-Workflow” von CodingWithMagga","type":"content","url":"/chapter01-sec01#maschinelles-lernen-ist-wie-kuchenbacken","position":9},{"hierarchy":{"lvl1":"1.1 Was ist maschinelles Lernen?","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter01-sec01#zusammenfassung-und-ausblick","position":10},{"hierarchy":{"lvl1":"1.1 Was ist maschinelles Lernen?","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Kapitel haben wir zwar die drei grundlegenden Bestandteile eines\nML-Systems (Daten, Algorithmus und Modell) kennengelernt, aber entscheidend ist\nauch die Anwendung, welche Art von Daten vorliegen. Im nächsten Kapitel werden\nwir die drei großen Kategorien betrachten, in die ML-Modelle eingeteilt werden:\n\nÜberwachtes Lernen (Supervised Learning),\n\nUnüberwachtes Lernen (Unsupervised Learning) und\n\nVerstärkendes Lernen (Reinforcement Learning).","type":"content","url":"/chapter01-sec01#zusammenfassung-und-ausblick","position":11},{"hierarchy":{"lvl1":"1.2 Überwachtes, unüberwachtes und verstärkendes Lernen"},"type":"lvl1","url":"/chapter01-sec02","position":0},{"hierarchy":{"lvl1":"1.2 Überwachtes, unüberwachtes und verstärkendes Lernen"},"content":"Nachdem im letzten Kapitel erklärt wurde, was machinelles Lernen überhaupt\nist, betrachten wir in diesem Kapitel die drei großen Kategorien von\nML-Modellen: überwachtes Lernen (Supervised Learning), unüberwachtes Lernen\n(Unsupervised Learning) und verstärkendes Lernen (Reinforcement Learning).","type":"content","url":"/chapter01-sec02","position":1},{"hierarchy":{"lvl1":"1.2 Überwachtes, unüberwachtes und verstärkendes Lernen","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter01-sec02#lernziele","position":2},{"hierarchy":{"lvl1":"1.2 Überwachtes, unüberwachtes und verstärkendes Lernen","lvl2":"Lernziele"},"content":"Lernziele\n\nSie können anhand eines Beispiels erklären, was die Fachbegriffe\n\nüberwachtes Lernen (Supervised Learning),\n\nunüberwachtes Lernen (Unsupervised Learning) und\n\nverstärkendes Lernen (Reinforcement Learning) bedeuten.\n\nSie können beim überwachten Lernen zwischen Regression und\nKlassifikation unterscheiden.","type":"content","url":"/chapter01-sec02#lernziele","position":3},{"hierarchy":{"lvl1":"1.2 Überwachtes, unüberwachtes und verstärkendes Lernen","lvl2":"Überwachtes Lernen (Supervised Learning)"},"type":"lvl2","url":"/chapter01-sec02#id-berwachtes-lernen-supervised-learning","position":4},{"hierarchy":{"lvl1":"1.2 Überwachtes, unüberwachtes und verstärkendes Lernen","lvl2":"Überwachtes Lernen (Supervised Learning)"},"content":"Im letzten Kapitel haben wir im Video \n\n»So lernen Maschinen:\nAlgorithmen« die Aufgabenstellung kennengelernt,\nauf Fotos Hunde von Katzen zu unterscheiden. Diese Art von Problemstellung ist\ntypisch für überwachtes Lernen. Die Daten werden vorab gekennzeichnet, sie\nerhalten ein Label. So lernen auch Kinder. Stellen Sie sich vor, in einem\nKorb liegen Äpfel und Bananen und ein Kind soll den Unterschied erlernen. Jedes\nStück Obst wird aus dem Korb genommen und dem Kind gezeigt. Dazu sagen wir dann\nentweder »Apfel« oder »Banane«. Das Kind hat also einen Lehrer oder Trainer. Mit\nder Zeit wird das Kind zwischen beiden Obstsorten unterscheiden können.\n\nWas ist ... überwachtes Lernen?\n\nÜberwachtes Lernen ist eine Kategorie des maschinellen Lernens. Beim überwachten\nLernen liegen die Daten als Eingabe- und Ausgabedaten mit Labels vor. Ein\nmaschineller Lernalgorithmus versucht ein Modell zu finden, das bestmöglich den\nEingabedaten die Ausgabedaten zuordnet.\n\nBeim überwachten Lernen können die Prognosen des Modells für bekannte Daten mit\nden korrekten Ergebnissen (Labels) verglichen werden. Das Modell wird also\nüberwacht.\n\nPrinzipiell werden dabei wiederum zwei Arten von Labels unterschieden:\n\nkontinuierliche Labels und\n\ndiskrete Labels.\n\nBei dem Beispiel mit den Hunde- und Katzenfotos sind die Labels diskret. Mit\ndiskreten Labels ist gemeint, dass nur wenige verschiedene Labels existieren.\nIn diesem Fall sind es genau zwei verschiedene Labels, nämlich zum einen das\nLabel »Hund« und zum anderen das Label »Katze«. Ein anderes Beispiel für\ndiskrete Labels sind die Schulnoten sehr gut, gut, befriedigend, ausreichend,\nmangelhaft und ungenügend. Es gibt nur sechs verschiedene Noten, die eine\nSchülerin oder ein Schüler in einem Test erreichen kann. Dabei müssen die\ndiskreten Labels keine Texte sein. Die Schulnoten könnten wir auch mit den Labels\n1, 2, 3, 4, 5 und 6 kennzeichnen.\n\nBei den kontinuierlichen Labels gibt es sehr viele, normalerweise unendliche\nviele verschiedene Labels. Textbezeichnungen sind dann nicht mehr sinnvoll, so\ndass kontinuierliche Labels durch Zahlen repräsentiert werden. Ein Beispiel für\nkontinuierliche Ausgabedaten ist der Verkaufspreis eines Autos abhängig vom\nKilometerstand. Normalerweise kosten Neuwagen mit einem Kilometerstand von 0 km\nam meisten und der Preis sinkt, je mehr Kilometer das Auto bereits gefahren\nwurde. Die Verkaufspreise könnte man nun als ganze Zahlen darstellen, wenn man\nsie in ganzen Euros angibt, oder als Fließkommazahl, wenn der Preis auf den Cent\ngenau angegeben wird. Es gibt nicht unendlich viele Verkaufspreise, aber sehr\nviele verschiedene mögliche Werte.\n\nViele ML-Modelle funktionieren sowohl für diskrete als auch kontinuierliche\nDaten, aber nicht alle. Daher ist es notwendig, bereits zu Beginn zu\nentscheiden, ob das Modell für diskrete oder kontinuierliche Ausgabedaten\neingesetzt werden soll.\n\nDas überwachte Lernen wird daher wiederum in zwei Arten unterteilt:\n\nRegression für kontinuierliche Ausgabedaten und\n\nKlassifikation  für diskrete Ausgabedaten.\n\nAuf beide Problemstellungen gehen die nächsten Videos ein.\n\nVideo zu “ML Tutorial - #3 Supervised Learning” von CodingWithMagga","type":"content","url":"/chapter01-sec02#id-berwachtes-lernen-supervised-learning","position":5},{"hierarchy":{"lvl1":"1.2 Überwachtes, unüberwachtes und verstärkendes Lernen","lvl3":"Regression","lvl2":"Überwachtes Lernen (Supervised Learning)"},"type":"lvl3","url":"/chapter01-sec02#regression","position":6},{"hierarchy":{"lvl1":"1.2 Überwachtes, unüberwachtes und verstärkendes Lernen","lvl3":"Regression","lvl2":"Überwachtes Lernen (Supervised Learning)"},"content":"Was ist ... Regression?\n\nRegression ist das Teilgebiet des überwachten maschinellen Lernens, bei dem\nModelle den Zusammenhang zwischen Eingabedaten und kontinuierlichen Ausgabedaten\nprognostizieren sollen.\n\nVideo zu “Überwachtes Lernen – Regression” von Plattform Lernende Systeme","type":"content","url":"/chapter01-sec02#regression","position":7},{"hierarchy":{"lvl1":"1.2 Überwachtes, unüberwachtes und verstärkendes Lernen","lvl3":"Klassifikation","lvl2":"Überwachtes Lernen (Supervised Learning)"},"type":"lvl3","url":"/chapter01-sec02#klassifikation","position":8},{"hierarchy":{"lvl1":"1.2 Überwachtes, unüberwachtes und verstärkendes Lernen","lvl3":"Klassifikation","lvl2":"Überwachtes Lernen (Supervised Learning)"},"content":"Was ist ... Klassifikation?\n\nKlassifikation ist das Teilgebiet des überwachten maschinellen Lernens, bei dem\nModelle den Zusammenhang zwischen Eingabedaten und diskreten Ausgabedaten\nprognostizieren sollen.\n\nVideo zu “Überwachtes Lernen – Klassifikation” von Plattform Lernende Systeme","type":"content","url":"/chapter01-sec02#klassifikation","position":9},{"hierarchy":{"lvl1":"1.2 Überwachtes, unüberwachtes und verstärkendes Lernen","lvl2":"Unüberwachtes Lernen (Unsupervised Learning)"},"type":"lvl2","url":"/chapter01-sec02#un-berwachtes-lernen-unsupervised-learning","position":10},{"hierarchy":{"lvl1":"1.2 Überwachtes, unüberwachtes und verstärkendes Lernen","lvl2":"Unüberwachtes Lernen (Unsupervised Learning)"},"content":"Beim überwachten Lernen liegen Eingabedaten und Ausgabedaten mit Labels vor. Die\nPrognosen eines Modells können für bekannte Paare von Eingabe- und Ausgabedaten\nüberwacht werden. Das ist beim unüberwachten Lernen nicht der Fall. Beim\nunüberwachten Lernen (Unsupervised Learning) gibt es keine Ausgabedaten,\nalso keine Labels. Stattdessen soll der maschinelle Lernalgorithmus eigenständig\nMuster erlernen und Strukturen in den Daten finden.\n\nWas ist ... unüberwachtes Lernen (Unsupervised Learning)?\n\nUnüberwachtes Lernen ist ein Teilgebiet des maschinellen Lernens, bei dem ein\nAlgorithmus versucht, Muster und Strukturen in Daten zu finden. Dabei sind die\nDaten nicht vorab in Eingabe- und Ausgabedaten aufgeteilt bzw. mit Labels\ngekennzeichnet.\n\nEin Kind könnte auch selbstständig einen Obstkorb erkunden. Vielleicht würde das\nKind mit der Zeit lernen, dass es Obst gibt, das ihm schmeckt, wohingegen\nanderes Obst dem Kind nicht schmeckt. Vielleicht würde das Kind das Obst auch in\ngroßes Obst und kleines Obst unterteilen oder nach Farbe sortieren. Das Kind\ngruppiert also Obst nach selbst gewählten Eigenschaften. Es bildet Cluster,\ndementsprechend heißt dieser Vorgang Clustering.\n\nVideo zu “Unüberwachtes Lernen: Clustering” von Plattform Lernende Systeme\n\nVideo zu “ML Tutorial - #4 Unsupervised Learning” von CodingWithMagga","type":"content","url":"/chapter01-sec02#un-berwachtes-lernen-unsupervised-learning","position":11},{"hierarchy":{"lvl1":"1.2 Überwachtes, unüberwachtes und verstärkendes Lernen","lvl2":"Verstärkendes Lernen (Reinforcement Learning)"},"type":"lvl2","url":"/chapter01-sec02#verst-rkendes-lernen-reinforcement-learning","position":12},{"hierarchy":{"lvl1":"1.2 Überwachtes, unüberwachtes und verstärkendes Lernen","lvl2":"Verstärkendes Lernen (Reinforcement Learning)"},"content":"Wir schließen unsere Übersicht der maschinellen Lernverfahren mit dem\nverstärkendem Lernen ab.\n\nWas ist ... verstärkendes Lernen (Reinforcement Learning)?\n\nVerstärkendes Lernen (Reinforcement Learning) ist eine Art des maschinellen\nLernens, bei dem ein ML-Algorithmus durch versuch und Irrtum erlernt, was das\noptimale Verhalten ist, um ein bestimmtes Ziel zu erreichen. Es werden Aktionen\nausgeführt und entweder bestraft oder belohnt, je nachdem, ob durch diese\nAktionen das Ziel besser oder schlechter erreicht wird.\n\nEin Beispiel aus dem Alltag für verstärkendes Lernen ist das Training eines\nHaustieres, eines Hundes beispielsweise. Folgt der Hund dem Befehl »Sitz!«, so\nerhält er ein Leckerli. Mit der Zeit wird der Hund auf das Kommando »Sitz!«\nreagieren und sich setzen, auch wenn es nicht immer eine Belohnung dafür gibt.\n\nEin bekanntes Beispiel aus dem Bereich Künstliche Intelligenz für verstärkendes\nLernen sind Schachsysteme. Anfangs kennt das Schachsystem nur die grundlegenden\nSchachregeln, aber keinerlei Strategie. Durch das Spielen vieler Spiele, wobei\nder Computer bei jedem Sieg eine “Belohnung” erhält und bei jeder Niederlage\neine “Strafe”, lernt das Schachsystem allmählich, welche Züge gewinnbringend\nsind und welche eher zu Niederlagen führen. Nach Tausenden oder sogar Millionen\nvon Spielen kann das Schachsystem dann auf einem sehr hohen Niveau spielen -\nalles durch verstärkendes Lernen.\n\nVideo zu “Verstärkendes Lernen” von Plattform Lernende Systeme\n\nVide zu “ML Tutorial - #5 Reinforcement Learning” von CodingWithMagga","type":"content","url":"/chapter01-sec02#verst-rkendes-lernen-reinforcement-learning","position":13},{"hierarchy":{"lvl1":"1.2 Überwachtes, unüberwachtes und verstärkendes Lernen","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter01-sec02#zusammenfassung-und-ausblick","position":14},{"hierarchy":{"lvl1":"1.2 Überwachtes, unüberwachtes und verstärkendes Lernen","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Abschnitt haben Sie die drei wichtigsten Kategorien des maschinellen\nLernens kennengelernt: überwachtes Lernen, unüberwachtes Lernen und\nverstärkendes Lernen. Für die Ingenieurwissenschaften ist vor allem das\nüberwachte Lernen von Bedeutung. Dabei unterscheiden wir zwischen überwachtem\nLernen für diskrete Ausgabedaten (= Klassen, Kategorien), das wir Klassifikation\nnennen, und überwachtem Lernen für kontinuierliche Ausgabedaten, das wir\nRegression nennen.\n\nDiese Vorlesung richtet sich an Studierende der Ingenieurwissenschaften. Daher\nsteht vor allem die praktische Umsetzung im Fokus. Im nächsten Kapitel werden\nwir daher die technischen Voraussetzungen dafür klären.","type":"content","url":"/chapter01-sec02#zusammenfassung-und-ausblick","position":15},{"hierarchy":{"lvl1":"1.3 Technische Voraussetzungen"},"type":"lvl1","url":"/chapter01-sec03","position":0},{"hierarchy":{"lvl1":"1.3 Technische Voraussetzungen"},"content":"Für maschinelles Lernen ist Python die Programmiersprache der Wahl. Das\nliegt vor allem auch daran, dass Google eine sehr wichtige ML-Bibliothek für\nPython zur Verfügung stellt, die sogenannte Bibliothek\n\n\nTensorFlow. Glücklicherweise müssen wir die\nAlgorithmen nicht selbst in Python umsetzen, sondern können schon fertige\nModelle aus Bibliotheken wie beispielsweise\n\n\nscikit-learn verwenden, die wir\ndann noch an die Daten anpassen müssen. In diesem Kapitel werden die technischen\nVoraussetzungen beschrieben, um maschinelles Lernen mit Python und den\nsogenannten Jupyter Notebooks umzusetzen.","type":"content","url":"/chapter01-sec03","position":1},{"hierarchy":{"lvl1":"1.3 Technische Voraussetzungen","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter01-sec03#lernziele","position":2},{"hierarchy":{"lvl1":"1.3 Technische Voraussetzungen","lvl2":"Lernziele"},"content":"Lernziele\n\nSie haben eine lauffähige Python-Distribution installiert.\n\nSie können JupyterLab starten und ein Jupyter Notebook erzeugen.\n\nSie kennen den prinzipiellen Aufbau eines Jupyter Notebooks mit\nMarkdown-Zellen und Code-Zellen.","type":"content","url":"/chapter01-sec03#lernziele","position":3},{"hierarchy":{"lvl1":"1.3 Technische Voraussetzungen","lvl2":"Was sind Jupyter Notebooks?"},"type":"lvl2","url":"/chapter01-sec03#was-sind-jupyter-notebooks","position":4},{"hierarchy":{"lvl1":"1.3 Technische Voraussetzungen","lvl2":"Was sind Jupyter Notebooks?"},"content":"Die Vorlesung wird in Form von Jupyter Notebooks zur Verfügung gestellt.\nJupyter Notebooks sind interaktive digitale Notizbücher, die sowohl Texte,\nBilder oder Videos enthalten können als auch Python-Code, der direkt im\nNotizbuch ausführbar ist.\n\nDie Kombination von Text, Python-Code und Visualisierungen macht Jupyter\nNotebooks zu einem sehr leistungsstarken Werkzeug für die Datenanalyse. Daten\nkönnen direkt in die Jupyter Notebooks eingegeben oder importiert werden.\nFehlende Daten oder Ausreißer können direkt korrigiert werden, ohne dass mit\neiner externen Software die Korrektur dokumentiert werden muss. Die Ergebnisse\nder Analysen oder ML-Modelle können sofort im Jupyter Notebook dargestellt\nwerden, ohne dass eine externe Anwendung gestartet werden müssen. Daher sind sie\neine der bekanntesten Anwendungen im Bereich Data Science und werden oft zur\nDatenanalyse, maschinellem Lernen und Visualisierung eingesetzt.","type":"content","url":"/chapter01-sec03#was-sind-jupyter-notebooks","position":5},{"hierarchy":{"lvl1":"1.3 Technische Voraussetzungen","lvl2":"Installation Python"},"type":"lvl2","url":"/chapter01-sec03#installation-python","position":6},{"hierarchy":{"lvl1":"1.3 Technische Voraussetzungen","lvl2":"Installation Python"},"content":"Python wird in der Regel mit dem Betriebsystem ausgeliefert. Für maschinelles\nLernen benötigen wir jedoch weitere Python-Module, die die grundlegenden\nFunktionalitäten von Python um ML-Funktionalitäten erweitern. Diese sind\nnormalerweise nicht vorinstalliert, sondern müssen nachinstalliert werden. Bevor\nman sich die Module aus verschiedenen Internetquellen zusammensucht, ist es\neinfacher, eine sogenannte Python-Distribution zu benutzen.\n\nEine Distribution ist eine Zusammenstellung von Software oder\nBibliotheken/Modulen. Die Firma Anaconda, Inc. wurde 2012 mit dem Ziel\ngegründet, Python in Unternehmen speziell für die Geschäftsfeldanalyse (Business\nAnalytics) einzuführen, was die Open Source Community so nicht leisten konnte.\nDaher enthält die Python-Distribution \n\nAnaconda eine\nReihe von nützlichen Paketen und Bibliotheken für wissenschaftliche\nBerechnungen, Datenanalyse, maschinelles Lernen und andere Anwendungen. Da sie\neine Benutzeroberfläche beinhaltet, mit der die Python-Bibliotheken verwaltet\nwerden, ist sie auch gerade für Einsteiger in Python eine gute Wahl.\n\nDie Python-Distribution Anaconda gibt es in verschiedenen Editionen mit\nentsprechenden Preismodellen. Für diese Vorlesung ist die sogenannte »Individual\nEdition« ausreichend, die von Anaconda kostenlos zur Verfügung gestellt wird.\n\nHier ist eine Schritt-für-Schritt-Anleitung zum Installieren von Python mit der\nDistribution Anaconda für Windows und MacOS:\n\nÖffnen Sie die offizielle Anaconda-Website unter\n\n\nhttps://​www​.anaconda​.com​/products​/individual und laden Sie die neueste\nVersion von Anaconda für Ihr Betriebssystem herunter.\n\nFühren Sie die Installationsdatei aus und folgen Sie den Anweisungen auf dem\nBildschirm.\n\nÖffnen Sie nach der Installation das Anaconda-Navigator-Programm, das im\nStartmenü oder Launchpad verfügbar sein sollte.","type":"content","url":"/chapter01-sec03#installation-python","position":7},{"hierarchy":{"lvl1":"1.3 Technische Voraussetzungen","lvl2":"Mit welcher App wird ein Jupyter Notebook bearbeitet?"},"type":"lvl2","url":"/chapter01-sec03#mit-welcher-app-wird-ein-jupyter-notebook-bearbeitet","position":8},{"hierarchy":{"lvl1":"1.3 Technische Voraussetzungen","lvl2":"Mit welcher App wird ein Jupyter Notebook bearbeitet?"},"content":"Es gibt mehrere Applikationen, die Jupyter Notebooks bearbeiten können. Am\nbekanntesten ist sicherlich \n\nJupyterLab, das wir auch in\ndieser Vorlesung verwenden. Neben JupyterLab gibt es aber auch weitere\nMöglichkeiten, um Jupyter Notebooks zu bearbeiten.\n\nDie beiden Entwicklungsumgebungen\n\nPyCharm\n\nMicrosoft Visual Studio Code\n\nermöglichen ebenfalls die direkte Bearbeitung von Jupyter Notebooks. Auch\nzahlreiche Cloudanbieter bieten direkt das Bearbeiten und Ausführen von Jupyter\nNotebooks an, z.B.\n\nGoogle Colab\n\nMicrosoft Azure\n\nDeepnote\n\nreplit\n\nWie bei allen Clouddiensten sollte man sich jedoch eingehend mit den\nDatenschutzbestimmungen des Anbieters vertraut machen, bevor man den Dienst in\nAnspruch nimmt. Aufgrund des Datenschutzes empfehle ich stets, Python/Anaconda\nlokal zu installieren.","type":"content","url":"/chapter01-sec03#mit-welcher-app-wird-ein-jupyter-notebook-bearbeitet","position":9},{"hierarchy":{"lvl1":"1.3 Technische Voraussetzungen","lvl2":"Start von JupyterLab und das erste Jupyter Notebook"},"type":"lvl2","url":"/chapter01-sec03#start-von-jupyterlab-und-das-erste-jupyter-notebook","position":10},{"hierarchy":{"lvl1":"1.3 Technische Voraussetzungen","lvl2":"Start von JupyterLab und das erste Jupyter Notebook"},"content":"Anaconda installiert JupyterLab automatisch mit, so dass wir direkt loslegen\nkönnen. Sollte es Probleme geben, finden Sie hier die \n\nDokumentation von\nJupyterLab.\n\nJupyterLab startet im Hintergrund einen sogenannten Jupyter-Kernel, der die\ninteraktiven Python-Code-Zellen ausführt. Der Client ist in der Regel der\nStandard-Browser.\n\nDie folgende Schritt-für-Schritt-Anleitung zeigt, wie ein neues Jupyter Notebook\nin JupyterLab erstellt wird.\n\nUm ein neues Jupyter Notebook zu erstellen, klicken Sie auf “Home” im\nAnaconda-Navigator und wählen “JupyterLab” aus. Alternativ können Sie\nJupyterLab auch mit dem Befehl “jupyter-lab” aus einem Terminal oder einer\nKonsole starten (Linux oder MacOS).\n\nWählen Sie “Python 3 (ipykernel)” aus, um ein neues Notebook zu erstellen.\n\nSie können jetzt Python-Code in dem Notebook schreiben und ausführen. Wenn\nSie zusätzliche Pakete benötigen, können Sie diese über den\n“Environments”-Tab im Anaconda-Navigator installieren.\n\n\n\nFigure 1:Startansicht der Software JupyterLab: ein neues Jupyter Notebook wird mit Klick auf den Button Python 3 (ipykernel) erstellt.","type":"content","url":"/chapter01-sec03#start-von-jupyterlab-und-das-erste-jupyter-notebook","position":11},{"hierarchy":{"lvl1":"1.3 Technische Voraussetzungen","lvl2":"Grundlegende Funktionalitäten von Jupyter Notebooks"},"type":"lvl2","url":"/chapter01-sec03#grundlegende-funktionalit-ten-von-jupyter-notebooks","position":12},{"hierarchy":{"lvl1":"1.3 Technische Voraussetzungen","lvl2":"Grundlegende Funktionalitäten von Jupyter Notebooks"},"content":"Ein Jupyter Notebook besteht aus einer Abfolge von Zellen, in denen Text, Code\nund Visualisierungen eingebettet werden. Die Zellen können entweder in der\nProgrammiersprache Python oder in einer Reihe anderer Programmiersprachen wie R,\nJulia oder JavaScript geschrieben werden. Erkennbar sind Jupyter Notebooks an\nder Dateiendung ipynb, die die Abkürzung für »intelligentes Python\nNotebook« darstellt.\n\n\n\nFigure 2:Screenshot eines Jupyter Notebooks mit einer nicht ausgeführten Markdown-Zelle (1), einer ausgeführten Code-Zelle (2) und dem Run-Button (3)\n\nWie im obigen Screenshot zu sehen, sind Zellen mit einem Rahmen versehen. Eine\nZelle kann entweder eine Text-Zelle (1) oder eine Code-Zelle (2) sein. In\nText-Zellen wird die sogenannte\n\n\nMarkdown-Formatierung benutzt, weshalb\nsie Markdown-Zellen genannt werden. Bei dieser Art, Text zu formatieren,\nwerden Textzeichen benutzt anstatt auf einen Button zu klicken. Um\nbeispielsweise ein Wort fettgedruckt anzuzeigen, werden zwei Sternchen ** vor\nund hinter das Wort gesetzt, also ich bin **fett** gedruckt.\n\nIn Code-Zellen (2) können Sie direkt Python-Code eingeben. Sie erkennen eine\nCode-Zelle daran, dass eckige Klammern links daneben stehen. Eine Code-Zelle\nwird ausgeführt, indem Sie auf “Run” klicken. Der Run-Button verbirgt sich\nhinter dem kleinen nach rechts gerichteten Dreick in der Menü-Leiste des Jupyter\nNotebooks (3). Danach erscheint die Ausgabe, die der Python-Interpreter ggf.\nproduziert. Wird eine Code-Zelle ausgeführt, so erscheint eine Zahl in den\neckigen Klammern. Diese Zahl zeigt die Reihenfolge an, in der Code-Zellen\nausgeführt wurden.","type":"content","url":"/chapter01-sec03#grundlegende-funktionalit-ten-von-jupyter-notebooks","position":13},{"hierarchy":{"lvl1":"1.3 Technische Voraussetzungen","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter01-sec03#zusammenfassung-und-ausblick","position":14},{"hierarchy":{"lvl1":"1.3 Technische Voraussetzungen","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Kapitel haben wir uns mit den technischen Voraussetzungen für\nmaschinelles Lernen mit Python beschäftigt. Zum Standardwerkzeug im Bereich des\nmaschinellen Lernens sind die Jupyter Notebooks geworden, die wir in den\nnächsten Kapiteln immer besser kennenlernen werden. Im nächsten Part werden wir\nim Schnelldurchlauf Python wiederholen.","type":"content","url":"/chapter01-sec03#zusammenfassung-und-ausblick","position":15},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()"},"type":"lvl1","url":"/chapter02-sec01","position":0},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()"},"content":"Beim maschinellen Lernen geht es um Daten und Algorithmen. Dabei können die\nDaten alles Mögliche umfassen, beispielsweise Zahlen oder Texte. Daher\nbeschäftigen wir uns zuerst mit Datentypen. Dann geht es um Variablen und deren\nAusgabe auf dem Bildschirm.","type":"content","url":"/chapter02-sec01","position":1},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter02-sec01#lernziele","position":2},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl2":"Lernziele"},"content":"Lernziele\n\nSie kennen die einfachen Datentypen:\n\nInteger\n\nFloat\n\nString\n\nSie wissen, was eine Variable ist und kennen den Zuweisungsoperator.\n\nSie können die print()-Funktion zur Ausgabe auf dem Bildschirm anwenden.","type":"content","url":"/chapter02-sec01#lernziele","position":3},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl2":"Einfache Datentypen"},"type":"lvl2","url":"/chapter02-sec01#einfache-datentypen","position":4},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl2":"Einfache Datentypen"},"content":"Beim maschinellen Lernen geht es um die Sammlung, Erkundung und Analyse von\nDaten, um Antworten auf vorgegebene Fragen zu finden.  Der Computer kann\nInformationen aber nur als 0 und 1 verarbeiten. Auf dem Speichermedium oder im\nSpeicher selbst werden Daten daher als eine Folge von 0 und 1 gespeichert. Damit\nes für uns Programmierinnen und Programmierer einfacher wird, Daten zu speichern\nund zu verarbeiten, wurden Datentypen eingeführt.\n\nDatentypen fassen gleichartige Objekte zusammen und stellen den\nProgrammiererinnen und Programmierern passende Operationen zur Verfügung. Mit\nOperationen sind die Aktionen gemeint, die mit diesen Datenobjekten durchgeführt\nwerden dürfen. Zahlen dürfen beispielsweise addiert werden, Buchstaben aber\nnicht. Es hängt von der Programmiersprache ab, welche Datentypen zur Verfügung\nstehen, wie diese im Hintergrund gespeichert werden und welche Operationen\nmöglich sind.\n\nIn diesem Kapitel beschäftigen wir uns mit den einfachen Datentypen\n\nInteger,\n\nFloat und\n\nString.","type":"content","url":"/chapter02-sec01#einfache-datentypen","position":5},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl3":"Integer und Float","lvl2":"Einfache Datentypen"},"type":"lvl3","url":"/chapter02-sec01#integer-und-float","position":6},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl3":"Integer und Float","lvl2":"Einfache Datentypen"},"content":"In der Programmierung unterscheidet man grundsätzlich zwischen zwei Zahlenarten,\nden ganzen Zahlen und den Fließkommazahlen. In der Informatik wird jedoch\nder englische Begriff dafür verwendet: Integer.\n\nMit Integern können wir ganz normal rechnen, also arithmetische Operationen\nausführen:\n\n2+3\n\n\n\n2*3\n\n\n\n6-7\n\n\n\n3*(4+7)\n\n\n\n25/5\n\n\n\n4**2\n\n\n\nMit einer Operation verlassen wir aber bereits den Bereich der ganzen Zahlen,\nden Bereich der Integer. 25/5 ist wieder eine ganze Zahl, nicht jedoch\n25/3. Damit sind wir bei den Fließkommazahlen. Auch hier wird üblicherweise\nder englische Begriff Float für Fließkommazahl verwendet.\n\n2.3 + 4.5\n\n\n\n5.6 - 2.1\n\n\n\n2.1 * 3.5\n\n\n\n3.4 / 1.7\n\n\n\n3.4 ** 2\n\n\n\n3.5 * (2.6 - 3.8 / 1.9)\n\n\n\nHinweis\n\nVerwenden Sie stets einen Punkt als Dezimaltrennzeichen, nicht ein\nKomma!\n\nDas folgende Video gibt eine Einführung in das Thema »Zahlen mit Python«.\n\nVideo zu “Zahlen in Python” von Programmieren lernen","type":"content","url":"/chapter02-sec01#integer-und-float","position":7},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl3":"String","lvl2":"Einfache Datentypen"},"type":"lvl3","url":"/chapter02-sec01#string","position":8},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl3":"String","lvl2":"Einfache Datentypen"},"content":"Daten sind aber sehr oft keine Zahlen. Beispielsweise könnte man sich\nvorstellen, eine Einkaufsliste zu erstellen und diese im Computer oder in einer\nNotiz-App auf dem Handy zu speichern. Eine solche Zeichenkette heißt in der\nInformatik String. Mit Zeichen meint man dabei Zahlen, Buchstaben oder\nandere Zeichen wie beispielsweise !\"§$%&/()=?.\n\nStrings werden in Python durch einfache Hochkomma oder Anführungszeichen\ndefiniert.\n\n'Dies ist ein String!'\n\n\n\nMit Strings kann man ebenfalls “rechnen”, nur ist das Ergebnis vielleicht anders\nals erwartet.\n\n2 * 'Dies ist ein String!'\n\n\n\n'String 1 ' + 'String 2'\n\n\n\nDas folgende Video gibt eine Einführung zum Thema »Strings in Python«.\n\nVideo zu “Strings in Python” von Programmieren lernen","type":"content","url":"/chapter02-sec01#string","position":9},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl2":"Variablen und Zuweisung"},"type":"lvl2","url":"/chapter02-sec01#variablen-und-zuweisung","position":10},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl2":"Variablen und Zuweisung"},"content":"Variablen sind beschriftete Schubladen. Oder anders formuliert sind\nVariablen Objekte, denen man einen Namen gibt. Technisch gesehen sind diese\nSchubladen ein kleiner Bereich im Arbeitsspeicher des Computers. Der Inhalt der\nSchubladen kann sehr unterschiedlich sein. Beispielsweise können in den\nSchubladen die Telefonnummer des ADAC-Pannendienstes, die 10. Nachkommastelle\nvon \\pi oder die aktuelle Position des Mauszeigers enthalten sein.","type":"content","url":"/chapter02-sec01#variablen-und-zuweisung","position":11},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl3":"Zuweisungen sind keine Gleichungen","lvl2":"Variablen und Zuweisung"},"type":"lvl3","url":"/chapter02-sec01#zuweisungen-sind-keine-gleichungen","position":12},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl3":"Zuweisungen sind keine Gleichungen","lvl2":"Variablen und Zuweisung"},"content":"Wir verwenden Variablen, um bestimmte Werte oder ein bestimmtes Objekt zu\nspeichern. Eine Variable wird durch eine Zuweisung erzeugt. Damit meinen\nwir, dass eine Schublade angelegt wird und die Schublade dann erstmalig gefüllt\nwird. Das erstmalige Füllen der Schublade nennt man in der Informatik auch\nInitialisieren. Für die Zuweisung wird in Python das =-Zeichen verwendet.\n\nx = 0.5\n\n\n\nSobald die Variable x in diesem Beispiel durch eine Zuweisung von 0.5 erstellt\nwurde, können wir sie verwenden:\n\nx * 3\n\n\n\nx + 17\n\n\n\nWichtig ist, dass das = in der Informatik eine andere Bedeutung hat als in der\nMathematik. = meint nicht das Gleichheitszeichen, sondern den sogenannten\nZuweisungsoperator. Das ist in der Programmierung ein Kommando, das eine\nSchublade befüllt oder technischer ausgedrückt, ein Objekt einer Variable\nzuweist.\n\nVariablen müssen initalisiert (erstmalig mit einem Wert versehen) werden, bevor\nsie verwendet werden können, sonst tritt ein Fehler auf.\n\nMini-Übung\n\nFügen Sie eine Code-Zelle ein und schreiben Sie in die Code-Zelle einfach nur n. Lassen Sie die Code-Zelle ausführen. Was passiert?\n\nLösung\n\nEs erscheint eine Fehlermeldung, da eine Variable einen Wert haben muss, bevor\nsie das erste Mal benutzt wird.\n\nSehr häufig findet man Code wie\n\nx = x + 1\n\n\n\nWürden wir dies als Gleichung lesen, wie wir es aus der Mathematik gewohnt sind,\nx = x + 1, könnten wir x auf beiden Seiten subtrahieren und erhalten 0 = 1. Wir\nwissen, dass dies nicht wahr ist, also stimmt hier etwas nicht.\n\nIn Python sind Gleichungen keine mathematischen Gleichungen, sondern\nZuweisungen. “=” ist kein Gleichheitszeichen im mathematischen Sinne, sondern\neine Zuweisung. Die Zuweisung muss immer in der folgenden Weise zweistufig\ngelesen werden:\n\nBerechne den Wert auf der rechten Seite (also x+1).\n\nWeise den Wert auf der rechten Seite dem auf der linken Seite stehenden\nVariablennamen zu (in Python-Sprechweise: binde dem Namen auf der linken\nSeite an das auf der rechten Seite angezeigte Objekt).\n\nx = 4     \nx = x + 1\nx\n\n\n\n","type":"content","url":"/chapter02-sec01#zuweisungen-sind-keine-gleichungen","position":13},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl3":"Richtlinien für Variablennamen","lvl2":"Variablen und Zuweisung"},"type":"lvl3","url":"/chapter02-sec01#richtlinien-f-r-variablennamen","position":14},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl3":"Richtlinien für Variablennamen","lvl2":"Variablen und Zuweisung"},"content":"Früher war der Speicherplatz von Computern klein, daher wurden häufig nur kurze\nVariablennamen wie beispielsweise i oder N verwendet. Heutzutage ist es\nStandard, nur in Ausnahmefällen (z.B. in Schleifen, dazu kommen wir noch) kurze\nVariablennamen zu nehmen. Stattdessen werden Namen benutzt, bei denen man\nerraten kann, was die Variable für einen Einsatzzweck hat. Beispielsweise lässt\nder Code\n\nm = 0.19\nn = 80\nb = n + m*n\nb\n\n\n\nnur schwer vermuten, was damit bezweckt wird. Dagegen erahnt man bei diesem Code\nschon eher, was bezweckt wird:\n\nmehrwertsteuersatz = 19/100\nnettopreis = 80\nbruttopreis = nettopreis + mehrwertsteuersatz * nettopreis\nbruttopreis\n\n\n\nCode-Zellen zeigen arithmetische Operationen direkt nach Ausführen der\nCode-Zelle an. Wenn allerdings in der Code-Zelle mehrere Python-Anweisungen\nsind, müssen wir in die letzte Zeile nochmal die Variable selbst hinschreiben,\nderen Wert angezeigt werden soll. Das ist etwas umständlich und funktioniert\nauch nur mit Jupyter Notebooks. Normalerweise gibt es dazu eine Python-Funktion\nprint(), auf die wir später noch zurückkommen.\n\nVerwenden Sie für Variablennamen nur ASCII-Zeichen, also keine Umlaute wie ö, ü\noder ß. Python erlaubt es zwar, Umlaute in Variablennamen zu verwenden, es ist\naber gute Praxis, dies nicht zu tun. Zahlen sind erlaubt, aber nicht am Anfang\ndes Namens. Es ist sinnvoll, lange Variablen durch einen Unterstrich besser\nlesbar zu gestalten (sogenannte Snake-Case-Formatierung). Ich empfehle für\nVariablennamen beispielsweise\n\ndateiname_alt oder dateiname_neu\n\nwenn beispielsweise eine Datei umbenannt wird. Sie sind frei in der Gestaltung\nder Variablennamen, verboten sind nur die sogannnten Schlüsselwörter.\n\nDie folgenden beiden Videos fassen die beiden Themen Variablen und Zuweisungen\nnochmals zusammen.\n\nVideo zu “Variablen in Python” von Programmieren lernen\n\nVideo zu “Zuweisungen in Python” von Programmieren lernen","type":"content","url":"/chapter02-sec01#richtlinien-f-r-variablennamen","position":15},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl3":"Datentypen ermitteln mit type()","lvl2":"Variablen und Zuweisung"},"type":"lvl3","url":"/chapter02-sec01#datentypen-ermitteln-mit-type","position":16},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl3":"Datentypen ermitteln mit type()","lvl2":"Variablen und Zuweisung"},"content":"Werden zwei Integer geteilt, so wird das Ergebnis automatisch in einen Float\numgewandelt. Mit Hilfe der Funktion type() können wir den Python-Interpreter\nbestimmen lassen, welcher Datentyp in einer Variable gespeichert ist. Mit\nFunktion ist hier keine mathematische Funktion gemeint. Eine Funktion sind\nviele Anweisungen nacheinander, um eine bestimmte Teilaufgabe zu lösen. Damit\nklar ist, dass es sich um eine Funktion und nicht um eine Variable handelt,\nwerden runde Klammern an den Namen der Funktion gehängt.\n\nIn diesem Fall soll der Datentyp eines Objektes ermittelt werden. Damit der\nPython-Interpreter weiß, von welcher Variable der Datentyp ermittelt werden\nsoll, schreiben wir die Variable in runde Klammern.\n\nx = 25 * 5\ntype(x)\n\n\n\nx = 25 / 5\ntype(x)\n\n\n\nNicht immer ist es aber möglich, Datentypen zu mischen. Dann meldet Python einen\nFehler.\n\nDas folgende Video fasst die Datentypen Integer, Float und String nochmal zusammen.\n\nVideo zu “Datentypen in Python” von Programmieren lernen","type":"content","url":"/chapter02-sec01#datentypen-ermitteln-mit-type","position":17},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl2":"Ausgaben mit print()"},"type":"lvl2","url":"/chapter02-sec01#ausgaben-mit-print","position":18},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl2":"Ausgaben mit print()"},"content":"Jetzt lernen Sie eine weitere Python-Funktion kennen. Bei den obigen\nRechenaufgaben wurde automatisch das Ergebnis der Rechnung angezeigt, sobald die\nCode-Zelle ausgeführt wurde. Dies ist eine Besonderheit der Jupyter Notebooks,\nwürde aber in einem normalen Python-Programm nicht funktionieren. Auch möchte\nman vielleicht ein Zwischenergebnis anzeigen lassen. Die interaktive Ausgabe der\nJupyter Notebooks zeigt jedoch immer nur den Inhalt der letzten Zeile an.\n\nFür die Anzeige von Rechenergebnissen oder Texten gibt es in Python die\nprint()-Funktion. Die print()-Funktion in Python gibt den Wert am Bildschirm\naus, der ihr als sogenanntes Argument in den runden Klammern übergeben wird.\nDas kann zum Beispiel eine Zahl oder eine Rechenaufgabe sein, wie in dem\nfolgenden Beispiel.\n\nprint(2)\nprint(3+3)\n\n\n\nIn der ersten Zeile ist das Argument für die print()-Funktion die Zahl 2. Das\nArgument wird in runde Klammern hinter den Funktionsnamen print geschrieben.\nEin Argument ist sozusagen der Input, der an die print()-Funktion übergeben\nwird, damit der Python-Interpreter weiß, welcher Wert auf dem Bildschirm\nangezeigt werden soll.\n\nDas zweite Beispiel in der zweiten Zeile funktioniert genauso. Nur wird diesmal\neine komplette Rechnung als Argument an die print()-Funktion übergeben. In dem\nFall rechnet der Python-Interpreter erst den Wert der Rechnung, also 3+3=6 aus\nund übergibt dann die 6 an die print()-Funktion. Die print()-Funktion wiederum\nzeigt dann die 6 am Bildschirm an.\n\nMini-Übung\n\nLassen Sie Python den Term 3:4 berechnen und geben Sie das Ergebnis mit der print()-Funktion aus.\n\n# Geben Sie nach diesem Kommentar Ihren Code ein:\n\n\n\nLösungprint(3/4)\n\nPython kann mit der print()-Funktion jedoch nicht nur Zahlen ausgeben, sondern\nauch Texte, also Strings.\n\nprint('Hallo')\n\n\n\nMini-Übung\n\nProbieren Sie aus was passiert, wenn Sie die einfachen Anführungszeichen '\ndurch doppelte Anführungszeichen \" ersetzen. Lassen Sie den Text Hallo Welt\nausgeben :-)\n\n# Geben Sie nach diesem Kommentar Ihren Code ein:\n\n\n\nLösungprint(\"Hallo Welt\")\n\nZum Schluss behandeln wir noch formatierte Strings, die sogenannten f-Strings.\nSeit Python 3.6 erleichtert dieser Typ von String die Programmierung. Falls Sie\nPython-Code sehen, in dem Prozentzeichen vorkommen (ganz, ganz alt) oder die\n.format()-Methode benutzt wird, wundern Sie sich nicht. In dieser Vorlesung\nverwenden wir jedoch f-Strings.\n\nf-Strings sind die Abkürzung für “formatted string literals”. Sie\nermöglichen es, den Wert einer Variable oder einen Ausdruck direkt in den String\neinzubetten. Dazu werden geschweifte Klammern verwendet, also { und } und zu\nBeginn des Strings wird ein f eingefügt, um aus dem String einen f-String zu\nmachen. Der Python-Interpreter fügt dann zur Laufzeit des Programms den\nentsprechenden Wert der Variable in den String ein.\n\nHier ein Beispiel:\n\nname = 'Alice'\nalter = 20\nprint(f'Mein Name ist {name} und ich bin {alter} Jahre alt.')\n\n\n\nInsbesondere bei Ausgabe von Zahlen sind f-Strings besonders nützlich. Wenn nach\ndem Variablennamen ein Doppelpunkt eingefügt wird, kann danach die Anzahl der\ngewünschten Stellen vor dem Komma (hier natürlich ein Punkt) und der\nNachkommastellen festgelegt werden. Zusätzlich setzen wir ein f in die\ngeschweiften Klammern, um einen Float anzeigen zu lassen. Im folgenden Beispiel\ngeben wir \\pi auf zwei Nachkommastellen an.\n\npi = 3.141592653589793238462643383279\nprint(f'Pi = {pi:1.2f}')\n\n\n\nEs ist schwierig, sich alle Formatierungsoptionen zu merken. Auf der\nInternetseite\n\n\nhttps://​cheatography​.com​/brianallan​/cheat​-sheets​/python​-f​-strings​-basics/\nfinden Sie eine umfangreiche Übersicht und können sich zudem ein pdf-Dokument\nherunterladen.\n\nMini-Übung\n\nSchreiben Sie ein Programm, mit dem der Flächeninhalt eines Rechtecks berechnet werden soll. Die beiden Seitenlängen werden jeweils in den Variablen laenge und breite gespeichert (suchen Sie sich eigene Zahlen aus). Ausgegeben werden soll dann: “Der Flächeninhalt eines Rechtecks mit den Seiten XX und XX ist XX.”, wobei XX durch die korrekten Zahlen ersetzt werden und der Flächeninhalt auf eine Nachkommastelle gerundet werden soll.\n\nLösung# Eingabe\nlaenge = 5.5\nbreite = 6.3\n\n# Verarbeitung\nflaeche = laenge * breite\n\n# Ausgabe \nprint(f'Der Flächeninhalt eines Rechtecks mit den Seiten {laenge} und {breite} ist {flaeche:.1f}.')","type":"content","url":"/chapter02-sec01#ausgaben-mit-print","position":19},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter02-sec01#zusammenfassung-und-ausblick","position":20},{"hierarchy":{"lvl1":"2.1 Datentypen, Variablen und print()","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Kapitel haben wir gelernt, was ein Datentyp ist, wie eine Variable mit\neinem Wert gefüllt wird und mit der print()-Funktion am Bildschirm ausgegeben\nwird. Die einfachsten Datentypen Integer, Float und String reichen allerdings\nnicht aus, um z.B. die eine Adresse mit Straße (String), Hausnummer (Integer)\nund Postleitzahl (Integer) in einer Variablen gemeinsam zu speichern. Dazu\nlernen wir im nächsten Abschnitt den Datentyp Liste kennen.","type":"content","url":"/chapter02-sec01#zusammenfassung-und-ausblick","position":21},{"hierarchy":{"lvl1":"2.2 Listen und for-Schleifen"},"type":"lvl1","url":"/chapter02-sec02","position":0},{"hierarchy":{"lvl1":"2.2 Listen und for-Schleifen"},"content":"Bisher haben wir drei verschiedene Datentypen kennengelernt:\n\nInteger (ganze Zahlen),\n\nFloats (Fließkommazahlen) und\n\nStrings (Zeichenketten).\n\nDamit können wir einzelne Objekte der realen Welt ganz gut abbilden. Mit einem\nString können wir den Namen einer Person erfassen, mit einem Integer das Alter\nder Person und mit einem Float die Körpergröße der Person gemessen in Meter. Was\nuns aber bisher fehlt ist eine Sammlung von Namen oder eine Sammlung von\nKörpergrößen. Daher werden wir uns in diesem Kapitel mit dem Datentyp Liste\nbeschäftigen.\n\nOft kommt es vor, dass für jedes Element der Liste bestimmte Aktionen\ndurchgeführt werden sollen. Daher werden wir uns auch mit der Wiederholung von\nCode-Abschnitten mittels der sogenannten for-Schleife beschäftigen.","type":"content","url":"/chapter02-sec02","position":1},{"hierarchy":{"lvl1":"2.2 Listen und for-Schleifen","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter02-sec02#lernziele","position":2},{"hierarchy":{"lvl1":"2.2 Listen und for-Schleifen","lvl2":"Lernziele"},"content":"Lernziele\n\nSie kennen den Datentyp Liste.\n\nSie können Listen mit eckigen Klammern erzeugen.\n\nSie können Listen mit dem Plus-Operator verketten und Elemente mit\n.append() anhängen.\n\nSie können über den Index auf einzelne Listenelemente zugreifen.\n\nSie können eine for-Schleife mit Liste programmieren.\n\nSie wissen, wie die Fachbegriffe der einzelnen Bestandteile der Schleife\nlauten:\n\nKopfzeile, wird mit Doppelpunkt : abgeschlossen\n\nSchlüsselwörter for und in\n\nSchleifenvariable\n\nSie wissen, dass der Anweisungsblock des Schleifeninneren eingerückt werden\nmuss. Die Einrückung muss immer mit der gleichen Anzahl von Zeichen\n(Leerzeichen oder Tab) erfolgen.","type":"content","url":"/chapter02-sec02#lernziele","position":3},{"hierarchy":{"lvl1":"2.2 Listen und for-Schleifen","lvl2":"Datentyp Liste"},"type":"lvl2","url":"/chapter02-sec02#datentyp-liste","position":4},{"hierarchy":{"lvl1":"2.2 Listen und for-Schleifen","lvl2":"Datentyp Liste"},"content":"Eine Liste ist eine Sammlung von Objekten. Dabei können die Objekte einen\nbeliebigen Datentyp aufweisen. Eine Liste wird durch eckige Klammern erzeugt.\n\nBeispielsweise könnte eine Liste drei Integer enthalten:\n\na = [34, 12, 54]\nprint(a)\n\n\n\nDas folgende Beispiel zeigt eine Liste mit vier Namen, die durch Strings\nrepräsentiert werden:\n\na = ['Alice', 'Bob', 'Charlie', 'Dora']\nprint(a)\n\n\n\nEine leere Liste wird durch [] definiert:\n\na = []\nprint(a)\n\n\n\nListen können gekürzt und erweitert werden. Eine sehr nützliche Funktion ist\ndaher die len()-Funktion. Das len steht dabei für length. Wird die\nFunktion len() mit einer Liste (oder mit einem String) als Argument\naufgerufen, gibt sie die Anzahl der Listenelemente (oder Anzahl der Zeichen im\nString) zurück.\n\na = ['Hund', 'Katze', 'Maus', 'Affe','Elefant']\nlen(a)\n\n\n\nListen müssen nicht nur Elemente eines Datentyps enthalten. In Python ist es\nerlaubt, in eine Liste Objekte mit verschiedenen Datentypen zu sammeln. Das\nfolgende Beispiel zeigt eine Mischung aus Elementen der drei Datentypen Integer,\nFloat und String.\n\na = [123, 'Ente', -42, 17.4, 0, 'Elefant']\nprint(a)\n\n\n\nMini-Übung\n\nErzeugen Sie eine Einkaufsliste, um einen Obstsalat zuzubereiten und speichern\nSie diese Liste in der Variablen einkaufsliste. Lassen Sie dann den Computer\nbzw. den Python-Interpreter zählen, wie viele Zutaten Ihre Liste enthält und\ngeben Sie dann die Anzahl aus.\n\n# Hier Ihr Code:\n\n\n\nLösungeinkaufsliste = ['Apfel', 'Banane', 'Trauben', 'Joghurt']\nanzahl_zutaten = len(einkaufsliste)\nprint(anzahl_zutaten)","type":"content","url":"/chapter02-sec02#datentyp-liste","position":5},{"hierarchy":{"lvl1":"2.2 Listen und for-Schleifen","lvl2":"Listen bearbeiten"},"type":"lvl2","url":"/chapter02-sec02#listen-bearbeiten","position":6},{"hierarchy":{"lvl1":"2.2 Listen und for-Schleifen","lvl2":"Listen bearbeiten"},"content":"Listen sind in Python veränderlich. Besonders häufig kommt es vor, dass zwei\nListen zu einer neuen Liste kombiniert werden sollen. Da diese Aktion so wichtig\nist, kann dies in Python direkt mit dem +-Operator erledigt werden. Der\nFachbegriff für das Aneinanderhängen von Listen ist Verkettung oder auf\nEnglisch Concatenation.\n\na = [37, 3, 5] + [3, 35, 100]\nprint(a)\n\n\n\nUm an das Ende der Liste ein neues Element einzufügen, verwendet man die Methode\nappend(). Eine Methode ist eine spezielle Funktion, die zu dem Datentyp\ngehört und daher an die Variable angehängt wird, indem man einen Punkt schreibt\nund dann den Methodennamen.\n\na = [34, 56, 23]\nprint(a)\n\na.append(42)\nprint(a)\n\n\n\nMini-Übung\n\nNehmen Sie Ihre Einkaufsliste für den Obstsalat von vorhin. Fügen Sie noch Zimt\nund Zucker hinzu und lassen Sie die Anzahl der Elemente ausgeben.\n\n# Hier Ihr Code:\n\n\n\nLösungeinkaufsliste = ['Apfel', 'Banane', 'Trauben', 'Joghurt']\neinkaufsliste.append('Zimt')\neinkaufsliste.append('Zucker')\nanzahl = len(einkaufsliste)\nprint(f'Anzahl: {anzahl}')\n\nVideo zu “Listen in Python - Einführung” von Programmieren lernen","type":"content","url":"/chapter02-sec02#listen-bearbeiten","position":7},{"hierarchy":{"lvl1":"2.2 Listen und for-Schleifen","lvl2":"Zugriff auf einzelne Listenelemente"},"type":"lvl2","url":"/chapter02-sec02#zugriff-auf-einzelne-listenelemente","position":8},{"hierarchy":{"lvl1":"2.2 Listen und for-Schleifen","lvl2":"Zugriff auf einzelne Listenelemente"},"content":"Listen sind in Python durchnummeriert, beginnend bei Index 0. Die\nPositionsnummer eines Elements nennt man Index. Um auf ein einzelnes Element\nzuzugreifen, schreibt man liste[i], wobei i der Index ist. Um einfach auf\ndas letzte Element einer Liste zugreifen zu können, hat Python den Index -1\neingeführt.\n\nProbieren wir ein Beispiel aus:\n\na = [34, 56, 23, 42]\nerstes = a[0]\nprint(f'Das erste Element in der Liste ist: {erstes}')\n\nletztes = a[-1]\nprint(f'Das letzte Element in der Liste ist: {letztes}')\n\n\n\n# Erzeugung Liste\nmeine_liste = ['rot', 'grün', 'blau', 'gelb', 'weiß', 'schwarz']\n\n# das fünfte Element weiß wird durch lila ersetzt\nmeine_liste[4] = 'lila'\nprint(meine_liste)\n\n\n\nDas Bearbeiten von einzelnen Listenelementen wird auch Zugriff genannt. Das\nfolgende Video zeigt die Zugriffsmöglichkeiten von Listen.\n\nVideo zu “Zugriff aus Listen” von Programmieren lernen","type":"content","url":"/chapter02-sec02#zugriff-auf-einzelne-listenelemente","position":9},{"hierarchy":{"lvl1":"2.2 Listen und for-Schleifen","lvl2":"Code wiederholen mit der for-Schleife"},"type":"lvl2","url":"/chapter02-sec02#code-wiederholen-mit-der-for-schleife","position":10},{"hierarchy":{"lvl1":"2.2 Listen und for-Schleifen","lvl2":"Code wiederholen mit der for-Schleife"},"content":"Wenn wir mit jedem Element einer Liste etwas tun wollen, wäre es mühsam, jedes\nElement einzeln über seinen Index anzusprechen. Hierfür gibt es die\nfor-Schleife.\n\nfor i in [2, 4, 6, 8, 10]:\n    print(i)\n\n\n\nEine Schleife beginnt mit dem Schlüsselwort for. Danach kommt der Name der\nsogenannten Schleifenvariable, in diesem Fall also i. Als nächstes folgt\nwieder ein Schlüsselwort, nämlich in und zuletzt Liste. Diese Zeile nennt\nman Kopfzeile.\n\nPython muss wissen, welche Kommandos für jeden Schleifendurchgang ausgeführt\nwerden sollen. Daher wird die Kopfzeile der Schleife mit einem Doppelpunkt :\nbeendet. Danach werden alle Kommandos aufgelistet, die ausgeführt werden sollen.\nDamit Python weiß, wann es wieder mit dem normalen Programm weitergehen soll,\nmüssen wir dem Python-Interpreter das Ende der Schleife signalisieren. In vielen\nProgrammiersprachen wird das mit dem Schlüsselwort end gemacht oder es werden\nKlammern gesetzt. In Python wird stattdessen mit Einrückung gearbeitet. Alle\nZeilen mit Anweisungen, die eingerückt sind, werden in der Schleife wiederholt.\n\nWie sieht das nun bei unserem Beispiel aus? Die Schleifenvariable heißt i. Sie\nnimmt beim 1. Schleifendurchgang den Wert 2 an. Dann werden die Anweisungen im\nSchleifeninneren ausgeführt, also die print()-Funktion für i = 2 angewendet\nund eine 2 ausgegeben. Dann wird die Schleife ein 2. Mal durchlaufen. Diesmal\nnimmt die Schleifenvariable i den Wert 4 an und die print()-Funktion gibt 4\naus. Das geht so weiter bis zum 5. Schleifendurchgang, wo die Schleifenvariable\nden Wert i = 10 annimmt und eine 10 auf dem Bildschirm angezeigt wird. Da die\n10 das letzte Element der Liste war, macht der Python-Interpreter mit dem\nnormalen Programm weiter. Bei unserem kurzen Beispiel ist aber schon das Ende\ndes Programmes erreicht. Zusammengefasst, werden nacheinander die Elemente der\nListe [2, 4, 6, 8, 10] auf dem Bildschirm ausgegeben.\n\nSchauen wir uns ein weiteres Beispiel an. Jedes Element der Liste\n[4,5,7,11,21] soll um 2 erhöht werden.\n\nfor zahl in [4,5,7,11,21]:\n    ergebnis = zahl + 2\n    print(f'Wenn ich {zahl} + 2 rechne, erhalte ich {ergebnis}.')\nprint('Ich bin fertig!')\n\n\n\nMini-Übung\n\nLassen Sie nacheinander die Zutaten Ihrer Einkaufsliste ausgeben.\n\n# Hier Ihr Code:\n\n\n\nLösungeinkaufsliste = ['Apfel', 'Bananen', 'Trauben', 'Joghurt', 'Honig', 'Zimt']\nfor zutat in einkaufsliste:\n    print(zutat)\n\nVideo zu “Schleifen in Python: for-Schleife” von Programmieren lernen\n\nEs kommt sehr häufig vor, dass über Listen mit Zahlen iteriert werden soll.\nDafür stellt Python3 die Funktion range() zur Verfügung. Das folgende\nCode-Fragment gibt beispielsweise die Zahlen von 0 bis 4 aus.\n\n# Zahlen von 0 bis 4 ausgeben\nfor i in range(5):\n    print(i)\n\n\n\nMehr Details zu range() in Kombination mit einer for-Schleife finden Sie in\ndem folgenden Video.\n\nVideo zu “for-Schleife in Python: Zählerschleife”","type":"content","url":"/chapter02-sec02#code-wiederholen-mit-der-for-schleife","position":11},{"hierarchy":{"lvl1":"2.2 Listen und for-Schleifen","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter02-sec02#zusammenfassung-und-ausblick","position":12},{"hierarchy":{"lvl1":"2.2 Listen und for-Schleifen","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Abschnitt haben wir uns mit dem Datentyp Liste befasst, der zur\nSammlung verschiedener Datenobjekte dient. Für die sequentielle Bearbeitung von\nListenelementen ist die for-Schleife besonders geeignet. Es existieren\nzusätzliche Datentypen wie Dictionary, Tupel und Set, die sich ebenfalls zum\nSpeichern von Datenobjekten eignen. Neben der for-Schleife gibt es eine\nalternative Schleifenstruktur, die while-Schleife, die Code wiederholt, solange\neine bestimmte Bedingung erfüllt ist. Anstatt uns weiterhin auf solche Aspekte\nder Python-Programmierung zu konzentrieren, werden wir im nächsten Kapitel den\nFokus auf die Implementierung eigener Funktionen legen und einen kurzen Ausflug\nin die objektorientierte Programmierung unternehmen.","type":"content","url":"/chapter02-sec02#zusammenfassung-und-ausblick","position":13},{"hierarchy":{"lvl1":"2.3 Dictionaries, Funktionen und Methoden"},"type":"lvl1","url":"/chapter02-sec03","position":0},{"hierarchy":{"lvl1":"2.3 Dictionaries, Funktionen und Methoden"},"content":"Sobald die Funktionalitäten komplexer werden, ist es wichtig zu verstehen, wie\nwir mit verschiedenen Datenstrukturen arbeiten und wie Code in Python\norganisiert ist. In diesem Kapitel lernen wir zunächst mit Dictionaries eine\nweitere wichtige Datenstruktur kennen. Anschließend vertiefen wir unser\nVerständnis von Funktionen und lernen das Konzept der objektorientierten\nProgrammierung kennen. Funktionen, die direkt an einen Datentyp gekoppelt sind,\nwerden Methoden genannt.","type":"content","url":"/chapter02-sec03","position":1},{"hierarchy":{"lvl1":"2.3 Dictionaries, Funktionen und Methoden","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter02-sec03#lernziele","position":2},{"hierarchy":{"lvl1":"2.3 Dictionaries, Funktionen und Methoden","lvl2":"Lernziele"},"content":"Lernziele\n\nSie kennen den Datentyp Dictionary und können Schlüssel-Wert-Paare\nerstellen und nutzen.\n\nSie verstehen, wie Funktionen aufgerufen werden und können mit Argumenten und\nRückgabewerten arbeiten.\n\nSie wissen, was Methoden sind und kennen das Konzept der\nobjektorientierten Programmierung.","type":"content","url":"/chapter02-sec03#lernziele","position":3},{"hierarchy":{"lvl1":"2.3 Dictionaries, Funktionen und Methoden","lvl2":"Dictionaries"},"type":"lvl2","url":"/chapter02-sec03#dictionaries","position":4},{"hierarchy":{"lvl1":"2.3 Dictionaries, Funktionen und Methoden","lvl2":"Dictionaries"},"content":"Bisher haben wir Listen kennengelernt, um mehrere Daten zu sammeln. Listen haben\njedoch einen Nachteil: Wir müssen uns merken, an welcher Position welche\nInformation steht. Bei einer Liste [7900, 200000, 190] ist nicht sofort klar,\nwas die einzelnen Zahlen bedeuten. Für solche Fälle gibt es eine Datenstruktur,\ndie besser geeignet ist, das sogenannte Dictionary.\n\nDictionaries (deutsch: Wörterbücher) speichern Daten in Form von\nSchlüssel-Wert-Paaren. Wir können uns ein Dictionary wie ein Wörterbuch\nvorstellen: Wir schlagen einen Begriff nach (das ist der Schlüssel) und erhalten\ndie zugehörige Information (das ist der Wert). Im Gegensatz zu Listen, die über\neinen numerischen Index angesprochen werden, erfolgt der Zugriff bei\nDictionaries über aussagekräftige Schlüssel.\n\nEin Dictionary wird mit geschweiften Klammern {} erstellt. Die\nSchlüssel-Wert-Paare werden durch Doppelpunkte : getrennt, mehrere Paare durch\nKommas ,. Schauen wir uns dazu ein Beispiel an:\n\nperson = {\n    \"name\": \"Alice\",\n    \"alter\": 25,\n    \"stadt\": \"Berlin\"\n}\nprint(person)\n\n\n\nDer Zugriff auf einzelne Werte erfolgt über die Angabe des Schlüssels in eckigen\nKlammern:\n\nprint(f'Name: {person[\"name\"]}')\nprint(f'Alter: {person[\"alter\"]}')\nprint(f'Stadt: {person[\"stadt\"]}')\n\n\n\nWir können auch Werte ändern oder neue Schlüssel-Wert-Paare hinzufügen:\n\n# Wert ändern\nperson[\"alter\"] = 26\nprint(f'Neues Alter: {person[\"alter\"]}')\n\n# Neues Schlüssel-Wert-Paar hinzufügen\nperson[\"beruf\"] = \"Datenwissenschaftlerin\"\nprint(person)\n\n\n\nDer Vorteil von Dictionaries gegenüber Listen wird besonders deutlich, wenn wir\ndie beiden Datenstrukturen vergleichen. In einer Liste müssten wir uns merken,\ndass der Name an Position 0 steht, das Alter an Position 1 und so weiter. Bei\neinem Dictionary können wir direkt mit aussagekräftigen Begriffen arbeiten.\n\n# Als Liste (unübersichtlich)\nperson_liste = [\"Alice\", 25, \"Berlin\"]\nprint(f'Name (Liste): {person_liste[0]}')  # Was bedeutet Index 0?\n\n# Als Dictionary (selbsterklärend)\nperson_dict = {\"name\": \"Alice\", \"alter\": 25, \"stadt\": \"Berlin\"}\nprint(f'Name (Dictionary): {person_dict[\"name\"]}')\n\n\n\nMini-Übung\n\nErstellen Sie ein Dictionary für einen Datenpunkt mit folgenden Informationen:\n\ntemperatur: 23.5\n\nluftfeuchtigkeit: 65\n\nstandort: “Sensor_01”\n\nGeben Sie dann die Temperatur und den Standort mit passenden Beschriftungen aus.\n\n# Hier Ihr Code:\n\n\n\nLösung# Dictionary für Datenpunkt erstellen\nmesswert = {\n    \"temperatur\": 23.5,\n    \"luftfeuchtigkeit\": 65,\n    \"standort\": \"Sensor_01\"\n}\n\n# Zugriff auf spezifische Werte\nprint(f'Temperatur: {messwert[\"temperatur\"]} °C')\nprint(f'Standort: {messwert[\"standort\"]}')\n\nDictionaries werden uns in späteren Kapiteln häufiger begegnen. Insbesondere bei\nder Visualisierung mit Plotly werden wir Dictionaries verwenden, um\nKonfigurationen für Diagramme zu definieren.","type":"content","url":"/chapter02-sec03#dictionaries","position":5},{"hierarchy":{"lvl1":"2.3 Dictionaries, Funktionen und Methoden","lvl2":"Funktionen"},"type":"lvl2","url":"/chapter02-sec03#funktionen","position":6},{"hierarchy":{"lvl1":"2.3 Dictionaries, Funktionen und Methoden","lvl2":"Funktionen"},"content":"Eine Funktion ist eine Zusammenfassung von Code, der eine bestimmte Teilaufgabe\nlöst. Wir haben bereits verschiedene Funktionen kennengelernt und verwendet. Die\nFunktion print() gibt Text oder Werte auf dem Bildschirm aus. Die Funktion\nlen() ermittelt die Länge einer Liste oder eines Strings. Die Funktion\ntype() gibt uns den Datentyp einer Variable zurück.\n\nEine Funktion wird aufgerufen, indem wir den Namen der Funktion hinschreiben und\ndann in runden Klammern ihre Argumente übergeben. Welche Argumente eine Funktion\nerwartet, hängt von der jeweiligen Funktion ab. Betrachten wir dazu einige\nBeispiele:\n\n# len() mit verschiedenen Argumenten\nprint(len('Hallo'))  # String als Argument\nprint(len([1, 2, 3, 4, 8, 2]))  # Liste als Argument\n\n# type() ermittelt den Datentyp\nx = 42\nprint(type(x))\ny = 3.14\nprint(type(y))\n\n\n\nDie meisten Funktionen geben ein Ergebnis zurück. Dieses können wir einer\nVariable zuweisen, um weiter damit zu arbeiten:\n\nwort = 'Maschinelles Lernen'\nanzahl_zeichen = len(wort)\nprint(f'Der Text \"{wort}\" hat {anzahl_zeichen} Zeichen.')\n\n\n\nBeim maschinellen Lernen werden wir vor allem Funktionen aus Bibliotheken wie\nNumPy, Pandas und scikit-learn verwenden. Diese Funktionen sind bereits fertig\nimplementiert und wir müssen nur wissen, wie wir sie aufrufen. Das Schreiben\neigener Funktionen ist seltener nötig, weshalb wir uns darauf konzentrieren, wie\nwir bestehende Funktionen effektiv nutzen können.\n\nFalls Sie sich dafür interessieren, Funktionen selbst zu definieren, finden Sie\nin den drei folgenden Videos weitere Details zur Implementierung von Funktionen.\n\nVideo zu “Funktionen selbst definieren” von Programmieren lernen\n\nVideo zu “Funktionen mit Parametern” von Programmieren lernen\n\nVideo zu “Funktionen mit Rückgabewert” von Programmieren lernen","type":"content","url":"/chapter02-sec03#funktionen","position":7},{"hierarchy":{"lvl1":"2.3 Dictionaries, Funktionen und Methoden","lvl2":"Objektorientierte Programmierung"},"type":"lvl2","url":"/chapter02-sec03#objektorientierte-programmierung","position":8},{"hierarchy":{"lvl1":"2.3 Dictionaries, Funktionen und Methoden","lvl2":"Objektorientierte Programmierung"},"content":"Nachdem wir gesehen haben, wie Funktionen aufgerufen werden, schauen wir uns nun\nan, wie Python Funktionen und Daten in Objekten organisiert.\n\nDie Idee der objektorientierten Programmierung ist, Daten und die dazugehörigen\nFunktionen zusammenzufassen. Bisher haben wir Funktionen und Daten getrennt. Die\nDaten werden in Variablen gespeichert und Funktionen verarbeiten diese Daten\nnach dem EVA-Prinzip (Eingabe, Verarbeitung, Ausgabe). In der objektorientierten\nProgrammierung werden Daten und Funktionen zu einem Objekt kombiniert. Die\nEigenschaften eines Objekts werden Attribute genannt. Funktionen, die zu\neinem Objekt gehören, nennen wir Methoden.\n\nTatsächlich haben wir bereits Methoden verwendet, ohne es explizit zu wissen.\nListen sind Objekte und wenn wir liste.append(4) schreiben, rufen wir die\nMethode .append() des Listen-Objekts auf. Auch bei Dictionaries stehen\nMethoden wie .keys() oder .values() zur Verfügung, auch wenn wir diese\nbisher nicht verwendet haben.\n\nmeine_liste = [17, 3.5]\nmeine_liste.append(4)\nprint(meine_liste)\n\n\n\nSehen wir uns noch ein weiteres Beispiel an. Ein String ist in Python ein\nObjekt. Strings haben verschiedene Methoden, die uns die Arbeit erleichtern.\n\ntext = \"Hallo Welt\"\n\n# Methode upper(): wandelt alle Buchstaben in Großbuchstaben um\ntext_gross = text.upper()\nprint(text_gross)\n\n# Methode lower(): wandelt alle Buchstaben in Kleinbuchstaben um\ntext_klein = text.lower()\nprint(text_klein)\n\n# Methode replace(): ersetzt Teile des Strings\ntext_neu = text.replace(\"Welt\", \"Python\")\nprint(text_neu)\n\n\n\nDas nächste Beispiel zeigt den Einsatz von Methoden bei Dictionaries:\n\n# Dictionary-Methoden\nperson = {\"name\": \"Alice\", \"alter\": 25}\nprint(person.keys())    # dict_keys(['name', 'alter'])\nprint(person.values())  # dict_values(['Alice', 25])\n\n\n\nMethoden werden also aufgerufen, indem wir die Variable (das Objekt)\nhinschreiben, dann einen Punkt setzen und dann den Methodennamen mit runden\nKlammern anfügen. Falls die Methode Argumente benötigt, werden diese in die\nrunden Klammern geschrieben.\n\nIn den kommenden Kapiteln werden wir viele Bibliotheken für maschinelles Lernen\nverwenden. Diese Bibliotheken arbeiten mit Objekten und Methoden. Beispielsweise\nwerden wir später mit scikit-learn arbeiten. Dort erstellen wir ein\nModell-Objekt und rufen dann Methoden wie fit() auf, um das Modell zu\ntrainieren, oder predict(), um Vorhersagen zu machen. Das Prinzip ist immer\ndas gleiche wie bei liste.append() oder text.upper().\n\nPython ermöglicht es uns auch, eigene Objekte zu definieren. Dazu verwenden wir\ndas Schlüsselwort class. In diesem Einführungskurs werden wir jedoch keine\neigenen Klassen erstellen, sondern nur die Klassen und Methoden verwenden, die\nuns die verschiedenen Bibliotheken zur Verfügung stellen. Die folgenden Videos\ngeben einen vertieften Einblick in die Objektorientierung mit Python für alle,\ndie mehr darüber erfahren möchten.\n\nVideo zu “Konzept der Objektorientierung” von Programmieren lernen\n\nVideo zu “Klassen und Objekte” von Programmieren lernen\n\nVideo zu “Der self Parameter” von Programmieren lernen\n\nVideo zu “Methoden in Klassen” von Programmieren lernen","type":"content","url":"/chapter02-sec03#objektorientierte-programmierung","position":9},{"hierarchy":{"lvl1":"2.3 Dictionaries, Funktionen und Methoden","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter02-sec03#zusammenfassung-und-ausblick","position":10},{"hierarchy":{"lvl1":"2.3 Dictionaries, Funktionen und Methoden","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Kapitel haben wir drei wichtige Konzepte kennengelernt. Zunächst haben\nwir mit Dictionaries eine Datenstruktur gesehen, die uns erlaubt, Daten über\naussagekräftige Schlüssel statt über numerische Indizes anzusprechen.\nAnschließend haben wir vertieft, wie wir Funktionen aufrufen und mit ihren\nRückgabewerten arbeiten. Zum Schluss haben wir einen ersten Einblick in die\nobjektorientierte Programmierung erhalten und verstehen nun, was Methoden sind\nund wie sie sich von normalen Funktionen unterscheiden.\n\nOft ist es aber praktischer, Funktionen und Klassen zu nutzen, die bereits\nimplementiert sind, anstatt das Rad neu zu erfinden. Vor allem bei der\nDatenexploration und den maschinellen Lernalgorithmen werden wir die\nvorgefertigten Funktionsbausteine nutzen, wie wir in den nächsten Kapiteln sehen\nwerden.","type":"content","url":"/chapter02-sec03#zusammenfassung-und-ausblick","position":11},{"hierarchy":{"lvl1":"Übungen"},"type":"lvl1","url":"/chapter02-sec04","position":0},{"hierarchy":{"lvl1":"Übungen"},"content":"Übung 2.1\n\nWelcher Datentyp liegt vor? Kopieren Sie diesen Text in eine Markdown-Zelle und\nschreiben Sie Ihre Antwort hinter den Pfeil.\n\n3 -->\n\n-3 -->\n\n‘drei’-->\n\n3.3 -->\n\n3,3 -->\n\n3**3 -->\n\n3**(1/3) -->\n\nVerwenden Sie anschließend type() in einer Code-Zelle, um Ihre Antwort zu\nüberprüfen.\n\nLösung\n\n3 –> int, also Integer\n\n-3 –> int, also Integer\n\n‘drei’–> str, also String\n\n3.3 –> float, also Float\n\n3,3 –> Fehlermeldung, das Dezimaltrennzeichen ist der Punkt, nicht das Komma\n\n3**3 –> int, also Integer\n\n3**(1/3) –> float, also Float\n\nÜbung 2.2\n\nSchreiben Sie ein Programm, das die Zahlen von 5 bis 15 mit ihrem Quadrat\nausgibt, also “Das Quadrat von 5 ist 25.” usw.\n\nLösungfor zahl in range(5, 16):\n    print(f'Das Quadrat von {zahl} ist {zahl**2}.')\n\nÜbung 2.3\n\nSchreiben Sie eine For-Schleife, die die Brüche 1/7, 2/7, 3/7, bis 7/7 als\nFließkommazahl gerundet auf 2 Nachkommastellen ausgibt. Beispielausgabe:1/7 = 0.14.\n2/7 = 0.29.\n3/7 = 0.43.\n4/7 = 0.57.\n5/7 = 0.71.\n6/7 = 0.86.\n7/7 = 1.00.\n\nLösungfor zahl in range(1, 8):\n    print(f'{zahl}/7 = {zahl/7:.2f}.')\n\nÜbung 2.4\n\nSchreiben Sie ein Programm, das eine Liste von Namen durchläuft und jede Person\nbegrüßt. Wenn beispielsweise die Namen Alice, Bob und Charlie in der Liste\nstehen, lauten die Begrüßungen:Hallo, Alice!\nHallo, Bob!\nHallo, Charlie!\n\nLösungnamensliste = ['Alice', 'Bob', 'Charlie']\nfor name in namensliste:\n    print(f'Hallo, {name}!')\n\nÜbung 2.5\n\nErstellen Sie ein Dictionary für einen Studierenden mit folgenden Informationen:\n\nvorname: “Max”\n\nnachname: “Mustermann”\n\nmatrikelnummer: 123456\n\nstudiengang: “Maschinenbau”\n\nGeben Sie dann folgende Information aus: “Max Mustermann (123456) studiert Maschinenbau.”\n\nLösungstudierender = {\n    \"vorname\": \"Max\",\n    \"nachname\": \"Mustermann\",\n    \"matrikelnummer\": 123456,\n    \"studiengang\": \"Maschinenbau\"\n}\n\nprint(f'{studierender[\"vorname\"]} {studierender[\"nachname\"]} ({studierender[\"matrikelnummer\"]}) studiert {studierender[\"studiengang\"]}.')\n\nÜbung 2.6\n\nGegeben ist die Liste: zahlen = [5, 12, 3, 8, 1]\n\nFügen Sie die Zahl 7 am Ende der Liste hinzu.\n\nErstellen Sie einen String text = \"Python Programmierung\" und wandeln Sie ihn in Großbuchstaben um.\n\nGeben Sie beide Ergebnisse aus.\n\nLösung# Liste bearbeiten\nzahlen = [5, 12, 3, 8, 1]\nzahlen.append(7)\nprint(f'Liste: {zahlen}')\n\n# String bearbeiten\ntext = \"Python Programmierung\"\ntext_gross = text.upper()\nprint(f'Text in Großbuchstaben: {text_gross}')","type":"content","url":"/chapter02-sec04","position":1},{"hierarchy":{"lvl1":"3.1 Pandas Series"},"type":"lvl1","url":"/chapter03-sec01","position":0},{"hierarchy":{"lvl1":"3.1 Pandas Series"},"content":"Eine Series ist eine von zwei grundlegenden Datenstrukturen des\nPandas-Moduls. Die Series dient vor allem dazu, Daten zu verwalten und\nstatistisch zu erkunden. Bevor wir die neue Datenstruktur näher beleuchten,\nmachen wir uns aber zuerst mit dem Modul Pandas vertraut.","type":"content","url":"/chapter03-sec01","position":1},{"hierarchy":{"lvl1":"3.1 Pandas Series","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter03-sec01#lernziele","position":2},{"hierarchy":{"lvl1":"3.1 Pandas Series","lvl2":"Lernziele"},"content":"Lernziele\n\nSie können erklären, was ein Modul in Python ist.\n\nSie kennen das Modul Pandas und können es mit seiner üblichen Abkürzung\npd importieren.\n\nSie kennen die Pandas-Datenstruktur Series.\n\nSie wissen, was ein Index ist.\n\nSie können aus Listen ein Series-Objekt erzeugen und mit einem Index\nversehen.\n\nSie können mit Series-Objekten rechnen.\n\nSie können die Elemente eines Series-Objektes mit sort_values()\naufsteigend und absteigend sortieren lassen.","type":"content","url":"/chapter03-sec01#lernziele","position":3},{"hierarchy":{"lvl1":"3.1 Pandas Series","lvl2":"Das Modul Pandas"},"type":"lvl2","url":"/chapter03-sec01#das-modul-pandas","position":4},{"hierarchy":{"lvl1":"3.1 Pandas Series","lvl2":"Das Modul Pandas"},"content":"In der Programmierung können wir alle Funktionalitäten selbst programmieren.\nOder wir verwenden schon fertige Komponenten und setzen sie so zusammen, wie wir\nes zur Lösung unseres Problems brauchen. Eine Sammlung von fertigen\nPython-Komponenten zu einem bestimmten Thema wird Modul genannt. In anderen\nProgrammiersprachen oder allgemein in der Informatik nennt man eine solche\nSammlung auch Bibliothek oder verwendet den englischen Begriff Library.\n\nWas ist ... ein Modul?\n\nEin Modul (oder eine Bibliothek oder eine Library) ist eine Sammlung von\nPython-Code zu einem bestimmten Thema, der als Werkzeug für eigene Programme\neingesetzt werden kann.\n\nUm ein Modul in Python benutzen zu können, muss es zunächst einmal installiert\nsein. Um dann die Funktionen, Klassen, Datentypen oder Konstanten benutzen zu\nkönnen, die das Modul zur Verfügung stellt, wird es importiert. Wir werden in\ndieser Vorlesung sehr intensiv das Modul Pandas verwenden. Pandas ist ein\nModul zur Verarbeitung und Analyse von Daten. Es ist üblich, das Modul pandas\nmit der Abkürzung pd zu importieren, damit wir nicht immer pandas schreiben\nmüssen, wenn wir Code aus dem Pandas-Modul benutzen.\n\nimport pandas as pd\n\n\n\nSollten jetzt eine Fehlermeldung auftauchen, ist das Pandas-Modul nicht\ninstalliert. Installieren Sie zunächst Pandas beispielsweise mit !conda install pandas oder !pip install pandas. Mit der Funktion dir() werden alle\nFunktionalitäten des Moduls aufgelistet.\n\ndir(pd)\n\n\n\nDie Ausgabe ist sehr lang und zunächst unübersichtlich. Wenn wir mehr über eine\nbestimmte Funktionalität erfahren möchten, können wir die help()-Funktion\nverwenden. Beispielsweise liefert help(pd.read_csv) eine ausführliche\nBeschreibung der Funktion zum Import von csv-Dateien:\n\nhelp(pd.read_csv)\n\n\n\nAlternativ können wir auch in der offiziellen\n\n\nPandas​-Dokumentation\nnachschlagen, die übersichtlicher formatiert ist und viele Beispiele enthält.","type":"content","url":"/chapter03-sec01#das-modul-pandas","position":5},{"hierarchy":{"lvl1":"3.1 Pandas Series","lvl2":"Die Datenstruktur Series"},"type":"lvl2","url":"/chapter03-sec01#die-datenstruktur-series","position":6},{"hierarchy":{"lvl1":"3.1 Pandas Series","lvl2":"Die Datenstruktur Series"},"content":"Einfache Listen reichen nicht aus, um größere Datenmengen effizient zu speichern\nund zu analysieren. Listen haben mehrere Nachteile: Sie speichern keine\nInformation über den Datentyp ihrer Elemente, bieten keine Möglichkeit,\nMetadaten wie beschreibende Labels zu hinterlegen, und sind bei großen\nDatenmengen ineffizient. Dazu benutzen Data Scientists die Datenstrukturen\nSeries oder DataFrame aus dem Pandas-Modul. Dabei wird Series für\nDatenreihen genommen. Damit sind Vektoren gemeint, wenn alle Elemente der\nDatenreihe aus Zahlen bestehen, oder eindimensionale Arrays. Die Datenstruktur\nDataFrame wiederum dient zum Speichern und Verarbeiten von tabellierten\nDaten, also Matrizen, wenn alle Elemente Zahlen sind, oder zweidimensionale\nArrays.\n\nWir starten mit der Datenstruktur Series. Als Beispiel betrachten wir die\nVerkaufspreise (in Euro) von zehn Autos. Die Daten stammen von der\nInternetplattform \n\nAutoscout24. Die Preise kommen\nzunächst in eine Liste (erkennbar an den eckigen Klammern), aus der dann ein\nSeries-Objekt erzeugt wird.\n\npreisliste = [1999, 35990, 17850, 46830, 27443, 14240, 19950, 15950, 21990, 50000]\npreise = pd.Series(preisliste)\nprint(preise)\n\n\n\nWas ist aber jetzt der Vorteil von Pandas? Warum nicht einfach bei der Liste\nbleiben? Der wichtigste Unterschied zwischen Liste und Series ist der Index.\n\nBei einer Liste werden Elemente über ihre Position angesprochen, der Index ist\nimmer eine Ganzzahl beginnend bei 0. Wenn bei einer Liste auf das dritte Element\nzugegriffen werden soll, dann verwenden wir den Index 2 (zur Erinnerung: Python\nzählt ab 0) und schreiben\n\npreis_drittes_auto = preisliste[2]\nprint(f'Preis des dritten Autos: {preis_drittes_auto} EUR')\n\n\n\nDie Datenstruktur Series ermöglicht es aber, einen expliziten Index zu setzen.\nÜber den optionalen Parameter index= speichern wir als Zusatzinformation noch\nab, von welchem Auto der Verkaufspreis erfasst wurde. Wir werden diesen\nDatensatz in den folgenden Kapiteln noch weiter vertiefen. An dieser Stelle\nhalten wir fest, dass die ersten drei Autos von der Marke Audi sind, die\nnächsten sind BMWs und die letzten fünf sind von der Marke Citroen.\n\nautos = ['Audi Nr. 1', 'Audi Nr. 2', 'Audi Nr. 3', 'BMW Nr. 1', 'BMW Nr. 2', 'Citroen Nr. 1', 'Citroen Nr. 2', 'Citroen Nr. 3', 'Citroen Nr. 4', 'Citroen Nr. 5']\npreise = pd.Series(preisliste, index = autos)\nprint(preise)\n\n\n\nBeim ersten Aufruf von print(preise) wurden die Zahlen 0, 1, 2, usw.\nausgegeben, weil das Series-Objekt zu dem Zeitpunkt noch einen automatisch\ngenerierten ganzzahligen Index hatte. Den expliziten Index nutzen wir jetzt, um\nauf den Verkaufspreis des dritten Autos zuzugreifen. Das dritte Auto ist Audi Nr. 3. Wie bei Listen verwenden wir eckige Klammern:\n\npreis_drittes_auto = preise['Audi Nr. 3']\nprint(f'Preis des dritten Autos: {preis_drittes_auto} EUR')\n\n\n\nDie Datenstruktur Series hat gegenüber der Liste noch einen weiteren Vorteil. In\nder Datenstruktur ist noch eine Zusatzinformation gespeichert, die Eigenschaft\ndtype. Darin gespeichert ist der Datentyp der Elemente des Series-Objektes.\nAuf diese Eigenschaft können wir direkt mit dem sogenannten Punktoperator\nzugreifen.\n\ndatentyp_preise = preise.dtype\nprint(f'Die einzelnen Elemente des Series-Objektes \"preise\" haben den Datentyp {datentyp_preise}, sind also Integer.')\n\n\n\nOffensichtlich sind die gespeicherten Werte Integer. Die Zusatzinformation\nint64 bedeutet, dass es sich um einen 64-Bit-Integer handelt.\n\nMini-Übung\n\nErzeugen Sie ein Series-Objekt mit den Wochentagen als Index. Verwenden Sie\ndabei nur die Werktage und speichern Sie, wie viele Stunden Sie an diesem Tag\nschlafen. Welchen Datentyp haben die Elemente?\n\n# Hier Ihr Code:\n\n\n\nLösungschlafzeiten = pd.Series([8, 9.5, 7.8, 8.1, 8.3], \n  index=['Montag', 'Dienstag', 'Mittwoch', 'Donnerstag', 'Freitag'])\nprint(schlafzeiten)\n\nprint(f'Datentyp: {schlafzeiten.dtype}')","type":"content","url":"/chapter03-sec01#die-datenstruktur-series","position":7},{"hierarchy":{"lvl1":"3.1 Pandas Series","lvl2":"Arbeiten mit Series-Objekten"},"type":"lvl2","url":"/chapter03-sec01#arbeiten-mit-series-objekten","position":8},{"hierarchy":{"lvl1":"3.1 Pandas Series","lvl2":"Arbeiten mit Series-Objekten"},"content":"Falls der Datentyp der einzelnen Elemente eines Series-Objektes ein numerischer\nTyp ist (Integer oder Float), können wir mit den Einträgen auch rechnen. So\nlassen sich beispielweise die Preise nicht in Euro, sondern als Preis pro\nTausend Euro angeben, wenn wir alle Preise durch 1000 teilen.\n\npreise_pro_1000euro = preise / 1000\nprint(preise_pro_1000euro)\n\n\n\nWir könnten auch auf die Idee kommen, das billigste Auto auf den Preis 0 zu\nsetzen und ausgeben, um wie viel Euro die anderen Autos teurer sind. Oder anders\nausgedrückt, wir subtrahieren von jedem Preis den Wert 1999 EUR:\n\npreise_differenz = preise - 1999\nprint(preise_differenz)\n\n\n\nBei zehn Autos war es relativ einfach, das billigste Auto zu ermitteln, indem\nwir einfach die Preisliste durchgeschaut haben. Hilfreicher ist es, vorher die\nPreise aufsteigend oder absteigend zu sortieren. Dazu nutzen wir die Methode\n.sort_values(). Der Name lässt vermuten, dass die Methode die Elemente nach\nihrem Wert sortiert.\n\npreise_aufsteigend = preise.sort_values()\nprint(preise_aufsteigend)\n\n\n\nJetzt zeigt sich auch der Vorteil des expliziten Index, denn auf die\nursprüngliche Reihenfolge kommt es nicht an. Der explizite Index ermöglicht uns,\njedes Auto auch in der nach Preisen aufsteigend sortierten Liste eindeutig\nwiederzufinden. Zum Abschluss sortieren wir noch absteigend. Mit dem optionalen\nArgument ascending wird gesteuert, ob aufsteigend sortiert werden soll oder\nnicht. Fehlt das Argument, so nimmt der Python-Interpreter an, dass ascending = True gewünscht wird, also dass aufsteigend = wahr sein soll. Wollen wir\nabsteigend sortieren, müssen wir aufsteigend = falsch setzen, also ascending  = False.\n\npreise_absteigend = preise.sort_values(ascending = False)\nprint(preise_absteigend)\n\n\n\nWir haben das sortierte Series-Objekt gleich in einer neuen Variable\nabgespeichert. Das ist notwendig, wenn die neue Sortierung erhalten bleiben\nsoll. Standardmäßig wirkt der Sortierungsbefehl nämlich nur einmalig und ändert\ndie eigentliche Reihenfolge im Original nicht. Auch das könnte man durch weitere\nParameter ändern (inplace = True), wie wir in der \n\nPandas-Dokumentation →\nsort_values()\nnachlesen können.\n\nMini-Übung\n\nAlice, Bob, Charlie und Dora sind 22, 20, 24 und 22 Jahre alt. Speichern Sie\ndiese Informationen in einem Series-Objekt und sortieren Sie von alt nach jung.\nZusatzfrage: Was fällt Ihnen bei der Sortierung auf, wenn zwei Personen gleich\nalt sind? Bleibt ihre ursprüngliche Reihenfolge erhalten?\n\n# Hier Ihr Code\n\n\n\nLösung# Erzeugung des Series-Objektes\nnamen = ['Alice', 'Bob', 'Charlie', 'Dora']\nalter = [22, 20, 24, 22]\npersonen = pd.Series(alter, index = namen)\n\n# Sortierung und Ausgabe\npersonen_sortiert = personen.sort_values(ascending = False)\nprint(personen_sortiert)\n\n# Beobachtung: Alice und Dora sind beide 22 Jahre alt. \n# In der sortierten Ausgabe erscheint Dora vor Alice, \n# obwohl Alice in der ursprünglichen Liste zuerst kam.\n# Die Sortierung ist also nicht stabil. Die ursprüngliche \n# Reihenfolge bei gleichen Werten wird nicht unbedingt erhalten.","type":"content","url":"/chapter03-sec01#arbeiten-mit-series-objekten","position":9},{"hierarchy":{"lvl1":"3.1 Pandas Series","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter03-sec01#zusammenfassung-und-ausblick","position":10},{"hierarchy":{"lvl1":"3.1 Pandas Series","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Kapitel haben wir Pandas und die sehr wichtige Datenstruktur Series\nkennengelernt. Im nächsten Kapitel geht es darum, die wichtigsten statistischen\nKennzahlen der Daten zu ermitteln, die in dem Series-Objekt gespeichert sind.","type":"content","url":"/chapter03-sec01#zusammenfassung-und-ausblick","position":11},{"hierarchy":{"lvl1":"3.2 Statistik mit Pandas"},"type":"lvl1","url":"/chapter03-sec02","position":0},{"hierarchy":{"lvl1":"3.2 Statistik mit Pandas"},"content":"Pandas dient nicht nur dazu, Daten zu sammeln, sondern ermöglicht auch\nstatistische Analysen. Die deskriptive Statistik hat zum Ziel, Daten durch\neinfache Kennzahlen und Diagramme zu beschreiben. In diesem Kapitel geht es\ndarum, die wichtigsten statistischen Kennzahlen mit Pandas zu ermitteln und zu\ninterpretieren.","type":"content","url":"/chapter03-sec02","position":1},{"hierarchy":{"lvl1":"3.2 Statistik mit Pandas","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter03-sec02#lernziele","position":2},{"hierarchy":{"lvl1":"3.2 Statistik mit Pandas","lvl2":"Lernziele"},"content":"Lernziele\n\nSie können sich mit .describe() eine Übersicht über statistische Kennzahlen\nverschaffen.\n\nSie wissen, wie Sie die Anzahl der gültigen Einträge mit .count() ermitteln.\n\nSie kennen die statistischen Kennzahlen Mittelwert und Standardabweichung und\nwissen, wie diese mit .mean() und .std() berechnet werden.\n\nSie können das Minimum und das Maximum mit .min() und .max() bestimmen.\n\nSie wissen, wie ein Quantil interpretiert wird und wie es mit .quantile()\nberechnet wird.","type":"content","url":"/chapter03-sec02#lernziele","position":3},{"hierarchy":{"lvl1":"3.2 Statistik mit Pandas","lvl2":"Schnelle Übersicht mit .describe()"},"type":"lvl2","url":"/chapter03-sec02#schnelle-bersicht-mit-describe","position":4},{"hierarchy":{"lvl1":"3.2 Statistik mit Pandas","lvl2":"Schnelle Übersicht mit .describe()"},"content":"Die Methode .describe() aus dem Pandas-Modul liefert eine schnelle Übersicht\nüber viele statistische Kennzahlen. Vor allem, wenn neue Daten geladen werden,\nsollte diese Methode direkt am Anfang angewendet werden. Wir bleiben bei unserem\nBeispiel mit den zehn Autos und deren Verkaufspreisen.\n\n# Import des Pandas-Moduls \nimport pandas as pd\n\n# Erzeugung der Daten als Series-Objekt\npreisliste = [1999, 35990, 17850, 46830, 27443, 14240, 19950, 15950, 21990, 12450]\nautos = ['Audi Nr. 1', 'Audi Nr. 2', 'Audi Nr. 3', 'BMW Nr. 1', 'BMW Nr. 2', 'Citroen Nr. 1', 'Citroen Nr. 2', 'Citroen Nr. 3', 'Citroen Nr. 4', 'Citroen Nr. 5']\npreise = pd.Series(preisliste, index = autos)\n\n\n\nDie Anwendung der .describe()-Methode liefert folgende Ausgabe:\n\npreise.describe()\n\n\n\nDie Methode .describe() liefert acht statistische Kennzahlen, deren Bedeutung\nin der\n\n\nPandas​-Dokumentation\nerläutert wird. Wir gehen im Folgenden jede Kennzahl einzeln durch.\n\nWas machen wir aber, wenn wir die statistischen Kennzahlen erst später verwenden\nwollen? Können wir sie zwischenspeichern? Probieren wir es aus.\n\nstatistische_kennzahlen = preise.describe()\n\n\n\nEs kommt keine Fehlermeldung. Und was ist in der Variable\nstatistische_kennzahlen nun genau gespeichert, welcher Datentyp?\n\ntype(statistische_kennzahlen)\n\n\n\nWie wir sehen wird durch das Anwenden der .describe()-Methode auf das\nSeries-Objekt preise ein neues Series-Objekt erzeugt, in dem wiederum die\nstatistischen Kennzahlen von preise gespeichert sind. Da wir im letzten\nKapitel schon gelernt haben, dass mit eckigen Klammern und dem Index auf einen\neinzelnen Wert zugegriffen werden kann, können wir uns so den minimalen\nVerkaufspreis ausgeben lassen:\n\nminimaler_preis = statistische_kennzahlen['min']\nprint(f'Das billigste Auto wird für {minimaler_preis} EUR angeboten.')\n\n\n\nMini-Übung\n\nLassen Sie die Verkaufspreise aufsteigend sortieren und ausgeben. Welches Auto\nist am teuersten und für wie viel EUR wird es angeboten?\n\nErmitteln Sie dann das Maximum mit .describe() und vergleichen Sie beide Werte.\n\nZusatzfrage: Die Spanne (Maximum - Minimum) ist über 44.000 EUR groß. Wie\ngroß ist die Standardabweichung im Vergleich zur Spanne? Was bedeutet das für\ndie Streuung der Daten?\n\n# Hier Ihr Code\n\n\n\nLösung# Sortierung und Ausgabe\npreise_aufsteigend = preise.sort_values()\nprint('Preise aufsteigend sortiert: ')\nprint(preise_aufsteigend)\n\n# Maximum ermitteln\nstatistische_kennzahlen = preise.describe()\nmaximaler_preis = statistische_kennzahlen['max']\nprint(f'\\nMaximaler Preis mit .describe() ermittelt: {maximaler_preis}')\n\n# Zusatzfrage: Spanne und Standardabweichung\nspanne = statistische_kennzahlen['max'] - statistische_kennzahlen['min']\nstandardabweichung = statistische_kennzahlen['std']\nprint(f'\\nSpanne: {spanne:.2f} EUR')\nprint(f'Standardabweichung: {standardabweichung:.2f} EUR')\nprint(f'Die Standardabweichung beträgt ca. {standardabweichung/spanne:.1%} der Spanne.')\n\nInterpretation: Das teuerste Auto ist BMW Nr. 1 für 46830 EUR. Die Standardabweichung\n(~13.000 EUR) ist etwa 29 % der Spanne (~44.800 EUR), was auf eine starke, aber nicht\nextreme Streuung hindeutet.\n\nNeben der Möglichkeit, die statistischen Kennzahlen über .describe() berechnen\nzu lassen und dann mit dem expliziten Index darauf zuzugreifen, gibt es auch\nMethoden, um die statistischen Kennzahlen direkt zu ermitteln.","type":"content","url":"/chapter03-sec02#schnelle-bersicht-mit-describe","position":5},{"hierarchy":{"lvl1":"3.2 Statistik mit Pandas","lvl2":"Anzahl count()"},"type":"lvl2","url":"/chapter03-sec02#anzahl-count","position":6},{"hierarchy":{"lvl1":"3.2 Statistik mit Pandas","lvl2":"Anzahl count()"},"content":"Mit .count() wird die Anzahl der Einträge bestimmt, die nicht ‘NA’ sind. Der\nBegriff ‘NA’ ist ein Fachbegriff der Datenanalyse. Gemeint sind fehlende\nEinträge, wobei die fehlenden Einträge verschiedene Ursachen haben können:\n\nNA = not available (der Messsensor hat versagt)\n\nNA = not applicable (es ist sinnlos bei einem Mann nachzufragen, ob er\nschwanger ist)\n\nNA = no answer (eine Person hat bei der Umfrage nichts angegeben)\n\nanzahl_gueltige_preise = preise.count()\nprint(f'Im Series-Objekt sind {anzahl_gueltige_preise} nicht NA-Werte, also gültige Datensätze gespeichert.')\n\n\n\n","type":"content","url":"/chapter03-sec02#anzahl-count","position":7},{"hierarchy":{"lvl1":"3.2 Statistik mit Pandas","lvl2":"Mittelwert mean()"},"type":"lvl2","url":"/chapter03-sec02#mittelwert-mean","position":8},{"hierarchy":{"lvl1":"3.2 Statistik mit Pandas","lvl2":"Mittelwert mean()"},"content":"Der Mittelwert ist die Summe aller Elemente geteilt durch ihre Anzahl. Wie\npraktisch, dass wir mit .count() schon die Anzahl der gültigen Werte geliefert\nbekommen. Rechnen wir zuerst einmal “händisch” nach, was der durchschnittliche\nVerkaufspreis der 10 Autos ist.\n\npreisliste = [1999, 35990, 17850, 46830, 27443, 14240, 19950, 15950, 21990, 12450]\nsumme = 1999 + 35990 + 17850 + 46830 + 27443 + 14240 + 19950 + 15950 + 21990 + 12450\nprint(f'Die Summe ist {summe} EUR.')\nmittelwert = summe / 10\nprint(f'Der durchschnittliche Verkaufspreis ist {mittelwert} EUR.')\n\n\n\nMittelwert heißt auf Englisch mean. Daher ist es nicht verwunderlich, dass die\nMethode .mean() den Mittelwert der Einträge in jeder Spalte berechnet.\n\nmittelwert = preise.mean()\nprint(f'Der mittlere Verkaufspreis beträgt {mittelwert} EUR.')\n\n\n\nDas folgende Video wiederholt das Konzept des Mittelwerts.\n\nVideo zu “Mittelwert” von Datatab","type":"content","url":"/chapter03-sec02#mittelwert-mean","position":9},{"hierarchy":{"lvl1":"3.2 Statistik mit Pandas","lvl2":"Standardabweichung std()"},"type":"lvl2","url":"/chapter03-sec02#standardabweichung-std","position":10},{"hierarchy":{"lvl1":"3.2 Statistik mit Pandas","lvl2":"Standardabweichung std()"},"content":"Der Mittelwert ist wichtig, aber er erzählt nicht die ganze Geschichte. Betrachten wir unsere Autopreise:\n\nmittelwert = preise.mean()\nprint(f'Durchschnittspreis: {mittelwert:.2f} EUR')\nprint('\\nAber schauen wir genauer hin:')\nprint(f'Günstigstes Auto: {preise.min():.2f} EUR')\nprint(f'Teuerstes Auto: {preise.max():.2f} EUR')\nprint(f'Spanne: {preise.max() - preise.min():.2f} EUR')\n\n\n\nDie Spanne von über 44.000 EUR zeigt: Diese Autos sind sehr unterschiedlich! Die Standardabweichung beschreibt diese Streuung in einer einzigen Zahl:\n\nstandardabweichung = preise.std()\nprint(f'Standardabweichung: {standardabweichung:.2f} EUR')\n\n\n\nWas bedeutet das konkret? Eine Standardabweichung von 13.000 EUR bei einem\nMittelwert von 21.500 EUR bedeutet: Die Preise schwanken stark. Wenn wir ein\n“durchschnittliches” Auto für 21.500 EUR erwarten, müssen wir mit Abweichungen\nvon ±13.000 EUR rechnen.\n\nSchauen wir uns das an unseren konkreten Daten an:\n\nMittelwert ± Standardabweichung = 21.469 ± 13.000 EUR\n\nDas ergibt den Bereich: [8.469 EUR, 34.469 EUR]\n\nTatsächlich liegen 7 von 10 Autos in diesem Bereich\n\nDie Standardabweichung hat dieselbe Einheit wie die Originaldaten (hier: EUR).\nDas macht sie direkt interpretierbar im Gegensatz zur Varianz, die in EUR²\ngemessen wird.\n\nVideo zu “Standardabweichung” von Datatab","type":"content","url":"/chapter03-sec02#standardabweichung-std","position":11},{"hierarchy":{"lvl1":"3.2 Statistik mit Pandas","lvl2":"Minimum und Maximum mit min() und max()"},"type":"lvl2","url":"/chapter03-sec02#minimum-und-maximum-mit-min-und-max","position":12},{"hierarchy":{"lvl1":"3.2 Statistik mit Pandas","lvl2":"Minimum und Maximum mit min() und max()"},"content":"Die Namen der Methoden .min() und max() sind fast schon wieder\nselbsterklärend. Die Methode .min() liefert den kleinsten Wert zurück, der\ngefunden wird. Umgekehrt liefert .max() den größten Eintrag. Wie häufig die\nminimalen und maximalen Werte vorkommen, ist dabei egal. Es kann durchaus sein,\ndass das Minimum oder das Maximum mehrfach vorkommt.\n\nSchauen wir uns an, was der niedrigste Verkaufspreis ist. Und dann schauen wir\nnach, welches Auto am teuersten ist.\n\npreis_min = preise.min()\nprint(f'Das billigste oder die billigsten Autos werden zum Preis von {preis_min} EUR angeboten.')\n\npreis_max = preise.max()\nprint(f'Das teuerste oder die teuersten Autos werden für {preis_max} EUR angeboten.')\n\n\n\n","type":"content","url":"/chapter03-sec02#minimum-und-maximum-mit-min-und-max","position":13},{"hierarchy":{"lvl1":"3.2 Statistik mit Pandas","lvl2":"Quantil mit quantile()"},"type":"lvl2","url":"/chapter03-sec02#quantil-mit-quantile","position":14},{"hierarchy":{"lvl1":"3.2 Statistik mit Pandas","lvl2":"Quantil mit quantile()"},"content":"Das Quantil p\\% ist der Wert, bei dem p\\% der Einträge kleiner oder gleich\ndiesem Wert sind und 100\\% - p\\% sind größer. Meist werden nicht Prozentzahlen\nverwendet, sondern p ist zwischen 0 und 1, wobei die 1 für 100 % steht.\n\nAngenommen, wir würden gerne das 0.5-Quantil (auch Median genannt) der Preise\nwissen. Mit der Methode .quantile() können wir diesen Wert leicht aus den\nDaten holen.\n\nquantil50 = preise.quantile(0.5)\nprint(f'Der Median, d.h. das 50 % Quantil, liegt bei {quantil50} EUR.')\n\n\n\nDas 50 % -Quantil liegt bei 18900 EUR. 50 % aller Autos werden zu einem Preis\nangeboten, der kleiner oder gleich 18900 EUR ist. Und 50 % aller Autos werden\nteurer angeboten. Wir schauen uns jetzt das 75 % Quantil an.\n\nquantil75 = preise.quantile(0.75)\nprint(f'75 % aller Autos haben einen Preis kleiner gleich {quantil75} EUR.')\n\n\n\n75 % aller Autos werden günstiger als 26079.75 EUR angeboten. Wir können uns für\njeden beliebigen Prozentsatz zwischen 0 % und 100 % das Quantil ansehen, aber\nhäufig wird neben dem 75 % Quantil, dem 50 % Quantil noch das 25 % Quantil\nbetrachtet.\n\nquantil25 = preise.quantile(0.25)\nprint(f'25 % aller Autos haben einen Preis kleiner gleich {quantil25} EUR.')\n\n\n\n","type":"content","url":"/chapter03-sec02#quantil-mit-quantile","position":15},{"hierarchy":{"lvl1":"3.2 Statistik mit Pandas","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter03-sec02#zusammenfassung-und-ausblick","position":16},{"hierarchy":{"lvl1":"3.2 Statistik mit Pandas","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Abschnitt haben wir uns mit einfachen statistischen Kennzahlen\nbeschäftigt, die Pandas mit der Methode .describe() zusammenfasst, die aber\nauch einzeln über\n\n.count()\n\n.mean()\n\n.std()\n\n.min() und .max()\n\n.quantile()\n\nberechnet und ausgegeben werden können. Im nächsten Kapitel geht es darum, durch\nDiagramme mehr über die Daten zu erfahren.","type":"content","url":"/chapter03-sec02#zusammenfassung-und-ausblick","position":17},{"hierarchy":{"lvl1":"3.3 Boxplots mit Plotly"},"type":"lvl1","url":"/chapter03-sec03","position":0},{"hierarchy":{"lvl1":"3.3 Boxplots mit Plotly"},"content":"Die wichtigsten statistischen Kennzahlen lassen sich mit einem Diagramm\nvisualisieren, das Boxplot genannt wird. Gelegentlich wird auch der deutsche\nBegriff Kastendiagramm dafür gebraucht. In diesem Kapitel visualisieren wir nur\neinen Datensatz. Die große Stärke der Boxplots ist normalerweise, die\nstatistischen Kennzahlen von verschiedenen Datensätzen nebeneinander zu\nvisualisieren, um so leicht einen Vergleich der Datensätze zu ermöglichen.","type":"content","url":"/chapter03-sec03","position":1},{"hierarchy":{"lvl1":"3.3 Boxplots mit Plotly","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter03-sec03#lernziele","position":2},{"hierarchy":{"lvl1":"3.3 Boxplots mit Plotly","lvl2":"Lernziele"},"content":"Lernziele\n\nSie können Plotly Express mit der typischen Abkürzung px importieren.\n\nSie können mit px.box() einen Boxplot eines Pandas-Series-Objektes\nvisualisieren.\n\nSie können die Beschriftung eines Boxplots verändern. Dazu gehört die\nBeschriftung der Achsen und der Titel.\n\nSie können die Datenpunkte neben einem Boxplot anzeigen lassen.\n\nSie wissen, was ein Ausreißer ist und können Ausreißer im Boxplot anzeigen\nlassen.","type":"content","url":"/chapter03-sec03#lernziele","position":3},{"hierarchy":{"lvl1":"3.3 Boxplots mit Plotly","lvl2":"Plotly"},"type":"lvl2","url":"/chapter03-sec03#plotly","position":4},{"hierarchy":{"lvl1":"3.3 Boxplots mit Plotly","lvl2":"Plotly"},"content":"Es gibt zahlreiche Python-Module zur Visualisierung von Daten. In dieser\nVorlesung verwenden wir Plotly, das im Gegensatz zu anderen Bibliotheken wie\nMatplotlib interaktive Diagramme erstellt. Mit Plotly Express steht uns eine\neinfach zu bedienende Schnittstelle zur Erstellung verschiedenster Diagrammtypen\nzur Verfügung.\n\nÜblicherweise wird Plotly Express als px abgekürzt.\n\nimport plotly.express as px\n\n\n\nSollte eine Fehlermeldung auftreten, kann Plotly mit !pip install plotly oder\n!conda install plotly nachinstalliert werden.","type":"content","url":"/chapter03-sec03#plotly","position":5},{"hierarchy":{"lvl1":"3.3 Boxplots mit Plotly","lvl2":"Boxplots mit Plotly Express"},"type":"lvl2","url":"/chapter03-sec03#boxplots-mit-plotly-express","position":6},{"hierarchy":{"lvl1":"3.3 Boxplots mit Plotly","lvl2":"Boxplots mit Plotly Express"},"content":"Wir greifen erneut unser Autoscout24-Beispiel mit den 10 Autos auf.\n\nimport pandas as pd\n\npreisliste = [1999, 35990, 17850, 46830, 27443, 14240, 19950, 15950, 21990, 12450]\npreise = pd.Series(preisliste, index=[\n  'Audi Nr. 1', 'Audi Nr. 2', 'Audi Nr. 3',\n  'BMW Nr. 1', 'BMW Nr. 2', \n  'Citroen Nr. 1', 'Citroen Nr. 2', 'Citroen Nr. 3', 'Citroen Nr. 4', 'Citroen Nr. 5'\n  ])\n\nprint(preise)\n\n\n\nUm einen Boxplot zu erstellen, nutzen wir die Funktion box() von Plotly\nExpress. Wir speichern das Diagramm, das durch diese Funktion erstellt wird, in\nder Variablen diagramm. Um es dann auch nach seiner Erzeugung tatsächlich\nanzeigen zu lassen, verwenden wir die Methode .show(). Zusammen sieht der\nPython-Code zur Erzeugung eines Boxplots folgendermaßen aus:\n\ndiagramm = px.box(preise)\ndiagramm.show()\n\n\n\nBewegen wir die Maus über dem Diagramm, sehen wir verschiedene interaktive Funktionen:\n\nHover-Informationen zeigen die genauen Werte an.\n\nRechts oben erscheinen Buttons zum Zoomen, Verschieben und Herunterladen.\n\nMit der Maus können wir in das Diagramm hineinzoomen.\n\nDie untere Antenne (Whisker) zeigt das Minimum an, die obere Antenne das Maximum\nder Daten. Der Kasten (Box) wird durch das untere Quartil Q1 und das obere\nQuartil Q3 begrenzt. Oder anders formuliert: In der Box liegen 50 % aller\nDatenpunkte. Der Median wird durch die horizontale Linie in der Box dargestellt.\n\nMini-Übung\n\nErstellen Sie einen Boxplot der Autopreise und beantworten Sie anhand des\nDiagramms:\n\nWas ist der Median (die mittlere Linie in der Box)?\n\nZwischen welchen beiden Werten liegen 50 % aller Autos?\n\nTipp: Bewegen Sie die Maus über die Box, um die genauen Werte zu sehen.\n\n# Hier Ihr Code\n\n\n\nLösungdiagramm = px.box(preise)\ndiagramm.show()\n\nAntworten:\n\nDer Median liegt bei ca. 18.900 EUR\n\n50 % der Autos liegen zwischen Q1 (ca. 15.200 EUR) und Q3 (ca. 25.200 EUR)\n\nDas folgende Video erklärt, wie der Boxplot zu interpretieren ist.\n\nVideo zu “Boxplot” von Datatab","type":"content","url":"/chapter03-sec03#boxplots-mit-plotly-express","position":7},{"hierarchy":{"lvl1":"3.3 Boxplots mit Plotly","lvl2":"Beschriftung des Boxplots verändern"},"type":"lvl2","url":"/chapter03-sec03#beschriftung-des-boxplots-ver-ndern","position":8},{"hierarchy":{"lvl1":"3.3 Boxplots mit Plotly","lvl2":"Beschriftung des Boxplots verändern"},"content":"Die Achsenbeschriftungen wurden automatisch gesetzt. Die x-Achse ist mit\n‘variable’ und die y-Achse mit ‘value’ beschriftet. Darüber hinaus ist der Titel\nder Box ‘0’. Der Name wird auch angezeigt, wenn wir die Maus über die Box\nbewegen.\n\nDie 0 erscheint, weil Pandas intern 0 als Standardnamen verwendet, wenn kein\nName angegeben wurde. Wir können der Spalte aber auch einen eigenen Namen geben.\nAm einfachsten funktioniert das direkt bei der Erzeugung, indem der Parameter\nname= gesetzt wird.\n\npreise_mit_name = pd.Series(preisliste, index=[\n  'Audi Nr. 1', 'Audi Nr. 2', 'Audi Nr. 3',\n  'BMW Nr. 1', 'BMW Nr. 2', \n  'Citroen Nr. 1', 'Citroen Nr. 2', 'Citroen Nr. 3', 'Citroen Nr. 4', 'Citroen Nr. 5'\n  ],\n  name='Liste von Autoscout24')\n\nprint(preise_mit_name)\n\n\n\nDer neue Name ‘Liste von Autoscout24’ wird zusätzlich zur Information\n‘dtype’ angezeigt. Erstellen wir nun den Boxplot mit diesem benannten\nSeries-Objekt:\n\ndiagramm = px.box(preise_mit_name)\ndiagramm.show()\n\n\n\nSollen nun auch noch die Achsenbeschriftungen geändert werden, müssen wir die\nautomatisch gesetzten Beschriftungen durch neue Namen ersetzt werden. Dazu wird\nein Dictionary konfiguriert und dem optionalen Argument labels= übergeben. Das\nDictionary wird mit geschweiften Klammern erzeugt. Die Schlüssel vor dem\nDoppelpunkt sind die alten Beschriftungen, die Werte nach dem Doppelpunkt die\nneuen.\n\nDa wir weiterhin den Variablennamen diagramm nutzen, wird das Diagramm aus der\nvorherigen Code-Zelle überschrieben.\n\ndiagramm = px.box(preise_mit_name,\n                  labels={'variable': 'Name des Datensatzes', 'value': 'Verkaufspreis [EUR]'})\ndiagramm.show()\n\n\n\nFehlt noch eine Überschrift, ein Titel. Wie das englische Wort ‘title’ heißt\nauch das entsprechende Schlüsselwort zum Erzeugen eines Titels, nämlich\ntitle=.\n\ndiagramm = px.box(preise_mit_name,\n                  labels={'variable': 'Name des Datensatzes', 'value': 'Verkaufspreis [EUR]'},\n                  title='Statistische Kennzahlen als Boxplot')\ndiagramm.show()\n\n\n\n","type":"content","url":"/chapter03-sec03#beschriftung-des-boxplots-ver-ndern","position":9},{"hierarchy":{"lvl1":"3.3 Boxplots mit Plotly","lvl2":"Datenpunkte im Boxplot anzeigen"},"type":"lvl2","url":"/chapter03-sec03#datenpunkte-im-boxplot-anzeigen","position":10},{"hierarchy":{"lvl1":"3.3 Boxplots mit Plotly","lvl2":"Datenpunkte im Boxplot anzeigen"},"content":"Oft ist es wünschenswert die Rohdaten zusammen mit dem Boxplot zu visualisieren.\nDas ist mit dem points=-Parameter recht einfach, jedoch haben wir zwei mögliche\nOptionen. Wir können mit 'all' alle Punkte anzeigen lassen oder nur die\nAusreißer ('outliers').\n\nLassen wir zuerst alle Punkte anzeigen und setzen also points='all'.\n\ndiagramm = px.box(preise_mit_name,\n                  labels={'variable': 'Name des Datensatzes', 'value': 'Verkaufspreis [EUR]'},\n                  points='all')\ndiagramm.show()\n\n\n\nDie Punkte werden links vom Boxplot platziert. Als nächstes lassen wir uns die\nAusreißer anzeigen.\n\ndiagramm = px.box(preise_mit_name,\n                  labels={'variable': 'Name des Datensatzes', 'value': 'Verkaufspreis [EUR]'},\n                  points='outliers')\ndiagramm.show()\n\n\n\nEs sind keine Punkte zu sehen, was ist falsch? Nun, um das zu klären, müssen wir\nerst einmal definieren, was ein Ausreißer ist.","type":"content","url":"/chapter03-sec03#datenpunkte-im-boxplot-anzeigen","position":11},{"hierarchy":{"lvl1":"3.3 Boxplots mit Plotly","lvl2":"Ausreißer berechnen und visualisieren"},"type":"lvl2","url":"/chapter03-sec03#ausrei-er-berechnen-und-visualisieren","position":12},{"hierarchy":{"lvl1":"3.3 Boxplots mit Plotly","lvl2":"Ausreißer berechnen und visualisieren"},"content":"Die Box im Boxplot enthält 50 % aller Datenpunkte, denn sie ist durch das untere\nQuartil Q1 und das obere Quartil Q3 begrenzt. Die Differenz zwischen Q1 und Q3,\nalso IQR = Q3 - Q1, wird Interquartilsabstand (manchmal auch kurz\nQuartilsabstand) genannt und mit IQR (englisch für Interquartile Range)\nabgekürzt. In der Statistik werden Punkte als Ausreißer angesehen, die kleiner\nals Q1 - 1.5 IQR oder größer als Q3 + 1.5 IQR sind. Dabei ist die Wahl des\nFaktors 1.5 eine statistische Konvention, die sich in der Praxis bewährt hat.\n\nIn unserem Beispiel mit den Autopreisen kommen keine Ausreißer vor, weil Minimum\nund Maximum noch innerhalb dieses Bereichs liegen. Wir fügen daher noch ein\nneues, teureres Auto ein. Jetzt sehen wir einen Ausreißer.\n\npreise_mit_name['BMW Nr. 3'] = 62999\ndiagramm = px.box(preise_mit_name, \n              labels={'variable': 'Name des Datensatzes', 'value': 'Verkaufspreis [EUR]'},\n              points='outliers')\ndiagramm.show()\n\n\n\nWir beenden dieses Kapitel mit einer Mini-Übung.\n\nMini-Übung\n\nErstellen Sie einen Boxplot mit folgenden Eigenschaften:\n\nBenennen Sie das Series-Objekt mit dem Namen “Gebrauchtwagen”\n\nBeschriften Sie die y-Achse mit “Preis [EUR]”\n\nFügen Sie den Titel “Gebrauchtwagen-Analyse” hinzu\n\nZeigen Sie alle Datenpunkte an\n\nZusatzfrage: Welche Autos liegen preislich am weitesten auseinander?\n\n# Hier Ihr Code\n\n\n\nLösungpreise_benannt = pd.Series(preisliste, \n                           index=['Audi Nr. 1', 'Audi Nr. 2', 'Audi Nr. 3',\n                                  'BMW Nr. 1', 'BMW Nr. 2', \n                                  'Citroen Nr. 1', 'Citroen Nr. 2', 'Citroen Nr. 3',\n                                  'Citroen Nr. 4', 'Citroen Nr. 5'],\n                           name='Gebrauchtwagen')\n\ndiagramm = px.box(preise_benannt,\n                  labels={'value': 'Preis [EUR]'},\n                  title='Gebrauchtwagen-Analyse',\n                  points='all')\ndiagramm.show()\n\nZusatzfrage: Audi Nr. 1 (1.999 EUR, günstigstes) und BMW Nr. 1 (46.830 EUR,\nteuerstes) liegen am weitesten auseinander.","type":"content","url":"/chapter03-sec03#ausrei-er-berechnen-und-visualisieren","position":13},{"hierarchy":{"lvl1":"3.3 Boxplots mit Plotly","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter03-sec03#zusammenfassung-und-ausblick","position":14},{"hierarchy":{"lvl1":"3.3 Boxplots mit Plotly","lvl2":"Zusammenfassung und Ausblick"},"content":"Der Boxplot ermöglicht eine einfache Visualisierung der wichtigsten\nstatistischen Kennzahlen eines Datensatzes. Seine Stärke zeigt sich besonders,\nsobald mehrere Datensätze miteinander verglichen werden sollen. Daher werden wir\nim nächsten Kapitel uns mit Tabellen beschäftigen.","type":"content","url":"/chapter03-sec03#zusammenfassung-und-ausblick","position":15},{"hierarchy":{"lvl1":"Übungen"},"type":"lvl1","url":"/chapter03-sec04","position":0},{"hierarchy":{"lvl1":"Übungen"},"content":"Gegeben sind folgende Daten zu der Verteilung von Studierenden\n(männlich/weiblich) auf die Hochschularten Universität und Fachhochschulen\n(Hochschulen für angewandte Wissenschaften), Quelle:\n[\n\nhttps://​www​.statistischebibliothek​.de​/mir​/receive​/DESerie​_mods​_00007716]bundeslaender = ['Baden-Württemberg', 'Bayern', 'Berlin', 'Brandenburg', \n                 'Bremen', 'Hamburg', 'Hessen', 'Mecklenburg-Vorpommern', \n                 'Niedersachsen', 'Nordrhein-Westfalen', 'Rheinland-Pfalz',\n                 'Saarland', 'Sachsen', 'Sachsen-Anhalt',\n                 'Schleswig-Holstein', 'Thüringen']\nstudierende_universitaeten_maennlich = [85183, 118703, 58682, 15845,\n                                        9291, 27444, 68753, 10349, \n                                        62192, 235564, 31487, 7806, \n                                        35826, 15847, 16548, 14350]\nstudierende_universitaeten_weiblich = [82635, 131158, 65587, 18742,\n                                       10181, 28438, 75292, 12821,\n                                       69866, 246467, 41755, 8391,\n                                       37669, 17061, 22760, 17245]\nstudierende_fachhochschulen_maennlich = [83058, 81163, 34727, 7778,\n                                         8299, 26818, 53998, 7120,\n                                         33147, 132976, 21759, 7407,\n                                         15497, 12023, 14167, 39330]\nstudierende_fachhochschulen_weiblich = [65332, 63198, 33333, 6323,\n                                        8235, 33558, 47600, 6886,\n                                        27157, 106755, 18042, 5767,\n                                        11087, 11273, 7943, 63669]\n\nÜbung 3.1\n\nSpeichern Sie die Daten zu den Studentinnen an Fachhochschulen als Pandas-Series.\nVerschaffen Sie sich einen Überblick über die statistischen Kennzahlen. Lesen\nSie dann ab: In welchem Bundesland studieren die wenigsten Studentinnen und im\nwelchem Bundesland die meisten?\n\nLösung\n\nZuerst speichern wir den Datensatz als Pandas-Series in der Variable\nstud_fh_weiblich und verschaffen uns einen Überblick über die statistischen\nKennzahlen, um Minimum und Maximum zu bestimmen.import pandas as pd\n\nstud_fh_weiblich = pd.Series(data=studierende_fachhochschulen_weiblich, index=bundeslaender, name='Studentinnen an Fachhochschulen')\nstud_fh_weiblich.describe()\n\nDie minimale Anzahl an Studentinnen ist 5767, die maximale Anzahl 106755. Mit print\nlassen wir den kompletten Datensatz anzeigen:print(stud_fh_weiblich)\n\nWir lesen ab: am wenigsten Studentinnen an Fachhochschulen studieren im Saarland\nund am meisten in Nordrhein-Westfalen.\n\nÜbung 3.2\n\nWählen Sie einen der drei verbleibenden Datensätze aus:\n\nStudenten an Universitäten\n\nStudentinnen an Universitäten\n\nStudenten an Fachhochschulen\n\nErstellen Sie ein Series-Objekt für diesen Datensatz und überprüfen Sie, ob auch\ndort das Saarland das Minimum und Nordrhein-Westfalen das Maximum hat. Lassen\nSie Minimum und Maximum mit .min() und .max() ausgeben und kontrollieren Sie\ndurch Anzeige des Datensatzes, welches Bundesland dazugehört.\n\nZusatzfrage: Was vermuten Sie für die anderen beiden Datensätze, die Sie nicht\nuntersucht haben? Begründen Sie Ihre Vermutung.\n\nLösung\n\nBeispiel für Studenten an Universitäten:import pandas as pd\n\nstud_uni_maennlich = pd.Series(data=studierende_universitaeten_maennlich, \n                                index=bundeslaender, \n                                name='Studenten an Universitäten')\n\nprint(f'Minimum: {stud_uni_maennlich.min()}, Maximum: {stud_uni_maennlich.max()}')\nprint(stud_uni_maennlich)\n\nErgebnis: Minimum im Saarland (7806) und Maximum in Nordrhein-Westfalen (235564).\n\nZusatzfrage: Vermutlich haben auch die anderen beiden Datensätze das Minimum im\nSaarland und das Maximum in Nordrhein-Westfalen. Begründung: Die Studierendenzahlen\nhängen stark von der Größe und Bevölkerungszahl des Bundeslandes ab. Das Saarland\nist das kleinste Flächenland, Nordrhein-Westfalen das bevölkerungsreichste\nBundesland Deutschlands.\n\nÜbung 3.3\n\nLassen Sie die Datensätze zu Studentinnen an Fachhochschulen und Studenten an\nFachhochschulen durch Boxplots visualisieren.\n\nTeil A: Erstellen Sie den ersten Boxplot für Studentinnen an Fachhochschulen\nmit folgenden Eigenschaften:\n\nBenennen Sie das Series-Objekt mit ‘Studentinnen FH’.\n\nBeschriften Sie die y-Achse mit ‘Anzahl Studierende’.\n\nSetzen Sie den Titel ‘Studentinnen an Fachhochschulen’.\n\nZeigen Sie alle Datenpunkte neben dem Boxplot an.\n\nTeil B: Erstellen Sie analog einen zweiten Boxplot für Studenten an\nFachhochschulen. Achten Sie darauf, eine andere Variablennamen für das Diagramm\nzu verwenden (z.B. diagramm2 statt diagramm), damit der erste Boxplot nicht\nüberschrieben wird.\n\nInterpretationsfrage: Gibt es Ausreißer? Wenn ja, bei welchem Datensatz und\nwelches Bundesland ist betroffen?\n\nLösung\n\nTeil A: Boxplot für Studentinnen an Fachhochschulen:import pandas as pd\nimport plotly.express as px\n\nstud_fh_weiblich = pd.Series(data=studierende_fachhochschulen_weiblich,\n                              index=bundeslaender,\n                              name='Studentinnen FH')\n\ndiagramm1 = px.box(stud_fh_weiblich,\n                   labels={'value': 'Anzahl Studierende'},\n                   title='Studentinnen an Fachhochschulen',\n                   points='all')\ndiagramm1.show()\n\nTeil B: Boxplot für Studenten an Fachhochschulen:stud_fh_maennlich = pd.Series(data=studierende_fachhochschulen_maennlich,\n                               index=bundeslaender,\n                               name='Studenten FH')\n\ndiagramm2 = px.box(stud_fh_maennlich,\n                   labels={'value': 'Anzahl Studierende'},\n                   title='Studenten an Fachhochschulen',\n                   points='all')\ndiagramm2.show()\n\nInterpretationsfrage: Ja, es gibt einen Ausreißer beim Datensatz “Studenten an\nFachhochschulen”. Das betroffene Bundesland ist Nordrhein-Westfalen mit 132.976\nStudenten. Bei den Studentinnen an Fachhochschulen gibt es keinen Ausreißer.\n\nÜbung 3.4\n\nErstellen Sie für alle vier Datensätze Boxplots mit aussagekräftigen\nBeschriftungen:\n\nStudentinnen an Fachhochschulen\n\nStudenten an Fachhochschulen\n\nStudenten an Universitäten\n\nStudentinnen an Universitäten\n\nVerwenden Sie für jeden Boxplot:\n\nEinen Namen für das Series-Objekt\n\nDie Beschriftung 'Anzahl Studierende' für die y-Achse\n\nEinen passenden Titel\n\nHinweis: Die Boxplots müssen nicht einzeln angezeigt werden, speichern Sie\nsie aber in den Variablen fig1, fig2, fig3 und fig4, damit Sie sie in\nder nächsten Aufgabe vergleichen können.\n\nLösungimport pandas as pd\nimport plotly.express as px\n\n# Series-Objekte erstellen\nstud_fh_weiblich = pd.Series(data=studierende_fachhochschulen_weiblich, \n                              index=bundeslaender, \n                              name='Studentinnen FH')\nstud_fh_maennlich = pd.Series(data=studierende_fachhochschulen_maennlich, \n                               index=bundeslaender, \n                               name='Studenten FH')\nstud_uni_maennlich = pd.Series(data=studierende_universitaeten_maennlich, \n                                index=bundeslaender, \n                                name='Studenten Uni')\nstud_uni_weiblich = pd.Series(data=studierende_universitaeten_weiblich, \n                               index=bundeslaender, \n                               name='Studentinnen Uni')\n\n# Boxplots erstellen\nfig1 = px.box(stud_fh_weiblich,\n              labels={'value': 'Anzahl Studierende'},\n              title='Studentinnen an Fachhochschulen')\n\nfig2 = px.box(stud_fh_maennlich,\n              labels={'value': 'Anzahl Studierende'},\n              title='Studenten an Fachhochschulen')\n\nfig3 = px.box(stud_uni_maennlich,\n              labels={'value': 'Anzahl Studierende'},\n              title='Studenten an Universitäten')\n\nfig4 = px.box(stud_uni_weiblich,\n              labels={'value': 'Anzahl Studierende'},\n              title='Studentinnen an Universitäten')\n\n# Optional: Einen oder alle anzeigen\nfig1.show()\n\nÜbung 3.5\n\nVergleichen Sie die vier Boxplots miteinander, indem Sie sie nacheinander mit\n.show() anzeigen lassen. Nutzen Sie die Hover-Funktion (Maus über die Box\nbewegen), um die Werte abzulesen.\n\nTeil A: Erstellen Sie eine Vergleichstabelle in einer Markdown-Zelle:\n\nDatensatz\n\nMedian\n\nQ1 (25%)\n\nQ3 (75%)\n\nAusreißer vorhanden?\n\nStudentinnen FH\n\n...\n\n...\n\n...\n\nja/nein\n\nStudenten FH\n\n...\n\n...\n\n...\n\nja/nein\n\nStudentinnen Uni\n\n...\n\n...\n\n...\n\nja/nein\n\nStudenten Uni\n\n...\n\n...\n\n...\n\nja/nein\n\nTeil B: Beantworten Sie folgende Fragen:\n\nAn welcher Hochschulart (Uni oder FH) ist die Streuung der Studierendenzahlen\ngrößer?\n\nBei welchem Datensatz liegt der Median am weitesten von der Mitte zwischen\nQ1 und Q3 entfernt? Was bedeutet das?\n\nWelche Bundesländer tauchen als Ausreißer auf?\n\nLösung\n\nTeil A: Boxplots anzeigen und Werte ablesen:# Alle vier Boxplots nacheinander anzeigen\nfig1.show()  # Studentinnen FH\nfig2.show()  # Studenten FH\nfig3.show()  # Studenten Uni\nfig4.show()  # Studentinnen Uni\n\nVergleichstabelle:\n\nDatensatz\n\nMedian\n\nQ1 (25%)\n\nQ3 (75%)\n\nAusreißer vorhanden?\n\nStudentinnen FH\n\n22599.5\n\n8162\n\n51499.5\n\nnein\n\nStudenten FH\n\n24288.5\n\n11092\n\n42997.0\n\nja (NRW)\n\nStudentinnen Uni\n\n33053.5\n\n17199.0\n\n71222.5\n\nja (NRW)\n\nStudenten Uni\n\n29465.5\n\n15471.25\n\n63832.25\n\nja (NRW)\n\nTeil B: Interpretation:\n\nStreuung Uni vs. FH: Die Streuung ist an Universitäten deutlich größer. Der\nInterquartilabstand (Q3 - Q1) beträgt bei Unis ca. 48000 (Männer) bzw. 54000\n(Frauen), bei FH nur ca. 32000 (Männer) bzw. 43000 (Frauen). Das liegt\nvermutlich daran, dass es in großen Bundesländern mehr Universitäten gibt.\n\nAsymmetrische Verteilung: Bei Studentinnen an Fachhochschulen liegt der\nMedian (22599.5) deutlich näher an Q1 (11092) als an Q3 (51499.5). Das\nbedeutet: Die Mehrzahl der Bundesländer hat wenige Studentinnen an\nFachhochschulen, aber einige wenige Bundesländer haben sehr viele.\n\nAusreißer: Nordrhein-Westfalen taucht bei drei Datensätzen als Ausreißer\nauf (nicht bei Studentinnen FH). Das ist plausibel, da NRW das\nbevölkerungsreichste Bundesland ist.","type":"content","url":"/chapter03-sec04","position":1},{"hierarchy":{"lvl1":"4.1 Datenstruktur DataFrame"},"type":"lvl1","url":"/chapter04-sec01","position":0},{"hierarchy":{"lvl1":"4.1 Datenstruktur DataFrame"},"content":"Bisher haben wir uns mit Datenreihen beschäftigt, sozusagen eindimensionalen\nArrays. Das Modul Pandas stellt zur Verwaltung von Datenreihen die Datenstruktur\nSeries zur Verfügung. In diesem Kapitel lernen wir die Datenstruktur\nDataFrame kennen, die die Verwaltung von tabellarischen Daten ermöglicht,\nalso sozusagen zweidimensionalen Arrays.","type":"content","url":"/chapter04-sec01","position":1},{"hierarchy":{"lvl1":"4.1 Datenstruktur DataFrame","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter04-sec01#lernziele","position":2},{"hierarchy":{"lvl1":"4.1 Datenstruktur DataFrame","lvl2":"Lernziele"},"content":"Lernziele\n\nSie kennen die Datenstruktur DataFrame.\n\nSie kennen das csv-Dateiformat.\n\nSie können eine csv-Datei mit read_csv() einlesen.\n\nSie können die ersten Zeilen eines DataFrames mit .head() anzeigen lassen.\n\nSie konnen mit .info() sich einen Überblick über die importierten Daten\nverschaffen.\n\nSie können mit .describe() die statistischen Kennzahlen ermitteln.","type":"content","url":"/chapter04-sec01#lernziele","position":3},{"hierarchy":{"lvl1":"4.1 Datenstruktur DataFrame","lvl2":"Was ist ein DataFrame?"},"type":"lvl2","url":"/chapter04-sec01#was-ist-ein-dataframe","position":4},{"hierarchy":{"lvl1":"4.1 Datenstruktur DataFrame","lvl2":"Was ist ein DataFrame?"},"content":"Bei Auswertung von Messungen ist der häufigste Fall der, dass Daten in Form\neiner Tabelle vorliegen. Ein DataFrame-Objekt entspricht einer Tabelle, wie man\nsie beispielsweise von Excel, LibreOffice oder Numbers kennt. Sowohl Zeile als\nauch Spalten sind indiziert. Typischerweise werden die Daten in der Tabelle\nzeilenweise angeordnet. Damit ist gemeint, dass jede Zeile einen Datensatz\ndarstellt und die Spalten die Eigenschaften speichern.\n\nEin DataFrame kann direkt über mehrere Pandas-Series-Objekte oder verschachtelte\nListen erzeugt werden. Da es in der Praxis nur selten vorkommt und nur für sehr\nkleine Datenmengen praktikabel ist, Daten händisch zu erfassen, fokussieren wir\ngleich auf die Erzeugung von DataFrame-Objekten aus einer Datei.","type":"content","url":"/chapter04-sec01#was-ist-ein-dataframe","position":5},{"hierarchy":{"lvl1":"4.1 Datenstruktur DataFrame","lvl2":"Import von Tabellen mit .read_csv()"},"type":"lvl2","url":"/chapter04-sec01#import-von-tabellen-mit-read-csv","position":6},{"hierarchy":{"lvl1":"4.1 Datenstruktur DataFrame","lvl2":"Import von Tabellen mit .read_csv()"},"content":"Tabellen liegen werden oft in dem Dateiformat abgespeichert, das die jeweilige\nTabellenkalkulationssoftware Excel, Numbers oder LibreOffice Calc als Standard\neingestellt hat. Wir betrachten in dieser Vorlesung Tabellen, die in einem\noffenen Standardformat vorliegen und damit unabhängig von der verwendeten\nSoftware und dem verwendeten Betriebssystem sind.\n\nDas Dateiformat CSV speichert Daten zeilenweise ab. Dabei steht CSV für\n“comma separated value”. Die Trennung der Spalten erfolgt durch ein\nTrennzeichen, normalerweise durch das Komma. Im deutschsprachigen Raum wird\ngelegentlich ein Semikolon verwendet, weil im deutschprachigen Raum das Komma\nals Dezimaltrennzeichen verwendet wird.\n\nUm Tabellen im csv-Format einzulesen, bietet Pandas eine eigene Funktion namens\nread_csv an (siehe \n\nDokumentation →\nread_csv).\nWird diese Funktion verwendet, um die Daten zu importieren, so wird automatisch\nein DataFrame-Objekt erzeugt. Beim Aufruf der Funktion wird mindestens der\nDateiname übergeben. Zusäztliche Optionen können über optionale Argumente\neingestellt werden. Beispielweise könnte auch das Semikolon als Trennzeichen\neingestellt werden.\n\nAm besten sehen wir uns die Funktionsweise von read_csv an einem Beispiel an.\nSollten Sie mit einem lokalen Jupyter Notebook arbeiten, laden Sie bitte die\nDatei \n\nautoscout24_xxs.csv herunter und speichern Sie sie in\ndenselben Ordner, in dem auch dieses Jupyter Notebook liegt. Alternativ können\nSie die csv-Datei auch über die URL importieren, wie es in der folgenden\nCode-Zelle gemacht wird. Die csv-Datei enthält die Angaben zu 10 Autos, die auf\n\n\nAutoscout24 zum Verkauf angeboten wurden.\n\nFühren Sie dann anschließend die folgende Code-Zelle aus.\n\nimport pandas as pd\n\ntabelle = pd.read_csv('autoscout24_xxs.csv')\n\n\n\nEs erscheint keine Fehlermeldung, aber den Inhalt der geladenen Datei sehen wir\ntrotzdem nicht. Dazu verwenden wir die Methode .head().","type":"content","url":"/chapter04-sec01#import-von-tabellen-mit-read-csv","position":7},{"hierarchy":{"lvl1":"4.1 Datenstruktur DataFrame","lvl2":"Anzeige der ersten Zeilen mit .head()"},"type":"lvl2","url":"/chapter04-sec01#anzeige-der-ersten-zeilen-mit-head","position":8},{"hierarchy":{"lvl1":"4.1 Datenstruktur DataFrame","lvl2":"Anzeige der ersten Zeilen mit .head()"},"content":"Probieren wir einfachmal aus, was die Anwendung der Methode .head() bewirkt.\n\ntabelle.head()\n\n\n\nDie Methode .head() zeigt uns die ersten fünf Zeilen der Tabelle an. Wenn wir\nbeispielsweise die ersten 10 Zeilen anzeigen lassen wollen, so verwenden wir die\nMethode .head() mit dem Argument 10, also .head(10):\n\ntabelle.head(10)\n\n\n\nOffensichtlich wurde beim Import der Daten wieder ein impliziter Index 0, 1, 2,\nusw. gesetzt. Das ist nicht weiter verwunderlich, denn Pandas kann nicht wissen,\nwelche Spalte wir als Index vorgesehen haben. Und manchmal ist ein automatisch\nerzeugter impliziter Index auch nicht schlecht. In diesem Fall würden wir aber\ngerne als Zeilenindex die Auto-IDs verwenden. Daher modifizieren wir den Befehl\nread_csv mit dem optionalen Argument index_col=. Die Namen stehen in der 1.\nSpalte, was in Python-Zählweise einer 0 entspricht.\n\ntabelle = pd.read_csv('autoscout24_xxs.csv', index_col=0)\ntabelle.head(10)\n\n\n\n","type":"content","url":"/chapter04-sec01#anzeige-der-ersten-zeilen-mit-head","position":9},{"hierarchy":{"lvl1":"4.1 Datenstruktur DataFrame","lvl2":"Übersicht verschaffen mit .info()"},"type":"lvl2","url":"/chapter04-sec01#id-bersicht-verschaffen-mit-info","position":10},{"hierarchy":{"lvl1":"4.1 Datenstruktur DataFrame","lvl2":"Übersicht verschaffen mit .info()"},"content":"Das obige Beispiel zeigt uns zwar nun die ersten 10 Zeilen des importierten\nDatensatzes, aber wie viele Daten insgesamt enthalten sind, können wir mit der\n.head()-Methode nicht erfassen. Dafür stellt Pandas die Methode .info() zur\nVerfügung. Probieren wir es einfach aus.\n\ntabelle.info()\n\n\n\nMit .info() erhalten wir eine Übersicht, wie viele Spalten es gibt und auch\ndie Spaltenüberschriften werden aufgelistet.\n\nWeiterhin entnehmen wir der Ausgabe von .info(), dass in jeder Spalte 10\nEinträge sind, die ‘non-null’ sind. Damit ist gemeint, dass diese Zellen beim\nImport nicht leer waren. Zudem wird bei jeder Spalte noch der Datentyp\nangegeben. Für die Marke oder das Modell, die als Strings gespeichert sind, wird\nder allgemeine Datentyp ‘object’ angegeben. Beim Jahr oder dem Preis wurden\nkorrektweise Integer erkannt. Der Verbrauch (Liter pro 100 Kilometer) wird als\nFloat gespeichert.","type":"content","url":"/chapter04-sec01#id-bersicht-verschaffen-mit-info","position":11},{"hierarchy":{"lvl1":"4.1 Datenstruktur DataFrame","lvl2":"Statistische Kennzahlen mit .describe()"},"type":"lvl2","url":"/chapter04-sec01#statistische-kennzahlen-mit-describe","position":12},{"hierarchy":{"lvl1":"4.1 Datenstruktur DataFrame","lvl2":"Statistische Kennzahlen mit .describe()"},"content":"So wie die Methode .info() uns einen schnellen Überblick über die Daten eines\nDataFrame-Objektes gibt, so liefert die Methode .describe() eine schnelle\nÜbersicht über statistische Kennzahlen.\n\ntabelle.describe()\n\n\n\nDa es sich eingebürgert hat, Daten zeilenweise abzuspeichern und die Eigenschaft\npro einzelnem Datensatz in den Spalten zu speichern, wertet .describe() jede\nSpalte für sich aus. Für jede Eigenschaft werden dann die statistischen\nKennzahlen\n\ncount\n\nmean\n\nstd\n\nmin\n\nmax\n\nQuantile 25 %, 50 % und 75 %\n\nmax\n\nausgegeben.\n\nDie Bedeutung der Kennzahlen wird in der\n\n\nDokumentation → describe()\nerläutert. Sie entsprechen den statistischen Kennzahlen, die die Methode\n.describe() für Series-Objekte liefert. Pandas hat hier auch auf den Datentyp\nreagiert. Nur für numerische Werte (Integer oder Float) wurden die statistischen\nKennzahlen ermittelt.","type":"content","url":"/chapter04-sec01#statistische-kennzahlen-mit-describe","position":13},{"hierarchy":{"lvl1":"4.1 Datenstruktur DataFrame","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter04-sec01#zusammenfassung-und-ausblick","position":14},{"hierarchy":{"lvl1":"4.1 Datenstruktur DataFrame","lvl2":"Zusammenfassung und Ausblick"},"content":"Mit Hilfe der Datenstruktur DataFrame können tabellarische Daten verwaltet\nwerden. In den nächsten Kapiteln werden wir uns damit beschäftigen, auf einzelne\nSpalten oder Zeilen zuzugreifen und die Datenpunkte als sogenannten Scatterplot\nzu visualisieren.","type":"content","url":"/chapter04-sec01#zusammenfassung-und-ausblick","position":15},{"hierarchy":{"lvl1":"4.2 Arbeiten mit Tabellendaten"},"type":"lvl1","url":"/chapter04-sec02","position":0},{"hierarchy":{"lvl1":"4.2 Arbeiten mit Tabellendaten"},"content":"In Tabellenkalkulationssoftware ist es möglich, einzelne Zeilen oder Spalten zu\nbearbeiten. Pandas mit seiner Dantestruktur DataFrame bietet diese Möglichkeit\nebenfalls. Wie auf einzelne Spalten und Zeilen zugegriffen wird und wie die\nDaten bearbeitet werden können, zeigt dieses Kapitel.","type":"content","url":"/chapter04-sec02","position":1},{"hierarchy":{"lvl1":"4.2 Arbeiten mit Tabellendaten","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter04-sec02#lernziele","position":2},{"hierarchy":{"lvl1":"4.2 Arbeiten mit Tabellendaten","lvl2":"Lernziele"},"content":"Lernziele\n\nSie können mit eckigen Klammern [] und dem Spaltenindex auf eine ganze\nSpalte zugreifen.\n\nSie können mit .loc[] und dem Zeilenindex auf eine ganze Zeile zugreifen.\n\nSie können mit .loc[zeileindex, spaltenindex] auf eine einzelne Zelle der\nTabelle zugreifen.\n\nSie können mehrere unzusammenhängende Zeilen/Spalten mittels Liste auswählen.\n\nSie können zusammenhängende Bereich mittels Slicing auswählen.\n\nSie können eine Tabelle um eine Zeile oder Spalte erweitern.","type":"content","url":"/chapter04-sec02#lernziele","position":3},{"hierarchy":{"lvl1":"4.2 Arbeiten mit Tabellendaten","lvl2":"Zugriff auf Spalten"},"type":"lvl2","url":"/chapter04-sec02#zugriff-auf-spalten","position":4},{"hierarchy":{"lvl1":"4.2 Arbeiten mit Tabellendaten","lvl2":"Zugriff auf Spalten"},"content":"Bei einer Liste oder der Pandas-Datenstruktur Series haben wir auf ein einzelnes\nElement zugegriffen, indem wir eckige Klammern benutzt haben. Bei Tabellen und\ndamit auch DataFrames ist es üblich, dass die Eigenschaften in den Spalten\nstehen und in den Zeilen die einzelnen Datensätze. Mit den eckigen Klammern und\ndem Indexnamen greifen wir diesmal also nicht nur ein Element heraus, sondern\ngleich eine ganze Spalte.\n\nFalls Sie aus dem vorherigen Kapitel den Datensatz {download}Download autoscout24_xxs.csv <https://gramschs.github.io/book_ml4ing/data/autoscout24_xxs.csv> noch geladen\nhaben, können Sie sich mit .info() die Spaltenüberschriften, also den Index,\ndirekt anzeigen lassen. Ansonsten importieren Sie zuerst Pandas mit seiner\nüblichen Abkürzung pd und laden den Datensatz.\n\nimport pandas as pd\ntabelle = pd.read_csv('autoscout24_xxs.csv', index_col=0)\ntabelle.info()\n\n\n\nDie Farbe der 10 Autos können wir folgendermaßen aus der Tabelle auswählen:\n\nfarbe = tabelle['Farbe']\n\n\n\nWas steckt jetzt in der Variable farbe? Ermitteln wir zunächst, welchen\nDatentyp das Objekt hat, das in farbe gespeichert ist.\n\ntype(farbe)\n\n\n\nEs ist ein Series-Objekt mit dem Namen Farbe, also dem Spaltenindex. Das neu\nerzeugte Series-Objekt kann also beispielsweise mit .head() angezeigt werden.\n\nfarbe.head()\n\n\n\nEin DataFrame besteht aus Series-Objekten.","type":"content","url":"/chapter04-sec02#zugriff-auf-spalten","position":5},{"hierarchy":{"lvl1":"4.2 Arbeiten mit Tabellendaten","lvl2":"Zugriff auf Zeilen"},"type":"lvl2","url":"/chapter04-sec02#zugriff-auf-zeilen","position":6},{"hierarchy":{"lvl1":"4.2 Arbeiten mit Tabellendaten","lvl2":"Zugriff auf Zeilen"},"content":"Natürlich kann es auch Gründe geben, sich einen einzelnen Datensatz mit allen\nEigenschaften herauszugreifen. Oder anders ausgedrückt, vielleicht möchte man in\nder Tabelle eine einzelne Zeile auswählen. Dazu gibt es das Attribut .loc.\nDanach werden wieder eckige Klammern benutzt, wobei diesmal der Zeilenindex\nverwendet wird.\n\nDer folgende Code-Schnippsel speichert die Zeile des 4. Autos (= BMW Nr. 1) in\nder Variable viertes_auto ab. Wir ermitteln gleich den Datentyp dazu.\n\nviertes_auto = tabelle.loc['BMW Nr. 1']\ntype(viertes_auto)\n\n\n\nAuch eine einzelne Zeile ist eine Series-Datenstruktur, die wir mit den\nSeries-Methoden weiter bearbeiten können. Der Name des Series-Objektes ist\ndiesmal der alte Zeilenindex. Wir lassen den Datensatz mit .head() anzeigen.\n\nviertes_auto.head()\n\n\n\n","type":"content","url":"/chapter04-sec02#zugriff-auf-zeilen","position":7},{"hierarchy":{"lvl1":"4.2 Arbeiten mit Tabellendaten","lvl2":"Zugriff auf Zellen"},"type":"lvl2","url":"/chapter04-sec02#zugriff-auf-zellen","position":8},{"hierarchy":{"lvl1":"4.2 Arbeiten mit Tabellendaten","lvl2":"Zugriff auf Zellen"},"content":"Es kann auch vorkommen, dass man gezielt auf eine einzelne Zelle zugreifen\nmöchte. Auch dazu benutzen wir das Attribut .loc[]. Für eine einzelne Zelle\nmüssen wir angeben, in welcher Zeile und in welcher Spalte sich diese Zelle\nbefindet. Das Attribut .loc[] ermöglicht auch zwei Angaben, also Zeile und\nSpalte, indem beide Werte durch ein Komma getrennt werden.\n\nWollen wir beispielsweise wissen, wann der Audi Nr. 3 zum zugelassen wurde, so\ngehen wir folgendermaßen vor:\n\nerstzulassung_audi3 = tabelle.loc['Audi Nr. 3', 'Erstzulassung']\ntype(erstzulassung_audi3)\n\n\n\nJetzt erhalten wir keine Series-Datenstruktur zurück, sondern den Datentyp des\nElements in dieser Zelle. In unserem Beispiel ist die Erstzulassung als String\ngespeichert, den wir mit der print()-Funktion ausgeben lassen können:\n\nprint(erstzulassung_audi3 )\n\n\n\nWir Menschen können diesen String natürlich interpretieren und sehen, dass der\nAudi Nr. 3 im November 2018 zum ersten Mal zugelassen wurde. Für Python ist es\nan dieser Stelle aber nicht möglich, eine korrekte Interpretation des Strings zu\nbieten.","type":"content","url":"/chapter04-sec02#zugriff-auf-zellen","position":9},{"hierarchy":{"lvl1":"4.2 Arbeiten mit Tabellendaten","lvl2":"Mehrere Zeilen oder Spalten"},"type":"lvl2","url":"/chapter04-sec02#mehrere-zeilen-oder-spalten","position":10},{"hierarchy":{"lvl1":"4.2 Arbeiten mit Tabellendaten","lvl2":"Mehrere Zeilen oder Spalten"},"content":"Sollen mehrere Zeilen oder Spalten gleichzeitig ausgewählt werden, so werden die\nentsprechenden Indizes als eine Liste in die eckigen Klammern gesetzt.\n\nDer folgende Code wählt sowohl die Erstzulassung als auch den Preis aus.\n\nmehrere_spalten = tabelle[ ['Erstzulassung', 'Preis (Euro)'] ]\nmehrere_spalten.head()\n\n\n\nWenn die Spalten oder Zeilen nacheinander kommen, also zusammenhängend sind,\nbrauchen wir nicht alle Indizes in die Liste schreiben. Dann genügt es, den\nersten Index und den letzten Index zu nehmen und dazwischen einen Doppelpunkt zu\nsetzen. Diese Art, Zeilen oder Spalten auszuwählen, wird in der Informatik als\nSlicing bezeichnet. Alle Autos der Marke Citroën werden also folgendermaßen\nextrahiert:\n\ncitroens = tabelle.loc[ 'Citroen Nr. 1' : 'Citroen Nr. 5'] \ncitroens.head()\n\n\n\nJetzt kann beispielsweise der mittlere Verkaufspreis aller Citroëns\nfolgendermaßen ermittelt werden:\n\nmittelwert = citroens['Preis (Euro)'].mean()\nprint(f'Der mittlere Verkaufspreis der Citroens ist {mittelwert:.2f} EUR.')\n\n\n\nBeim Slicing können wir den Angangsindex oder den Endindex oder sogar beides\nweglassen. Wenn wir den Anfangsindex weglassen, fängt Pandas bei der ersten\nZeile/Spalte an. Lassen wir den Endindex weg, geht der Slice automatisch bis zum\nEnde.","type":"content","url":"/chapter04-sec02#mehrere-zeilen-oder-spalten","position":11},{"hierarchy":{"lvl1":"4.2 Arbeiten mit Tabellendaten","lvl2":"Neue Spalte oder Zeile einfügen"},"type":"lvl2","url":"/chapter04-sec02#neue-spalte-oder-zeile-einf-gen","position":12},{"hierarchy":{"lvl1":"4.2 Arbeiten mit Tabellendaten","lvl2":"Neue Spalte oder Zeile einfügen"},"content":"Eine neue Spalte einzufügen, funktioniert recht einfach. Dazu wird ein neuer\nSpaltenindex erzeugt.\n\ntabelle['Verbrauch pro Leistung'] = tabelle['Verbrauch (l/100 km)'] / tabelle['Leistung (PS)']\ntabelle.head(10)\n\n\n\nNach demselben Prinzip können wir einen neuen Datensatz aufnehmen und eine neue\nZeile einfügen. Da wir uns auf die Zeilen beziehen, verwenden wir wieder\nloc[].\n\ntabelle.loc['Dacia Nr. 1'] = ['dacia', 'Dacia Duster', 'orange', '03/2023', 2023, 25749, 84, 114, 'Schaltgetriebe', 'Diesel', 5.3, 140, 5.0, 'Journey Blue dCi 115 4x4', 5.3/114] \ntabelle.head(11)\n\n\n\nWarnung\n\nDas oben beschriebene Prinzip, eine neue Spalte oder eine neue Zeile einzufügen,\nindem ein neuer Spalten- oder Zeilenindex hinzugefügt wird, funktioniert nur,\nwenn die neuen Daten das richtige Format haben. Im Zweifelsfall sollte die\nErweiterung eines DataFrames mit\n\n\nconcat\ndurchgeführt werden.","type":"content","url":"/chapter04-sec02#neue-spalte-oder-zeile-einf-gen","position":13},{"hierarchy":{"lvl1":"4.2 Arbeiten mit Tabellendaten","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter04-sec02#zusammenfassung-und-ausblick","position":14},{"hierarchy":{"lvl1":"4.2 Arbeiten mit Tabellendaten","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Kapitel haben wir uns damit beschäftigt, wie tabellarische Daten\nverwaltet werden. Im nächsten Kapitel geht es darum, diese zu visualisieren.","type":"content","url":"/chapter04-sec02#zusammenfassung-und-ausblick","position":15},{"hierarchy":{"lvl1":"4.3 Scatterplots und Scattermatrix"},"type":"lvl1","url":"/chapter04-sec03","position":0},{"hierarchy":{"lvl1":"4.3 Scatterplots und Scattermatrix"},"content":"Bei der Datenvisualisierung geht es darum, Daten durch eine Grafik so\naufzubereiten, dass Muster oder Unregelmäßigkeiten in den Daten entdeckt werden\nkönnen. Dabei kann die visuelle Darstellung der Daten helfen, Muster in den\nDaten zu entdecken, aber sie kann auch irreführend sein. Abhängig davon, wie die\nArt der Daten beschaffen ist, die wir visualisieren wollen, gibt es verschiedene\nDarstellungsformen, die sogenannten Diagrammtypen. Im Folgenden betrachten wir\ndie Diagrammtypen\n\nScatterplot und\n\nScattermatrix.\n\nDanach beschäftigen wir uns mit der Gestaltung bzw. dem Styling von Diagrammen.","type":"content","url":"/chapter04-sec03","position":1},{"hierarchy":{"lvl1":"4.3 Scatterplots und Scattermatrix","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter04-sec03#lernziele","position":2},{"hierarchy":{"lvl1":"4.3 Scatterplots und Scattermatrix","lvl2":"Lernziele"},"content":"Lernziele\n\nSie können mit der Funktion scatter() einen Scatterplot erzeugen, der\nnumerische Daten als Ursache-Wirkungs-Diagramm visualisiert.\n\nSie kennen die folgenden Styling-Optionen\n\nTextannotation text=,\n\nFarbe color= und\n\nGröße size=.\n\nSie können mit title= den Titel des Diagramms setzen.\n\nSie können eine Scattermatrix mit scatter_matrix() erzeugen und\ninterpretieren.","type":"content","url":"/chapter04-sec03#lernziele","position":3},{"hierarchy":{"lvl1":"4.3 Scatterplots und Scattermatrix","lvl2":"Scatterplots"},"type":"lvl2","url":"/chapter04-sec03#scatterplots","position":4},{"hierarchy":{"lvl1":"4.3 Scatterplots und Scattermatrix","lvl2":"Scatterplots"},"content":"Scatterplots (deutsch: Streudiagramme) werden eingesetzt, wenn der Zusammenhang\nzwischen zwei numerischen Größen untersucht werden soll. Das ist vor allem bei\nExperimenten häufig der Fall.\n\nIm Folgenden soll der Scatterplot anhand des Autoscout24-Beispiels\n{download}Download autoscout24_xxs.csv <https://gramschs.github.io/book_ml4ing/data/autoscout24_xxs.csv> demonstriert\nwerden. Dazu laden wir die Tabelle wie üblich mit Pandas.\n\nimport pandas as pd\ndaten = pd.read_csv('autoscout24_xxs.csv', index_col=0)\ndaten.info()\n\n\n\nUns interessieren zunächst die Verkaufspreise der Autos. Zu jedem Auto soll\nentlang der y-Achse der Verkaufspreis aufgetragen werden. Dazu wird zuerst\nPlotly Express mit der üblichen Abkürzung px importiert. Danach nutzen wir die\nFunktion scatter(). Das erste Argument in den runden Klammern ist die\nkomplette Tabelle, also daten. Danach geben wir direkt den Spaltenindex der\nSpalte an, die visualisiert werden soll, also y = 'Preis (Euro)'. Zuletzt\nlassen wir den Scatterplot auch mit .show() anzeigen.\n\nimport plotly.express as px\ndiagramm = px.scatter(daten, y = 'Preis (Euro)')\ndiagramm.show()\n\n\n\nDa wir für die x-Achse keine Angaben gemacht haben, wird automatisch der\nZeilenindex für die x-Achse verwendet.\n\nDer Scatterplot bietet im Vergleich zum Boxplot weitere Informationen.\nBeispielsweise erkennen wir nun, dass die Autos der Marke Citroën eher unter dem\nDurchschnitt liegen. Scatterplots bieten uns auch die Möglichkeit, Muster zu\nvisuell zu erkunden, um Abhängigkeiten von Ursache und Wirkung zu erkunden. Wir\nkönnten beispielsweise auf die Idee kommen, dass der Preis (= Wirkung) auch\nabhängig ist von der Anzahl der gefahrenen Kilometer (= Ursache). Wir setzen die\nvermutete Ursache auf die x-Achse mit dem Argument x = 'Kilometerstand (km)'\nund die vermutete Wirkung auf die y-Achse mit y = 'Preis (Euro)'.\n\ndiagramm = px.scatter(daten, x = 'Kilometerstand (km)', y = 'Preis (Euro)')\ndiagramm.show()\n\n\n\nVon der Tendenz her scheint unsere Vermutung richtig zu sein. Je mehr Kilometer\nein Auto bereits gefahren wurde, desto günstiger ist sein Verkaufspreis.\nAllerdings scheint es zwei Autos zu geben, die nicht ganz in dieses Muster\npassen. Ein Auto wird trotz eines Kilometerstandes von 117433 km für 46 TEUR\nangeboten, an anderes hat nur 15200 km auf dem Buckel, soll aber trotzdem für\nnur 12 TEUR verkauft werden. Aber welche Autos sind die beiden Ausnahmen? Um\nmehr Informationen aus den Daten zu holen, beschäftigen wir uns mit dem Styling\nvon Scatterplots.","type":"content","url":"/chapter04-sec03#scatterplots","position":5},{"hierarchy":{"lvl1":"4.3 Scatterplots und Scattermatrix","lvl2":"Styling von Scatterplots"},"type":"lvl2","url":"/chapter04-sec03#styling-von-scatterplots","position":6},{"hierarchy":{"lvl1":"4.3 Scatterplots und Scattermatrix","lvl2":"Styling von Scatterplots"},"content":"Die Voreinstellungen von Plotly sind bereits sehr gut gewählt, so dass ohne\nweitere Optionen bereits gut aussehende und informative Diagramme erstellt\nwerden können. Eine Möglichkeit, durch das Styling der Diagramme\nZusatzinformationen zu visualisieren, bietet die Option text=. Wir verwenden\nden Zeilenindex als Text, der in dem Attribut .index gespeichert ist.\n\ndiagramm = px.scatter(daten, x = 'Kilometerstand (km)', y = 'Preis (Euro)', text=daten.index)\ndiagramm.show()\n\n\n\nAn jedem Datenpunkt wird nun zusätzlich die Auto-ID eingeblendet. Leider\nüberschreibt der Text den Datenpunkt selbst. Das kann nachträglich geändert\nwerden, indem die Textposition relativ zu den Datenpunkten auf einen anderen\nWert gesetzt wird. Die einzelnen Bestandteile eines Plotly-Express-Diagramms\nheißen trace. Sie werden durch update_traces() aktualisiert oder anders\nausgedrückt, die Voreinstellungen werden dadurch überschrieben. Wir möchten,\ndass die Position der Texte oberhalb der Datenpunkte ist, aber dennoch zentriert\nzum Datenpunkt. Durch das Argument textposition='top center' erreichen wir\ndieses Ziel, wie der folgende Scatterplot zeigt.\n\ndiagramm = px.scatter(daten, x = 'Kilometerstand (km)', y = 'Preis (Euro)', text=daten.index)\ndiagramm.update_traces(textposition='top center')\ndiagramm.show()\n\n\n\nAls nächstes möchten wir weitere Zusatzinformationen in das Diagramm packen.\nNicht immer ist es sinnvoll, so viele Zusatzinformationen in ein Diagramm zu\nbringen, da damit das Publikum auch schnell überfordert werden kann. Daher\nsollte gut überlegt werden, ob die beiden nächsten Möglichkeiten gleichzeitig\ngenutzt werden sollen.\n\nDie Farbe ist eine weitere Möglichkeit, Zusatzinformationen zu visualisieren.\nWie alt ist ein Auto? Hat es ebenfalls einen Einfluss auf den Verkaufspreis? Wir\nnutzen das Jahr der Erstzulassung, um das Alter der Autos abzuschätzen. Die\nAnweisung an Python, die Punkte des Scatterplots nach der Erstzulassung\neinzufärben, wird durch das optionale Argument color='Jahr' gegeben.\n\ndiagramm = px.scatter(daten, x = 'Kilometerstand (km)', y = 'Preis (Euro)', text=daten.index, color='Jahr')\ndiagramm.update_traces(textposition='top center')\ndiagramm.show()\n\n\n\nDie Farbe scheint links gelber zu sein als rechts, wo Auto ‘Audi Nr. 1’ violett\ngefärbt ist. Also scheint das Jahr der Erstzulassung und damit das Alter der\nFahrzeuge auch etwas mit dem Kilometerstand zu tun zu haben, der auf der x-Achse\naufgetragen ist. Je jünger das Fahrzeug ist, desto weniger Kilomter wurde es\nbisher gefahren.\n\nAls zweite Möglichkeit, Zusatzinformationen direkt mit den Datenpunkten im\nScatterplot zu visualisieren, dient die Größe der Punkte. Mit dem optionalen\nArgument size= wird sie gesteuert. Wiederum verwenden wir einen Spaltenindex\nals Argument. Die Leistung könnte erfahrungsgemäß ebenfalls den Verkaufspreis\nbeeinflussen. Also setzen wir size='Leistung (PS)' und betrachten das so\nerweiterte Diagramm.\n\ndiagramm = px.scatter(daten, x = 'Kilometerstand (km)', y = 'Preis (Euro)', text=daten.index, color='Jahr', size='Leistung (PS)')\ndiagramm.update_traces(textposition='top center')\ndiagramm.show()\n\n\n\nDas Auto ‘BMW Nr. 1’, das uns schon zuvor aufgefallen ist, weil der Preis recht\nhoch ist, obwohl das Auto schon einen mittleren Kilomterstand hat, scheint\nbesonders viel PS zu haben. Vielleicht erklärt das den hohen Preis?\n\nAls letzte Styling-Möglichkeit betrachten wir den Titel. Im Gegensatz zu den\nvorherigen Styling-Möglichkeiten, ist der Titel stets Pflicht. Jedes Diagramm\nmuss einen Titel haben! Der Titel wird mit dem Argument title= gesetzt.\n\ndiagramm = px.scatter(daten, x = 'Kilometerstand (km)', y = 'Preis (Euro)', text=daten.index, color='Jahr', size='Leistung (PS)', title='Verkaufsdaten von 10 Autos (Quelle: Autocout24.de)')\ndiagramm.update_traces(textposition='top center')\ndiagramm.show()\n\n\n\n","type":"content","url":"/chapter04-sec03#styling-von-scatterplots","position":7},{"hierarchy":{"lvl1":"4.3 Scatterplots und Scattermatrix","lvl2":"Scattermatrix"},"type":"lvl2","url":"/chapter04-sec03#scattermatrix","position":8},{"hierarchy":{"lvl1":"4.3 Scatterplots und Scattermatrix","lvl2":"Scattermatrix"},"content":"Unsere Tabelle hat sieben Spalten mit numerischen Werten: Jahr, Preis (Euro),\nLeistung (kW), Leistung (PS), Verbrauch (l/100 km), Verbrauch (g/km) und\nKilometerstand (km). Damit können sieben Eigenschaften als Ursache interpretiert\nwerden und auf der x-Achse aufgetragen werden. Zu jeder dieser sieben\nEigenschaften können dann die verbleibenden sechs Eigenschaften als Wirkung\ninterpretiert werden und auf der y-Achse dargestellt werden. Also müssten wir 42\nScatterplots untersuchen. Die Scattermatrix vereinfacht das Zusammenstellen\ndieser Kombinationen. Dazu legen wir erst eine Liste mit den Spaltenindizes an,\ndie in die Scattermatrix aufgenommen werden sollen. Danach erzeugen wir mit der\nFunktion scatter_matrix() die gewünschten Kombinationen. Als erstes Argument\nwerden die Daten aus der Tabelle übergeben, dann folgt die Liste der\nausgewählten Spalten als Argument für den Parameter dimensions=.\n\nauswahl = ['Jahr', 'Preis (Euro)', 'Leistung (kW)', 'Leistung (PS)', 'Verbrauch (l/100 km)', 'Verbrauch (g/km)', 'Kilometerstand (km)']\ndiagramm = px.scatter_matrix(daten, dimensions=auswahl)\ndiagramm.show()\n\n\n\nEs werden 49 Diagramme angezeigt, die allerdings kaum lesbar sind. Warum 49 und\nnicht 42? Tatsächlich wird auch jede Eigenschaft als Ursache mit ihrer Wirkung\nauf sich selbst dargestellt. Da das Diagramm so kaum lesbar ist, reduzieren wir\ndie Auswahl weiter und nehmen nur die ersten vier Eigenschaften.\n\nauswahl = ['Jahr', 'Preis (Euro)', 'Leistung (kW)', 'Leistung (PS)']\ndiagramm = px.scatter_matrix(daten, dimensions=auswahl)\ndiagramm.show()\n\n\n\nAuf der Diagonalen befinden sich die Scatterplots, bei denen dieselbe\nEigenschaft auf der x- und auf der y-Achse aufgetragen ist. Daher müssen diese\nPunkte immer auf der Winkelhalbierenden liegen. Diese Darstellung zeigt uns\nschnell, dass die Auswahl der Autos nicht gleichmäßig bezogen auf das Jahr der\nErstzulassung erfolgt ist. Im Scatterplot Jahr - Jahr ist ein Punkt (1997) sehr\nweit von den restlichen Autos entfernt. Beim Preis hingegen sieht es besser aus,\ndiese Punkte sind entlang der Winkelhalbierenden relativ gleichmäßig verteilt.\nAllerdings zeigen beide Scatterplots für die Leistung wiederum, dass ein Auto\n(kW = 294 bzw. PS = 400) von den anderen Autos entfern ist. Bei beiden Autos\nkönnte man argumentieren, dass sie nicht repräsentativ für den Datensatz sind,\nsondern als Ausreißer betrachtet werden müssen. Es stellt sich die Frage, ob sie\nfür die weitere Datenverarbeitung aus dem Datensatz gelöscht werden sollen.\n\nBetrachten wir den Scatterplot Leistung (kW) vs. Leistung (PS), so stellen wir\nfest, dass die Punkte ebenfalls auf der Winkelhalbierenden liegen. Tatsächlich\nist das nicht verwunderlich, da die Leistung ja nur eine einzige Eigenschaft\ndarstellt, aber in zwei verschiedenen Einheiten angegeben wird. 1 Watt (W) sind\nungefähr 0,00136 Pferdestärken (PS). Die Scattermatrix zeigt uns nun (wenn wir\nes nicht schon vorher wussten), dass wir nur eine der beiden Spalten brauchen.\nDiese Spalte könnte für die weitere Datenexploration gelöscht werden.\n\nauswahl = ['Jahr', 'Preis (Euro)', 'Leistung (kW)']\ndiagramm = px.scatter_matrix(daten, dimensions=auswahl)\ndiagramm.show()\n\n\n\nAls letztes Interpretationsbeispiel betrachten wir die zweite Zeile der Matrix,\nwo der Preis (Euro) auf der y-Achse aufgetragen ist. Betrachten wir den ersten\nScatterplot der zweiten Zeile, so scheint das Jahr keinen besonderen Einfluss\nauf den Verkaufspreis zu haben. Beim dritten Scatterplot in der zweiten Zeile,\nscheint es aber ein Muster zu geben. Mit wachsender Leistung scheint auch der\nVerkaufspreis zu steigen. Die Scattermatrix hilft also gerade zu Beginn der\nDatenexploration schnell, interessante Zusammenhänge zwischen einzelnen\nEigenschaften aufzudecken, die dann durch einzelne Scatterplots näher untersucht\nwerden können.","type":"content","url":"/chapter04-sec03#scattermatrix","position":9},{"hierarchy":{"lvl1":"4.3 Scatterplots und Scattermatrix","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter04-sec03#zusammenfassung-und-ausblick","position":10},{"hierarchy":{"lvl1":"4.3 Scatterplots und Scattermatrix","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Kapitel haben wir uns mit der Visualisierung von numerischen Werten\nbeschäftigt. Die Scattermatrix ordnet alle Kombinationen von einzelnen\nScatterplots in einer Matrix an. Damit können schnell Muster in den Daten\ngefunden werden, deren Abhängigkeiten dann wiederum durch einzelne Scatterplots\ndetaillierter beleuchtet werden können. Bisher haben wir aber nur die\nnumerischen Werte untersucht. Wie auch die nicht-numerischen Werte wie\nbeispielsweise die Farbe der Autos mit in die Visualisierung einbezogen werden\nkönnen, sehen wir im nächsten Kapitel.","type":"content","url":"/chapter04-sec03#zusammenfassung-und-ausblick","position":11},{"hierarchy":{"lvl1":"Übungen"},"type":"lvl1","url":"/chapter04-sec04","position":0},{"hierarchy":{"lvl1":"Übungen"},"content":"Daten für die Übungen:\n\n12612-0001_de.csv\n\nstromverbrauch​_hessen​.csv\n\nÜbung 4.1 - Teilaufgabe a)\n\nSchauen Sie sich die csv-Datei ‘12612-0001_de.csv’ im Texteditor an. Der\nDatensatz enthält die Lebendgeburten in Deutschland getrennt nach männlich,\nweiblich und insgesamt (Quelle: \n\nStatistisches\nBundesamt).\n\nAb welcher Zeile beginnen die Daten und in welcher Zeile enden sie? Importieren\nSie dann die Daten, wobei Kopf- und Fußzeilen beim Import übersprungen werden\nsollen. Schlagen Sie dazu in der Dokumentation nach, wie Zeilen beim Lesen einer\ncsv-Datei Übersprungen werden.\n\nTipp: Importieren Sie pandas mit dem üblichen Alias pd und führen Sie dann die\nfolgende Code-Zeile in einer Code-Zelle aus:pd.read_csv?\n\nWelches Argument könnte für das Überspringen der Kopfzeilen stehen, welches für\ndie Fußzeilen?\n\nWelche Spalte ist als Index-Spalte geeignet? Setzen Sie eine passende\nSpalte als Index. Verschaffen Sie sich einen Überblick über die Daten und\nerstellen Sie eine Übersicht der statistischen Kennzahlen.\n\nLösungimport pandas as pd\n\npd.read_csv?# 5 Kopfzeilen, 4 Fußzeilen; Jahreszahlen sind der Index\ndata = pd.read_csv('data/12612-0001_de.csv', skiprows=5, skipfooter=4, index_col=0)\ndata.info()\n\nDer Datensatz enthält 3 Spalten: männlich, weiblich und insgesamt. Jede Spalte enthält 73 gültige Einträge. Jede Spalte enthält Integer.data.describe()\n\nÜbung 4.1 - Teilaufgabe b)\n\nKontrollieren Sie, ob die Spalte ‘insgesamt’ tatsächlich die Summe der beiden\nSpalten ‘männlich’ und ‘weiblich’ ist. Bilden Sie dazu die Differenz ‘insgesamt’ -\n‘männlich’ - ‘weiblich’ und fügen Sie diese Differenz als neue Spalte dem\nDataFrame hinzu. Wie lauten die statistischen Kennzahlen dieser Spalte? Welchen\nSchluss ziehen Sie daraus? Haben Sie eine Vermutung, was passiert ist?\n\nLösungdata['Differenz'] = data['insgesamt'] - data['männlich'] - data['weiblich']\ndata['Differenz'].describe()\n\nDer Mittelwert ist 0.369863 und nicht wie erwartet 0. Das Maximum ist 17 und\nnicht wie erwartet 0. Die Spalte ‘insgesamt’ ist daher nicht die Summe der\nbeiden Spalten ‘männlich’ und ‘weiblich’. Normalerweise würden wir jetzt nach\nden Jahren filtern, bei denen die Differenz nicht Null ist. Da wir das noch\nnicht gelernt haben, visualisieren wir die Differenz als Scatterplot.import plotly.express as px\n\nfig = px.scatter(data['Differenz'], \n                 title='Differenz \"insgesamt - männlich - weiblich\"',\n                 labels={'value': 'Anzahl Geburten', 'variable': 'Legende'})\nfig.show()\n\nDie Abweichungen treten in den Jahren 2016 und 2017 auf. Das passt zu der\nBemerkung “Ab 2016: Die Gesamtzahl der Lebendgeborenen enthält auch die Fälle\nmit unbestimmtem Geschlecht.” Vermutlich ist die Zahl der Geburten ‘insgesamt’\nhöher als die Summe der Geburten ‘männlich’ + ‘weiblich’, weil das Geschlecht\nunbestimmt war.\n\nÜbung 4.1 - Teilaufgabe c)\n\nLassen Sie die statistischen Kennzahlen der Lebendgeburten ‘männlich’ und\n‘weiblich’ als Boxplot visualisieren. Wurden mehr Jungen oder Mädchen geboren?\nLiegt der Median mittig zwischen Q1 und Q3?\n\nLösungimport plotly.express as px\n\nfig = px.box(data[['männlich','weiblich']],\n             title='Statistische Kennzahlen der Lebendgeburten in Deutschland',\n             labels={'value': 'Anzahl Geburten', 'variable': 'Geschlecht'})\nfig.show()\n\nEs wurden insgesamt mehr Jungen als Mädchen geboren. Bei beiden Geschlechtern\nist auffällig, dass der Median recht nahe bei dem Q1-Quantil liegt. Das deutet\nauf eine asymmetrische Verteilung der Geburten über die Jahre hin.\nWahrscheinlich gibt es wenige Ausreißer mit geburtenstarken Jahrgängen.\n\nÜbung 4.1 - Teilaufgabe d)\n\nVisualisieren Sie die Anzahl der männlichen und weiblichen Lebendgeburten pro\nJahr als Scatterplot. Beschriften Sie auch die Achsen und setzen Sie einen\nTitel.\n\nLösungfig = px.scatter(data[['männlich','weiblich']], \n                 title='Lebendgeborene in Deutschland',\n                 labels={'value': 'Anzahl Lebendgeburten', 'variable': 'Geschlecht'})\nfig.show()\n\nÜbung 4.2 - Teilaufgabe a)\n\nSchauen Sie sich die csv-Datei ‘stromverbrauch_hessen.csv’ im Texteditor an.\nWelche Daten enthält die Datei? Ab welcher Zeile beginnen die Daten und in\nwelcher Zeile enden sie? Importieren Sie dann die Daten, wobei Kopf- und\nFußzeilen beim Import übersprungen werden sollen. Welche Spalte ist als\nIndex-Spalte geeignet? Setzen Sie eine passende Spalte als Index. Verschaffen\nSie sich einen Überblick über die Daten und erstellen Sie eine Übersicht der\nstatistischen Kennzahlen. Interpretieren Sie den Stromverbrauch der\nVerbrauchergruppen.\n\nLösung\n\nDie Datei enthält den Stromverbrauch in Hessen 2000 bis 2021 nach\nVerbrauchergruppen in Gigawattstunden. Es bietet sich an, das Jahr als Index zu\nwählen.import pandas as pd\n\ndata = pd.read_csv('stromverbrauch_hessen.csv', skiprows=4, index_col=0)data.info()\n\nDer Datensatz enthält 22 Zeilen mit dem Stromverbrauch von 2000 bis\neinschließlich 2021. In den Spalten stehen der Stromverbrauch\n\nder Industrie,\n\ndes Verkehrs und\n\nder Haushalte, etc.\n\nAlle Einträge sind gültig.data.describe()\n\nDer mittlere Stromverbrauch der Industrie ist ungefähr 10x so groß wie der\nStromverbrauch des Verkehrs. Nochmal doppelt soviel Strom verbrauchen aber\nHaushalte, Gewerbe, etc.\n\nÜbung 4.2 - Teilaufgabe b)\n\nChecken Sie, ob die Spalte ‘insgesamt’ tatsächlich die Summe der anderen Spalten ist.\n\nLösungdata['Differenz'] = data['insgesamt'] - data['Industrie'] - data['Verkehr'] - data['Haushalte, Gewerbe, Handel, Dienstleistungen und übrige Verbraucher']\ndata['Differenz'].describe()\n\nEs gibt Abweichungen, Minimum und Maximum weichen jeweils um eine Gigawattstunde\nab. Angesichts der absoluten Werte wird es sich dabei um Rundungsfehler handeln\nund bedarf keiner weiteren Maßnahmen.\n\nÜbung 4.2 - Teilaufgabe c)\n\nFertigen Sie Boxplots an und interpretieren Sie die statistischen Kennzahlen.\n\nLösungimport plotly.express as px\n\nfig = px.box(data['insgesamt'], \n             title='Stromverbrauch Hessen von 2000 bis 2021 (insgesamt)',\n             labels={'value': 'Gigawattstunden', 'variable': ''})\nfig.show()\n\nDer Median liegt nahe am 25%-Quantil, die Verteilung des Stromverbrauchs der\neinzelnen Jahre scheint nicht symmetrisch zu sein. Es gibt einen Ausreißer mit\n39351 Gigawattstunden.fig = px.box(data[['Industrie', 'Verkehr', 'Haushalte, Gewerbe, Handel, Dienstleistungen und übrige Verbraucher']],\n            title='Stromverbrauch Hessen von 2000 bis 2021',\n            labels={'value': 'Gigawattstunden', 'variable': 'Sektoren'})\nfig.show()\n\nEine gemeinsame Visualisierung der drei Kategorien ist hier nicht sinnvoll, da die statistischen Kennzahlen weit auseinander liegen. Daher wird jede Kategorie einzeln untersucht.fig = px.box(data['Industrie'],\n            title='Stromverbrauch Hessen von 2000 bis 2021 (Industrie)',\n            labels={'value': 'Gigawattstunden', 'variable': ''})\nfig.show()\n\nDie Verteilung des Stromverbrauchs ist symmetrisch, der Median liegt ungefähr\nmittig zwischen Q1 und Q3. Es gibt zwei Ausreißer nach unten.fig = px.box(data['Verkehr'],\n            title='Stromverbrauch Hessen von 2000 bis 2021 (Verkehr)',\n            labels={'value': 'Gigawattstunden', 'variable': ''})\nfig.show()\n\nDie Verteilung des Stromverbrauchs ist ungefähr symmetrisch, es gibt einen\nAusreißer nach unten und einen Ausreißer nach oben.fig = px.box(data['Haushalte, Gewerbe, Handel, Dienstleistungen und übrige Verbraucher'],\n            title='Stromverbrauch Hessen von 2000 bis 2021 (Haushalte, etc.)',\n            labels={'value': 'Gigawattstunden', 'variable': ''})\nfig.show()\n\nEs gibt keine Ausreißer, der Median ist ungefähr mittig zwischen Q1 und Q3.\n\nÜbung 4.2 - Teilaufgabe d)\n\nVisualisieren Sie den Stromverbrauch der drei relevanten Sektoren Industrie,\nVerkehr und Haushalte abhängig vom Jahr in einem gemeinsamen Scatterplot. Setzen\nSie einen Titel und beschriften Sie auch die Achsen.\n\nGibt es Auffälligkeiten?\n\nLösungfig = px.scatter(data[['Industrie', 'Verkehr', 'Haushalte, Gewerbe, Handel, Dienstleistungen und übrige Verbraucher']],\n                 title='Stromverbrauch Hessen von 2000 bis 2021',\n                 labels={'value': 'Gigawattstunden', 'variable': 'Legende'})\nfig.show()\n\nDer Stromverbrauch der Industrie ist 2009 etwas eingebrochen. Vielleicht hat die\nWeltfinanzkrise 2007–2008 dazu geführt, dass 2009 die Industrie in Hessen\nweniger produziert hat und daher weniger Strom verbraucht hat. Aber das ist eine\nHypothese, die durch weitere Analysen belegt oder widerlegt werden müsste.","type":"content","url":"/chapter04-sec04","position":1},{"hierarchy":{"lvl1":"5.1 Was sind kategoriale Daten?"},"type":"lvl1","url":"/chapter05-sec01","position":0},{"hierarchy":{"lvl1":"5.1 Was sind kategoriale Daten?"},"content":"In unserem bisherigen Beispiel zu den Autoverkaufspreisen haben wir bestimmte\nEigenschaften der Autos, wie die Marke oder die Farbe, nicht berücksichtigt. In\nden vorherigen Kapiteln konzentrierten sich unsere Analysen hauptsächlich auf\nnumerische Werte wie den Kilometerstand. Dies liegt daran, dass es für Daten wie\nFarben oder Automarken keine Rechenoperationen gibt. In diesem Kapitel werden\nwir uns intensiver mit diesen nicht-numerischen Daten auseinandersetzen.","type":"content","url":"/chapter05-sec01","position":1},{"hierarchy":{"lvl1":"5.1 Was sind kategoriale Daten?","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter05-sec01#lernziele","position":2},{"hierarchy":{"lvl1":"5.1 Was sind kategoriale Daten?","lvl2":"Lernziele"},"content":"Lernziele\n\nSie wissen, was numerische (metrische oder quantitative) Daten sind.\n\nSie wissen, was kategoriale (qualitative) Daten sind.\n\nSie können die Methode .unique() benutzen, um die eindeutigen Werte eines\nPandas-Series-Objektes aufzulisten.\n\nSie kennen den Unterschied zwischen ungeordneten und geordneten\nkategorialen Daten.\n\nSie können mit der Methode .value_counts() die Anzahl der eindeutigen\nWerte eines Pandas-Series-Objektes bestimmen lassen.\n\nSie wissen, was der Modalwert oder Modus eines Datensatzes ist und\nkönnen diesen mit der Methode .mode() bestimmen lassen.","type":"content","url":"/chapter05-sec01#lernziele","position":3},{"hierarchy":{"lvl1":"5.1 Was sind kategoriale Daten?","lvl2":"Was wir bisher hatten: numerische Daten"},"type":"lvl2","url":"/chapter05-sec01#was-wir-bisher-hatten-numerische-daten","position":4},{"hierarchy":{"lvl1":"5.1 Was sind kategoriale Daten?","lvl2":"Was wir bisher hatten: numerische Daten"},"content":"In den bisherigen Kapiteln lag unser Fokus auf den Verkaufspreisen von Autos.\nDie Methode .describe() in Pandas bietet eine schnelle Möglichkeit, einen\nÜberblick über die statistischen Kennzahlen eines Datensatzes zu erhalten.\nInteressanterweise berücksichtigt die describe()-Methode nur numerische Werte\n(also Zahlen wie Integer und Floats) für die Auswertung. Dennoch bestimmen auch\ndie nicht-numerischen Eigenschaften eines Autos den Verkaufspreis. Die Farbe\neines Autos beispielsweise beeinflusst häufig die Kaufentscheidung und damit den\nPreis.\n\nBevor wir uns den nicht-numerischen Daten widmen, vertiefen wir unser\nVerständnis für numerische Daten. Diese werden oft auch als metrische oder\nquantitative Daten bezeichnet.\n\nWas sind ... metrische/quantitative/numerische Daten?\n\nMetrische Daten sind Informationen, die gemessen werden können. Daher können sie\ndurch Zahlen (ganze Zahlen, rationale oder reelle Zahlen) auf einer Skala\ndargestellt werden und werden numerische Daten genannt. Ein anderer Name für\nmetrische Daten ist der Begriff quantitative Daten.\n\nWir betrachten den Datensatz {download}Download autoscout24_DE_2020.csv <https://gramschs.github.io/book_ml4ing/data/autoscout24_DE_2020.csv> mit\nAutoverkaufspreises von \n\nAutoscout24.de, der alle\nAutos enthält, die im Jahr 2020 zugelassen wurden. Ein kurzer Überblick über den\nDatensatz hilft uns, die Art der Daten besser zu verstehen.\n\nimport pandas as pd\n\nurl = 'https://raw.githubusercontent.com/gramschs/assets/refs/heads/main/ml4ing/data/autoscout24_DE_2020.csv'\ndata = pd.read_csv(url)\ndata.info()\n\n\n\nMini-Übung\n\nWelche Eigenschaften der Autos sind numerisch (metrisch/quantitativ)? Würden Sie\nbei anderen Eigenschaften ebenfalls einen metrischen Datentyp erwarten?\n\nLösung\n\nDas Jahr und der Preis (Euro) sind ganze Zahlen (Integer). Die Leistung der\nAutos wird als Fließkommazahl (Float) angegeben, unabhängig von der Einheit kW\noder PS. Auch der Kilometerstand wird durch eine Fließkommazahl (Float)\nrepräsentiert. Das hätte man auch für den Verbrauch in Spalte 10 oder 11\nerwarten können. Die Angabe der Einheit l/100 km oder g/km in den Zellen hat\nverhindert, dass Pandas diese Informationen als Zahl interpretiert.\n\nMit numerischen Daten können wir umfangreiche Datenanalysen durchführen. Wir\nkönnen vergleichen, ob zwei Messwerte gleich oder ungleich sind. Wir können\nbeurteilen, ob ein Messwert kleiner oder größer als ein anderer ist oder sogar\ndas Minimum und das Maximum aller Messwerte bestimmen. Und vor allem können wir\nmit metrischen Daten rechnen. Erst dadurch ist es möglich, einen Mittelwert zu\nbilden oder Streuungsmaße wie Spannweite, Standardabweichung und\nInterquartilsabstand zu berechnen. Solche detaillierten Berechnungen sind nur\nbei metrischen (quantitativen) Daten möglich.","type":"content","url":"/chapter05-sec01#was-wir-bisher-hatten-numerische-daten","position":5},{"hierarchy":{"lvl1":"5.1 Was sind kategoriale Daten?","lvl2":"Das Gegenteil von numerischen Daten: kategoriale Daten"},"type":"lvl2","url":"/chapter05-sec01#das-gegenteil-von-numerischen-daten-kategoriale-daten","position":6},{"hierarchy":{"lvl1":"5.1 Was sind kategoriale Daten?","lvl2":"Das Gegenteil von numerischen Daten: kategoriale Daten"},"content":"Während numerische Daten messbare Informationen darstellen, sind kategoriale\nDaten durch ihre Zugehörigkeit zu bestimmten Kategorien oder Gruppen\ndefiniert. Ein weiterer Begriff für kategoriale Daten ist qualitative Daten.\nEin gutes Beispiel für kategoriale Daten ist die Farbe eines Autos. Oft gibt der\nDatentyp einer bestimmten Eigenschaft in einem Datensatz bereits Hinweise\ndarauf, ob es sich um kategoriale oder numerische Daten handelt.\n\nDie obige Ausführung der Anweisung data.info() hat gezeigt, dass einige Daten\nals objects gespeichert sind, was oft auf kategoriale Daten hinweist. Ein\nBlick in die Spalte »Marke« gibt uns weitere Einblicke.\n\ndata['Marke'].head(10)\n\n\n\nDie ersten 10 Autos sind offensichtlich Alfa Romeos. Sind vielleicht nur Alfa\nRomeos in der Tabelle enthalten? Wir schauen uns die letzten 10 Einträge an.\n\ndata['Marke'].tail(10)\n\n\n\nDie letzten Einträge sind Volvos. Vielleicht sind die Autos nach Marken\nalphabetisch geordnet? Es wäre schön zu wissen, welche verschiedenen Marken im\nDatensatz enthalten sind. Dazu gibt es die Methode .unique(). Sie gehört zu\nder Datenstruktur Pandas-Series. Wenn wir eine einzelne Spalte eines\nPandas-DataFrames herausgreifen, liegt automatisch ein Pandas-Series-Objekt vor,\nso dass wir diese Methode hier benutzen können.\n\ndata['Marke'].unique()\n\n\n\nObwohl der Datensatz insgesamt 18566 Autos umfasst, gibt es nur 41 verschiedene\nMarken. Allgemein gesagt, gibt es für die Eigenschaft »Marke« 41 Kategorien.\nEine nicht-metrische Eigenschaft, die nur eine begrenzte Anzahl von Werten\nannehmen kann, wird als kategoriale Variable bezeichnet. Ihre konkreten Werte\nwerden als kategoriale Daten oder qualitative Daten bezeichnet.\n\nWas sind ... kategoriale/qualitative Daten?\n\nKategoriale Daten sind Informationen, die nicht gemessen werden können.\nStattdessen werden sie durch die Zugehörigkeit zu einer Kategorie dargestellt.\nSie bekommen ein Etikett. Kategoriale Daten nehmen nur eine begrenzte Anzahl\nvon verschiedenen Werten an. Oft bezeichnet man kategoriale Daten auch als\nqualitative Daten.\n\nDiese Definition ist nicht präzise. Der Begriff ‘begrenzte Anzahl’ von Werten\nist etwas schwammig. Sind damit 10 Kategorien gemeint oder 100 oder 1000? Welche\nEigenschaften des Auto-Datensatzes sind kategorial? In der Praxis spricht man\nvon kategorialen Daten, wenn die Anzahl der Kategorien deutlich kleiner als die\nAnzahl der Datenpunkte ist (oft als Faustregel: < 5-10 % der Beobachtungen).\n\nMini-Übung\n\nSuchen Sie sich drei nicht-metrische Eigenschaften aus und bestimmen Sie\ndie Anzahl der einzigartigen Einträge dieser Eigenschaft.\n\nTipp: Die Methode .unique() liefert ein sogenanntes NumPy-Array zurück, das\nhier wie eine Liste benutzt werden kann. Die Python-Funktion len() kann die\nLänge einer Liste, also die Anzahl der Elemente der Liste, bestimmen.\n\nLösunganzahl_einzigartige_werte = len(data['Modell'].unique())\nprint(f'Die Spalte/Eigenschaft Modell hat {anzahl_einzigartige_werte} einzigartige Werte.')\n\nOffensichtlich gibt es 561 einzigartige Werte für die Spalte/Eigenschaft Modell.\n\nMit einer for-Schleife können wir auch alle Eigenschaften durchgehen, indem wir\ndas Attribut .columns nutzen, in dem alle Spaltenüberschriften gespeichert\nsind. Damit betrachten wir natürlich auch die metrischen/quantitativen\nEigenschaften.for col in data.columns:\n    anzahl_einzigartige_werte = len(data[col].unique())\n    print(f'Die Spalte/Eigenschaft {col} hat {anzahl_einzigartige_werte} einzigartige Werte.')\n\nDamit erhalten wir folgende Ausgabe:Die Spalte/Eigenschaft Marke hat 41 einzigartige Werte.\nDie Spalte/Eigenschaft Modell hat 561 einzigartige Werte.\nDie Spalte/Eigenschaft Farbe hat 15 einzigartige Werte.\nDie Spalte/Eigenschaft Erstzulassung hat 12 einzigartige Werte.\nDie Spalte/Eigenschaft Jahr hat 1 einzigartige Werte.\nDie Spalte/Eigenschaft Preis (Euro) hat 4926 einzigartige Werte.\nDie Spalte/Eigenschaft Leistung (kW) hat 260 einzigartige Werte.\nDie Spalte/Eigenschaft Leistung (PS) hat 260 einzigartige Werte.\nDie Spalte/Eigenschaft Getriebe hat 4 einzigartige Werte.\nDie Spalte/Eigenschaft Kraftstoff hat 10 einzigartige Werte.\nDie Spalte/Eigenschaft Verbrauch (l/100 km) hat 238 einzigartige Werte.\nDie Spalte/Eigenschaft Verbrauch (g/km) hat 616 einzigartige Werte.\nDie Spalte/Eigenschaft Kilometerstand (km) hat 10730 einzigartige Werte.\nDie Spalte/Eigenschaft Bemerkungen hat 16547 einzigartige Werte.\n\nBei unserem Beispiel sind die Eigenschaften Marke, Modell, Farbe, Getriebe,\nKraftstoff und Bemerkungen nicht-numerische Eigenschaften. Allerdings hat die\nEigenschaft Bemerkungen 16547 verschiedene Werte. Ein Merkmal von kategorialen\nbzw. qualitativen Daten ist aber, dass nur eine begrenzte Anzahl von\nverschiedenen Werten angenommen wird. Das ist hier nicht mehr der Fall, so dass\nwir die Eigenschaft Bemerkung nicht als kategoriale/qualitative Eigenschaft\neinstufen. Die kategorialen Eigenschaften in dem Beispiel sind also Marke,\nModell, Farbe, Getriebe und Kraftstoff.","type":"content","url":"/chapter05-sec01#das-gegenteil-von-numerischen-daten-kategoriale-daten","position":7},{"hierarchy":{"lvl1":"5.1 Was sind kategoriale Daten?","lvl2":"Kategoriale Daten: ungeordnet oder geordnet?"},"type":"lvl2","url":"/chapter05-sec01#kategoriale-daten-ungeordnet-oder-geordnet","position":8},{"hierarchy":{"lvl1":"5.1 Was sind kategoriale Daten?","lvl2":"Kategoriale Daten: ungeordnet oder geordnet?"},"content":"Innerhalb der kategorialen bzw. qualitativen Daten gibt es wiederum zwei Arten\nvon Datentypen:\n\nungeordnete kategoriale Daten und\n\ngeordnete kategoriale Daten.\n\nUngeordnete kategoriale Daten werden in der Statistik auch nominalskalierte\nDaten genannt, während die geordneten kategorialen Daten auch als\nordinalskalierte Daten bezeichnet werden.\n\nEin typisches Beispiel für ungeordnete kategoriale Daten sind die Farben der\nAutos. Es gibt keine natürliche Reihenfolge für Farben. Wir könnten die Farbe\nalphabetisch anordnen, doch sobald wir eine andere Sprache benutzen, wäre auch\ndie Reihenfolge anders. Farben sind ungeordnete Kategorien. Für statistische\nAnalysen heißt das, dass nur bestimmt werden kann, ob die Farbe eines Autos in\neine bestimmte Kategorie fällt oder nicht. Entweder das Auto ist gelb oder es\nist nicht gelb. Mathematisch gesehen können wir also nur auf Gleichheit oder\nUngleichheit prüfen. Es gibt keine Vergleiche bzgl. der Reihenfolge und auch\nkein Minimum oder Maximum. Auch sämtliche Rechenoperationen entfallen und daher\nkönnen auch nicht Mittelwerte oder Streuungsmaße bestimmt werden. Stattdessen\nwird der Modus oder Modalwert bestimmt.\n\nWas ist ... der Modus (Modalwert)?\n\nDer Modus, auch Modalwert genannt, ist der häufigste auftretende Wert in dem\nDatensatz. Er gehört zu den Lageparametern in der Statistik. Er existiert sowohl\nfür ungeordnete und geordnete kategoriale Daten als auch für metrische Daten.\nEin Datensatz kann auch mehrere Modi haben, wenn mehrere Werte gleich häufig\nvorkommen.\n\nPandas bietet eine Methode zur Bestimmung des Modus namens .mode(). Diese\nMethode funktioniert wieder nur für ein Pandas-Series-Objekt, so dass zuerst\neine einzelne Spalte aus der Tabelle herausgeschnitten wird. Auf diese Spalte\nwird dann die Methode angewendet. Gibt es mehrere Modi, so werden alle\nzurückgegeben.\n\nmodus_farben = data['Farbe'].mode()\nprint(f'Die häufigste Farbe ist {modus_farben}.')\n\n\n\nUnd wie häufig kommen die anderen Farben vor? Die Methode .value_counts()\nzählt die Anzahl an Autos mit einer bestimmten Farbe.\n\ndata['Farbe'].value_counts()\n\n\n\n","type":"content","url":"/chapter05-sec01#kategoriale-daten-ungeordnet-oder-geordnet","position":9},{"hierarchy":{"lvl1":"5.1 Was sind kategoriale Daten?","lvl2":"Der Mehrwert geordneter kategorialer Daten"},"type":"lvl2","url":"/chapter05-sec01#der-mehrwert-geordneter-kategorialer-daten","position":10},{"hierarchy":{"lvl1":"5.1 Was sind kategoriale Daten?","lvl2":"Der Mehrwert geordneter kategorialer Daten"},"content":"Was macht geordnete kategoriale Daten besonders? Bei ungeordneten kategorialen\nDaten wie Farben gibt es keine sinnvolle Reihenfolge. Bei geordneten\nkategorialen Daten wie Monaten dagegen schon:\n\n# Bei Farben: Sortierung ist willkürlich (alphabetisch)\nprint(\"Farben alphabetisch:\")\nprint(data['Farbe'].value_counts())\n\n# Bei Monaten: Sortierung ist sinnvoll (chronologisch)\nprint(\"\\nMonate chronologisch:\")\nprint(data['Erstzulassung'].value_counts().sort_index())\n\n\n\nJetzt können wir erkennen: Im ersten Quartal (01/2020, 02/2020, 03/2020) wurden\ndeutlich mehr Autos zugelassen als im letzten Quartal. Diese Aussage setzt\nvoraus, dass die Kategorien eine Reihenfolge haben.\n\nBei Farben wäre die Aussage “Im ersten Drittel der Farben wurden mehr Autos\nverkauft” sinnlos, denn was ist das “erste Drittel” von Farben?\n\nDas ist der entscheidende Unterschied: Bei nominalen Daten können wir nur\nHäufigkeiten zählen. Bei ordinalen Daten können wir zusätzlich die Reihenfolge\nnutzen, um Aussagen wie ‘im ersten Quartal’ oder ‘mehr als in der zweiten\nHälfte’ zu treffen.","type":"content","url":"/chapter05-sec01#der-mehrwert-geordneter-kategorialer-daten","position":11},{"hierarchy":{"lvl1":"5.1 Was sind kategoriale Daten?","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter05-sec01#zusammenfassung-und-ausblick","position":12},{"hierarchy":{"lvl1":"5.1 Was sind kategoriale Daten?","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Kapitel haben wir uns mit dem Unterschied zwischen numerischen\n(metrischen bzw. quantitativen) und kategorialen (qualitativen) Daten\nbeschäftigt. Dabei wird bei kategorialen Daten noch zwischen ungeordneten und\ngeordneten Kategorien unterschieden. Je nach Art der Daten können\nunterschiedliche statistische Kennzahlen erhoben werden. Bei ungeordneten\nkategorialen Daten kann nur der Modus (Modalwert) berechnet werden. Bei\ngeordneten kategorialen Daten können zusätzlich noch Quantile (insbesondere der\nMedian) berechnet werden. Nur bei numerischen (metrischen bzw. quantitativen)\nDaten ist es möglich, den Mittelwert und zusätzlich die Streuungsmaße\n(Spannbreite, Standardabweichung und Interquartilsabstand) zu bestimmen. Im\nnächsten Kapitel werden wir uns mit der Visualisierung der kategorialen Daten\nbeschäftigen.","type":"content","url":"/chapter05-sec01#zusammenfassung-und-ausblick","position":13},{"hierarchy":{"lvl1":"5.2 Barplots und Histogramme"},"type":"lvl1","url":"/chapter05-sec02","position":0},{"hierarchy":{"lvl1":"5.2 Barplots und Histogramme"},"content":"Barplots (Balken- oder Säulendiagramme) sind die am häufigsten verwendeten\nVisualisierungen für kategoriale Daten. In diesem Kapitel lernen wir, wie mit\nPlotly ein Barplot erstellt und von einem Histogramm unterschieden wird.","type":"content","url":"/chapter05-sec02","position":1},{"hierarchy":{"lvl1":"5.2 Barplots und Histogramme","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter05-sec02#lernziele","position":2},{"hierarchy":{"lvl1":"5.2 Barplots und Histogramme","lvl2":"Lernziele"},"content":"Lernziele\n\nSie kennen Barplots zur Visualisierung kategorialer Daten und können\nSäulen- und Balkendiagramme unterscheiden.\n\nSie können mit px.bar() Barplots erstellen und anpassen.\n\nSie wissen, wann Histogramme statt Barplots verwendet werden.\n\nSie können mit px.histogram() Histogramme erstellen und die Anzahl\nder Bins sinnvoll wählen.","type":"content","url":"/chapter05-sec02#lernziele","position":3},{"hierarchy":{"lvl1":"5.2 Barplots und Histogramme","lvl2":"Barplots"},"type":"lvl2","url":"/chapter05-sec02#barplots","position":4},{"hierarchy":{"lvl1":"5.2 Barplots und Histogramme","lvl2":"Barplots"},"content":"Im letzten Kapitel haben wir uns mit kategorialen (qualitativen) Daten\nauseinandergesetzt. Um kategoriale Daten zu visualisieren und zu vergleichen,\neignet sich besonders der Barplot. Ein Barplot zeigt numerische Werte (z.B.\nAnzahlen oder Durchschnittswerte) für verschiedene Kategorien an.\n\nBei der Visualisierung werden prinzipiell zwei Varianten unterschieden. Zum\neinen können die Kategorien entlang der x-Achse aneinandergereiht werden. Die\nHöhe der Rechtecke repräsentiert dann den Zahlenwert dieser Kategorie. Da die\nRechtecke an Säulen erinnern, wird diese Variante Säulendiagramm genannt.\nDie andere Möglichkeit ist, die Kategorien untereinander entlang der y-Achse\naufzuführen. Dann ist die Länge der Rechtecke repräsentativ für den Zahlenwert\ndieser Kategorie. Diese Variante wird Balkendiagramm genannt.\n\nWas ist ... ein Barplot?\n\nEin Barplot ist ein Diagramm, das numerische Werte für verschiedene Kategorien\nvisualisiert. Jede Kategorie wird durch die Höhe oder Länge eines Rechtecks\nrepräsentiert, das den zugehörigen numerischen Wert darstellt.\n\nProbieren wir Barplots am Beispiel der Autoscout24-Verkaufspreise für Autos aus,\ndie 2020 zugelassen wurden ({download}Download autoscout24_DE_2020.csv <https://gramschs.github.io/book_ml4ing/data/autoscout24_DE_2020.csv>). Zuerst\nladen wir die Daten und verschaffen uns einen Überblick.\n\nimport pandas as pd\n\nurl = 'https://raw.githubusercontent.com/gramschs/assets/refs/heads/main/ml4ing/data/autoscout24_DE_2020.csv'\ndata = pd.read_csv(url)\ndata.info()\n\n\n\nMit der Methode .value_counts() lassen wir Python die Anzahl der Autos pro\nMarke bestimmen.\n\nanzahl_pro_marke = data['Marke'].value_counts()\nprint(anzahl_pro_marke)\n\n\n\nDie Methode .value_counts() sortiert die Einträge standardmäßig von der\nhöchsten zur niedrigsten Anzahl.\n\nMit nur wenigen Zeilen Code können wir mit der Funktion bar() aus dem\nPlotly-Express-Modul eine Visualisierung erstellen. Zuerst importieren wir das\nModul, dann erzeugen wir das Diagramm mit bar() und zuletzt lassen wir das\nDiagramm mit show() anzeigen. Mittels der Option orientation='h' erzeugen\nwir ein Balkendiagramm mit horizontaler Ausrichtung.\n\nimport plotly.express as px\n\nsaeulendiagramm = px.bar(anzahl_pro_marke)\nsaeulendiagramm.show()\n\nbalkendiagramm = px.bar(anzahl_pro_marke, orientation='h')\nbalkendiagramm.show()\n\n\n\n\n\nObwohl Plotly Express bereits eine ansprechende Visualisierung bietet, könnten\ndie automatisch generierten Beschriftungen “index”, “value” und “variable”\nverbessert werden. Außerdem sollte ein Diagrammtitel hinzugefügt werden. Der\nTitel kann direkt in der bar()-Funktion über das title= Argument gesetzt\nwerden. Für die Achsenbeschriftungen und den Legendentitel verwenden wir die\nFunktion update_layout(). Die Argumente xaxis_title= und yaxis_title=\nmodifizieren die Beschriftung der x- und y-Achse. Mit legend_title= wird der\nTitel der Legende neu beschriftet.\n\nfig = px.bar(anzahl_pro_marke, title='Autoscout24 (Zulassungsjahr 2020)')\nfig.update_layout(\n    xaxis_title='Marke',\n    yaxis_title='Anzahl Autos',\n    legend_title='Anzahl Autos pro Marke',\n)\nfig.show()\n\n\n\n","type":"content","url":"/chapter05-sec02#barplots","position":5},{"hierarchy":{"lvl1":"5.2 Barplots und Histogramme","lvl2":"Histogramm"},"type":"lvl2","url":"/chapter05-sec02#histogramm","position":6},{"hierarchy":{"lvl1":"5.2 Barplots und Histogramme","lvl2":"Histogramm"},"content":"Während Barplots in erster Linie kategoriale Daten visualisieren, dienen\nHistogramme zur Darstellung numerischer Daten. Ein Barplot zeigt typischerweise\ndie Anzahl der Werte pro Kategorie. Bei numerischen Daten wäre eine solche\nDarstellung oft nicht sinnvoll. Nehmen wir als Beispiel die Kilometerstände von\nAutos. Wir lassen zuerst mit der Methode .unique() die verschiedenen\nKilometerstände bestimmen. Das Ergebnis ist ein sogenanntes NumPy-Array, das\nhier wie eine Liste benutzt werden kann. Mit Hilfe der len()-Funktion können\nwir die Anzahl der Einträge berechnen.\n\nkilometerstaende = data['Kilometerstand (km)'].unique()\nanzahl_kilometerstaende = len(kilometerstaende)\nprint(f'Es gibt {anzahl_kilometerstaende} verschiedene Kilometerstände.')\n\n\n\nMit über 10.000 verschiedenen Kilometerständen wäre eine direkte Visualisierung\nnicht zielführend. Um dennoch eine sinnvolle Analyse durchzuführen, können wir\nden Bereich der Kilometerstände in Intervalle einteilen. Dazu bestimmen wir das\nMinimum und das Maximum der Kilometerstände.\n\nminimaler_kilometerstand = data['Kilometerstand (km)'].min()\nmaximaler_kilometerstand = data['Kilometerstand (km)'].max()\n\nprint(f'minimaler Kilometerstand: {minimaler_kilometerstand}')\nprint(f'maximaler Kilometerstand: {maximaler_kilometerstand}')\n\n\n\nDie Daten reichen von Neuwagen (minimaler Kilometerstand 0 km) bis zu Autos mit\nhohem Kilometerstand (maximaler Kilometerstand 435909 km). Wir können diesen\nBereich in gleichmäßige Intervalle unterteilen. Wählen wir beispielsweise 10\nIntervalle, so würde das erste Intervall alle Autos mit einem Kilometerstand von\n0 km bis 50000 km umfassen. Das zweite Intervall geht dann von 50000 km bis\n100000 km usw. Um jetzt zu ermitteln, wie viele Autos in das jeweilige Intervall\nfallen, könnten wir ein kleines Python-Programm schreiben. Tatsächlich brauchen\nwir das nicht, denn diese Funktionalität ist bereits in der\nhistogram()-Funktion integriert, die auch die Visualisierung übernimmt.\n\nWir übergeben der Funktion die Daten als erstes Argument. Als optionales zweites\nArgument können wir die gewünschte Anzahl an Intervallen übergeben. Die\nkünstlich gewählten Intervalle werden auch als Bins bezeichnet. Daher lautet das\nArgument zum Setzen der Anzahl der Bins nbins=, so wie der englische Begriff\n»number of bins«.\n\nfig = px.histogram(data['Kilometerstand (km)'], nbins=10, \n    title='10 künstlich gewählte Intervalle bzgl. des Kilometerstandes (km)')\nfig.update_layout(\n    xaxis_title='Kategorien der Kilometerstände (km)',\n    yaxis_title='Anzahl Autos',\n    legend_title='Anzahl Autos pro Kategorie',\n)\nfig.show()\n\n\n\nDie meisten Autos haben weniger als 200000 km auf dem Kilometerzähler.\n\nEin charakteristisches Merkmal von Histogrammen ist, dass die Balken ohne Lücke\naneinander liegen, was die kontinuierliche Natur der numerischen Daten\nwiderspiegelt. Die Anzahl der Kategorien (Bins) beeinflusst die Darstellung\nmaßgeblich und sollte sorgfältig gewählt werden.\n\nDie Anzahl der Kategorien ist ein sehr wichtiger Faktor bei der Visualisierung.\nWerden zu wenige Kategorien gewählt, werden auch nicht die Unterschiede\nsichtbar. Werden zu viele Kategorien gewählt, sind ggf. einige Kategorien leer.\n\nMini-Übung\n\nExperimentieren Sie mit verschiedenen Werten für nbins=. Probieren Sie\nnbins=5, nbins=20 und nbins=50 aus. Was beobachten Sie? Welche\nVor- und Nachteile haben wenige vs. viele Bins?\n\nLösung\n\nMit nbins=5 sind die Kategorien sehr breit - man erkennt nur grobe Muster.\n\nMit nbins=50 werden viele Kategorien leer oder haben nur wenige Werte - das\nDiagramm wird unübersichtlich.\n\nMit nbins=10 bis nbins=20 entsteht ein guter Kompromiss: Die Verteilung\nist erkennbar, ohne dass das Diagramm überladen wirkt.\n\nZusammenfassend wird ein Histogramm folgendermaßen beschrieben.\n\nWas ist ... ein Histogramm?\n\nEin Histogramm ist eine grafische Darstellung der Häufigkeitsverteilung\nnumerischer Daten. Dabei wird der Wertebereich in gleich große Intervalle\n(sogenannte Bins oder Klassen) eingeteilt. Die Höhe jedes Balkens zeigt, wie\nviele Datenpunkte in das jeweilige Intervall fallen. Ein charakteristisches\nMerkmal: Die Balken liegen ohne Lücken aneinander, da sie eine kontinuierliche\nSkala repräsentieren.","type":"content","url":"/chapter05-sec02#histogramm","position":7},{"hierarchy":{"lvl1":"5.2 Barplots und Histogramme","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter05-sec02#zusammenfassung-und-ausblick","position":8},{"hierarchy":{"lvl1":"5.2 Barplots und Histogramme","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Kapitel wurden zwei wichtige Diagrammtypen vorgestellt: der Barplot\nund das Histogramm. Obwohl beide mit Rechtecken arbeiten, haben sie\nunterschiedliche Anwendungsbereiche und sollten nicht verwechselt werden.\nWährend der Barplot ideal für kategoriale Daten ist, eignet sich das Histogramm\nzur Visualisierung numerischer Daten. Im nächsten Kapitel widmen wir uns dem\nThema Datenfilterung.","type":"content","url":"/chapter05-sec02#zusammenfassung-und-ausblick","position":9},{"hierarchy":{"lvl1":"5.3 Daten filtern und gruppieren"},"type":"lvl1","url":"/chapter05-sec03","position":0},{"hierarchy":{"lvl1":"5.3 Daten filtern und gruppieren"},"content":"Im vorherigen Kapitel haben wir Autos basierend auf ihrem Kilometerstand\ngruppiert und visualisiert. Während diese Gruppierung automatisch im Hintergrund\nstattfand, werden wir in diesem Kapitel lernen, wie wir direkt auf die\ngruppierten Daten zugreifen und zusätzliche Analysen durchführen können.","type":"content","url":"/chapter05-sec03","position":1},{"hierarchy":{"lvl1":"5.3 Daten filtern und gruppieren","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter05-sec03#lernziele","position":2},{"hierarchy":{"lvl1":"5.3 Daten filtern und gruppieren","lvl2":"Lernziele"},"content":"Lernziele\n\nSie wissen, dass die Wahrheitswerte True (wahr)  oder False (falsch) in\ndem Datentyp bool gespeichert werden.\n\nSie kennen die wichtigsten Vergleichsoperatoren (<, <=, >, >=, ==,\n!=, in, not in) in Python.\n\nSie können ein Pandas-DataFrame-Objekt nach einem Wert filtern.\n\nSie können ein Pandas-DataFrame-Objekt mit den Methoden groupby() und\nget_group() gruppieren.","type":"content","url":"/chapter05-sec03#lernziele","position":3},{"hierarchy":{"lvl1":"5.3 Daten filtern und gruppieren","lvl2":"Daten filtern"},"type":"lvl2","url":"/chapter05-sec03#daten-filtern","position":4},{"hierarchy":{"lvl1":"5.3 Daten filtern und gruppieren","lvl2":"Daten filtern"},"content":"Im vorherigen Kapitel haben wir die Kilometerstände von Autos untersucht, die im\nJahr 2020 zugelassen und Mitte 2023 auf \n\nAutoscout24.de angeboten wurden. Bei der\nKategorisierung der Kilometerstände fiel auf, dass Fahrzeuge mit einer\nLaufleistung von über 200000 km selten sind. Trotzdem beeinflusste dies die\nAufteilung in zehn gleichmäßige Gruppen, die von 0 km bis 435909 km reichten,\nerheblich. Um eine genauere Analyse zu ermöglichen, wäre es sinnvoll, Fahrzeuge\nmit einer Laufleistung von bis zu 200.000 km in den Fokus zu nehmen und die\nAusreißer auszuschließen. Daher widmen wir uns in diesem Kapitel der Filterung\nvon tabellarischen Datensätzen mithilfe von Pandas.\n\nZuerst laden wir den Datensatz autoscout24_DE_2020.csv und überprüfen den\nInhalt.\n\nimport pandas as pd\n\nurl = 'https://raw.githubusercontent.com/gramschs/assets/refs/heads/main/ml4ing/data/autoscout24_DE_2020.csv'\ndata = pd.read_csv(url)\ndata.info()\n\n\n\nUm die Autos mit einem Kilometerstand von bis zu 200000 km zu filtern,\nvergleichen wir die entsprechende Spalte mit dem Wert 200000, indem wir den aus\nder Mathematik bekannten Kleiner-gleich-Operator <= benutzen. Das Ergebnis\ndieses Vergleichs speichern wir in der Variable bedingung_kilometerstand.\n\nbedingung_kilometerstand = data['Kilometerstand (km)'] <= 200000\n\n\n\nAber was genau ist in der Variable bedingung_kilometerstand enthalten? Schauen\nwir uns den Datentyp an:\n\ntype(bedingung_kilometerstand)\n\n\n\nOffensichtlich handelt es sich um ein Pandas-Series-Objekt. Für weitere\nInformationen können wir die .info()-Methode aufrufen:\n\nbedingung_kilometerstand.info()\n\n\n\nIn dem Series-Objekt sind 18566 Einträge vom Datentyp bool gespeichert. Diesen\nDatentyp haben wir bisher nicht kennengelernt. Wir lassen die ersten fünf\nEinträge ausgeben:\n\nbedingung_kilometerstand.head()\n\n\n\nSind alle Einträge mit dem Wert True gefüllt? Wie viele und vor allem welche\neinzigartige Einträge gibt es in diesem Series-Objekt?\n\nbedingung_kilometerstand.unique()\n\n\n\nDas Series-Objekt enthält nur True und False, was den Datentyp bool\ncharakterisiert. In diesem Datentyp können nur zwei verschiedene Werte\ngespeichert werden, nämlich wahr (True) und falsch (False). Oft sind\nWahrheitswerte das Ergebnis eines Vergleichs, wie das folgende Code-Beispiel\nzeigt:\n\nx = 19\nprint(x < 100)\n\n\n\nIn der Python-Programmierung wird der Datentyp bool oft verwendet, um\nProgrammcode zu verzweigen. Damit ist gemeint, dass Teile des Programms nur\ndurchlaufen und ausgeführt werden, wenn eine bestimmte Bedingung wahr (True)\nist. In dieser Vorlesung benutzen wir bool-Werte hauptsächlich zum Filtern von\nDaten.\n\nWelche Vergleichsoperatoren kennt Python?\n\nIn Python können die mathematischen Vergleichsoperatoren in ihrer gewohnten\nSchreibweise verwendet werden:\n\n< kleiner als\n\n<= kleiner als oder gleich\n\n> größer als\n\n>= größer als oder gleich\n\n== gleich (= ist der Zuweisungsoperator, nicht mit Gleichheit\nverwechseln!)\n\n!= ungleich\n\nDarüber hinaus kann mit in oder not in getestet werden, ob\nein Element in einer Liste ist oder eben nicht.\n\nAber was machen wir jetzt mit diesem Series-Objekt? Wir können es als Index\nbenutzen für den ursprünglichen Datensatz benutzen. Die Zeilen, in denen True\nsteht, werden übernommen, die anderen verworfen.\n\nautos_bis_200000km = data[bedingung_kilometerstand]\nautos_bis_200000km.info()\n\n\n\nVon den 18566 Autos wurden 18525 Autos übernommen. Ist denn die Filterung\ngeglückt? Wir verschaffen uns mit der .describe()-Methode einen schnellen\nÜberblick.\n\nautos_bis_200000km.describe()\n\n\n\nDer maximale Eintrag für die Spalte Kilometerstand (km) ist 199000 km. Mit dem\nTilde-Operator ~ können wir das Pandas-Series-Objekt\nbedingung_kilometerstand in das Gegenteil umwandeln. Damit können wir also die\nAutos mit einem Kilometerstand über 200.000 km herausfiltern.\n\nautos_ab_200000km = data[~bedingung_kilometerstand]\nautos_ab_200000km.info()\n\n\n\n41 Autos, die 2020 zugelassen wurden, sollten Mitte 2023 mit einem\nKilometerstand von mehr als 200000 km verkauft werden. Schauen wir uns die\nStatistik an.\n\nautos_ab_200000km.describe()\n\n\n\nUnd was sind das für Autos?\n\nautos_ab_200000km.head(10)\n\n\n\nMini-Übung\n\nFiltern Sie den Datensatz so, dass nur Autos mit einem Preis zwischen\n10.000 und 30.000 Euro übrig bleiben. Wie viele Autos erfüllen diese\nBedingung? Tipp: Sie können zwei Bedingungen mit & (und) verknüpfen.\n\nLösungbedingung_preis = (data['Preis (Euro)'] >= 10000) & (data['Preis (Euro)'] <= 30000)\nautos_mittlerer_preis = data[bedingung_preis]\nprint(f\"Anzahl Autos: {len(autos_mittlerer_preis)}\")\n\nEs gibt 11.793 Autos in dieser Preisklasse.\n\nWichtig: Die Klammern um die einzelnen Bedingungen sind notwendig!","type":"content","url":"/chapter05-sec03#daten-filtern","position":5},{"hierarchy":{"lvl1":"5.3 Daten filtern und gruppieren","lvl2":"Daten gruppieren"},"type":"lvl2","url":"/chapter05-sec03#daten-gruppieren","position":6},{"hierarchy":{"lvl1":"5.3 Daten filtern und gruppieren","lvl2":"Daten gruppieren"},"content":"Eine Filterung nach Kilometerstand ermöglicht es uns, die Autos in zwei\nDatensätze zu teilen: Autos mit bis zu 200000 km Laufleistung und jene mit mehr\nals 200000 km (hierzu kann der Tilde-Operator (~) verwendet werden).\n\nWenden wir nun diese Technik an, um die Fahrzeuge basierend auf ihrer Marke zu\ntrennen. Ein Beispiel: Um alle “Audi”-Fahrzeuge zu extrahieren, verwenden wir\nden folgenden Code:\n\nbedingung_audi = data['Marke'] == 'audi'\naudis = data[bedingung_audi]\naudis.info()\n\n\n\nDiese Bedingung erfüllen 1.190 Autos. Der Gesamtdatensatz enthält jedoch 41\nunterschiedliche Automarken. Es wäre ineffizient, für jede Marke eine separate\nFilterung durchzuführen. Deshalb bietet Pandas die .groupby()-Methode, die es\nerlaubt, die Daten automatisch nach den einzigartigen Einträgen einer Spalte zu\ngruppieren:\n\nautos_nach_marke = data.groupby('Marke')\ntype(autos_nach_marke)\n\n\n\nDas Resultat ist eine spezielle Pandas-Datenstruktur namens DataFrameGroupBy.\nAuf dieses Objekt sind nicht alle bekannten DataFrame-Methoden anwendbar, aber\nbeispielsweise die .describe()-Methode darf verwendet werden:\n\nautos_nach_marke.describe()\n\n\n\nFür jede Automarke werden nun für jede Spalte mit numerischen Informationen die\nstatistischen Kennzahlen ermittelt. Die entstehende Tabelle ist etwas\nunübersichtlich. Besser ist daher, sich die statistischen Kennzahlen einzeln\nausgeben zu lassen. Im Folgenden ermitteln wir die Mittelwerte der numerischen\nInformationen nach Automarke. Ohne das Argument numeric_only=True würde Pandas\nversuchen, auch die nicht-numerischen Spalten (wie ‘Marke’ oder ‘Farbe’) zu\nmitteln, was zu einer Fehlermeldung führen würde. Mit diesem Argument wird die\nOperation nur auf numerische Spalten angewendet.\n\ndurchschnittspreis_pro_marke = autos_nach_marke['Preis (Euro)'].mean()\n\n# Visualisierung\nimport plotly.express as px\nfig = px.bar(durchschnittspreis_pro_marke, \n             title='Durchschnittlicher Preis pro Automarke',\n             labels = {'value': 'Preis [EUR]'})\nfig.show()\n\n\n\nEine sehr wichtige Methode der GroupBy-Datenstruktur ist die\nget_group()-Methode. Damit können wir ein bestimmtes DataFrame-Objekt aus dem\nGroupBy-Objekt extrahieren:\n\naudis_alternativ = autos_nach_marke.get_group('audi')\naudis_alternativ.info()\n\n\n\nIn der Variablen audis_alternativ steckt nun der gleiche Datensatz wie in der\nVariablen audis, den wir bereits durch das Filtern des ursprünglichen\nDatensatzes extrahiert haben. Beide Methoden führen zum gleichen Ergebnis. Die\nFilterung mit Bedingungen ist direkter und oft intuitiver. Die Gruppierung mit\n.groupby() und .get_group() ist besonders nützlich, wenn wir mehrere Gruppen\nnacheinander untersuchen möchten, da wir die Gruppierung nur einmal durchführen\nmüssen.\n\nMini-Übung\n\nGruppieren Sie die Autos nach ‘Kraftstoff’ und berechnen Sie den\ndurchschnittlichen Preis für jede Kraftstoffart. Welche Kraftstoffart\nist im Durchschnitt am teuersten?\n\nLösungautos_nach_kraftstoff = data.groupby('Kraftstoff')\ndurchschnittspreis = autos_nach_kraftstoff['Preis (Euro)'].mean()\nprint(durchschnittspreis.sort_values(ascending=False))\n\nDie teuerste Kraftstoffart im Durchschnitt ist Hybrid (Elektro/Diesel). Einträge\nohne Angabe vernachlässigen wir hier.","type":"content","url":"/chapter05-sec03#daten-gruppieren","position":7},{"hierarchy":{"lvl1":"5.3 Daten filtern und gruppieren","lvl2":"Wann filtern, wann gruppieren?"},"type":"lvl2","url":"/chapter05-sec03#wann-filtern-wann-gruppieren","position":8},{"hierarchy":{"lvl1":"5.3 Daten filtern und gruppieren","lvl2":"Wann filtern, wann gruppieren?"},"content":"Wir filtern, wenn wir einen Ausschnitt der Daten analysieren wollen. Zum\nBeispiel könnten wir nur die Audis untersuchen wollen. Das Ergebnis ist ein\nneuer DataFrame mit den gefilterten Zeilen. Wenn wir jedoch alle Kategorien\nvergleichen wollen, nutzen wir die Gruppierung. Beispielsweise könnten wir die\nDurchschnittpreise pro Marke berechnen wollen.\n\nBeide Methoden können auch kombiniert werden. Wir können auch erst gruppieren\nund dann eine spezifische Gruppe mit .get_group() extrahieren. Dies ist\nbesonders nützlich, wenn wir die Gruppierung für mehrere Analysen\nwiederverwenden möchten.","type":"content","url":"/chapter05-sec03#wann-filtern-wann-gruppieren","position":9},{"hierarchy":{"lvl1":"5.3 Daten filtern und gruppieren","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter05-sec03#zusammenfassung-und-ausblick","position":10},{"hierarchy":{"lvl1":"5.3 Daten filtern und gruppieren","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Kapitel haben wir die Technik des Datenfilterns kennengelernt. Um\nspezifische Einträge aus einem Datensatz basierend auf einem bestimmten Wert zu\nextrahieren, nutzen wir Vergleichsoperationen und verwenden das resultierende\nSeries-Objekt als Index. Wenn das Ziel darin besteht, Daten anhand der\neinzigartigen Werte einer Spalte zu gruppieren, dann ist die Kombination von\n.groupby() und .get_group() oft der effizienteste Weg. Damit haben wir\nunsere Einführung in die Datenexploration abgeschlossen, obwohl es noch viele\nweitere Möglichkeiten gibt, die Daten zu erkunden. Im nächsten Kapitel beginnen\nwir mit den Grundlagen des maschinellen Lernens und beschäftigen uns mit\nEntscheidungsbäumen.","type":"content","url":"/chapter05-sec03#zusammenfassung-und-ausblick","position":11},{"hierarchy":{"lvl1":"Übungen"},"type":"lvl1","url":"/chapter05-sec04","position":0},{"hierarchy":{"lvl1":"Übungen"},"content":"Hinweis\n\nDie in dieser Übung verwendeten csv-Dateien können Studierende der Frankfurt UAS\nüber meinen campUAS-Kurs herunterladen. Für Externe sind die Quellen verlinkt,\njedoch werden ein Kaggle- bzw. ein Statista-Account benötigt.\n\nÜbung 5.1\n\nSchauen Sie sich die Datei ‘kaggle_germany-wind-energy.csv’ im Texteditor bzw.\nim JupyterLab an. Welche Spalte könnte als Zeilenindex dienen? Importieren Sie\npassend die Daten.\n\nVerschaffen Sie sich einen Überblick über die Daten. Welche Messwerte stehen in\nden Spalten? Weitere Informationen erhalten Sie hier:\n\n\nhttps://​www​.kaggle​.com​/datasets​/aymanlafaz​/wind​-energy​-germany\n\nErmitteln Sie die statistischen Kennzahlen und visualisieren Sie die\nstatistischen Kennzahlen als Boxplot. Ist es sinnvoll, einen gemeinsamen Boxplot\nzu verwenden? Interpretieren Sie die statistischen Kennzahlen.\n\nVisualisieren Sie die drei Eigenschaften als Scatterplots. Welche\nSchlussfolgerungen ziehen Sie aus den Plots?\n\nVisualisieren Sie drei Eigenschaften als Scattermatrix. Gibt es Abhängigkeiten?\n\nLösungimport pandas as pd\n\ndata = pd.read_csv('kaggle_germany-wind-energy.csv', index_col=0)\ndata.info()\n\nDie erste Spalte enthält Datumsangaben (vermutlich in UTC) und bietet sich daher\nals Zeilenindex an. Der Zeilenindex reicht dann vom 01.01.2017 bis zum\n30.12.2019.\n\nInsgesamt sind es dann drei Spalten mit Angaben zu\n\nwind_generation_actual,\n\nwind_capacity,\n\ntemperature.\n\nDie Quelle der Daten verrät, dass sich dahinter die\n\ntägliche produzierte Windenergie in MW,\n\nelektrische Windkapaität in MW und\n\nTemperatur in Grad Celsius\n\nverbergen.\n\nAlle Spalten enthalten 1094 gültige Einträge. Alle Spalten enthalten Floats.data.describe()import plotly.express as px\n\nfig = px.box(data, title='Windenergie in Deutschland von 2017 bis 2019')\nfig.show()\n\nEs nicht sinnvoll, einen gemeinsamen Boxplot zu verwenden. Die Unterschiede der\nMesswerte sind zu groß. Beispielsweise kann die Temperatur gar nicht mehr\nabgelesen werden. Außerdem haben die Spalten verschiedene Einheiten und sind\ndaher gar nicht direkt vergleichbar.\n\nIm Folgenden werden daher die statistischen Kennzahlen der drei Eigenschaften\neinzeln visualisiert.fig = px.box(data, y = 'wind_generation_actual', title='Windenergie in Deutschland von 2017 bis 2019')\nfig.show()\n\nDer Median liegt einigermaßen mittig zwischen dem Q1 und dem Q3-Quartil, jedoch gibt es einige Ausreißer nach oben. Die Ausreißer führen auch dazu, dass der Mittelwert 305819 MW über dem Median von 254332 MW liegt.fig = px.box(data, y = 'wind_capacity', title='Windenergie in Deutschland von 2017 bis 2019')\nfig.show()\n\nBei der Windkapazität gibt es keine Ausreißer. Der Median ist nicht mittig zwischen Q1 und Q3, stimmt aber ganz gut mit dem Mittelwert von 45066 MW überein.fig = px.box(data, y = 'temperature', title='Windenergie in Deutschland von 2017 bis 2019')\nfig.show()\n\nDie Temperatur ist gleichmäßig verteilt, der Median liegt mittig zwischen Q1 und\nQ3 und passt auch sehr gut zum Mittelwert 10.5 Grad Celsius.\n\nAls nächstes visualisieren wir den zeitlichen Verlauf der drei Eigenschaften.fig = px.scatter(data, y = 'wind_generation_actual')\nfig.show()\n\nDie Winderzeugung ist starken Schwankungen unterworfen. Dabei sind die\nSchwankungen in den Sommermonaten kleiner als in den Wintermonaten.fig = px.scatter(data, y = 'wind_capacity')\nfig.show()\n\nDie Windkapazität wurde seit Januar 2017 beständig ausgebaut. Im Januar 2017\nbetrug sie nur 37149 MW, im Dezember 2019 waren es 50542 MW. Das ist eine\nSteigerung um 36 %.fig = px.scatter(data, y = 'temperature')\nfig.show()\n\nWie erwartet schwanken die Temperaturen periodisch mit den Jahreszeiten. In den\nWintermonaten sind die Temperaturen niedrig, in den Monaten Juli / August am\nhöchsten.\n\nAls nächstes visualisieren wir die Scattermatrix.fig = px.scatter_matrix(data)\nfig.show()\n\nBei der Kombination wind_generation_actual - wind_capacity ist kein funktionaler\nZusammenhang erkennbar. Bei der Kombination wind_capacity - temperature sieht es\nnach einem Zusammenhang aus, aber die Temperatur kann nicht vom Ausbau der\nInfrastruktur in Deutschland abhängen und umgekehrt. Bleibt noch die Kombination\ntemperature - wind_generation_actual. Scheinbar wird bei Temperaturen von\nungefähr 5 Grad Celsius am meisten Windenergie erzeugt. Bei kälteren\nTemperaturen (Minusgrade) oder höheren Temperaturen (über 20 Grad Celsius) wird\nweniger als die Hälfte des Maximalwertes erzeugt.\n\nÜbung 5.2\n\nImportieren Sie den Datensatz ‘kaggle_ikea.csv’ und verschaffen Sie sich einen\nÜberblick über die Daten (Quelle: \n\nKaggle).\n\nIn welche verschiedenen Kategorien (category) sind die IKEA-Artikel unterteilt?\nErstellen Sie für jede Kategorie einen Boxplot der Verkaufspreise und ziehen Sie\nSchlussfolgerungen daraus.\n\nLassen Sie für jede Kategorie den durchschnittlichen Preis dieser Kategorie als\nBarplot visualisieren. Lesen Sie danach ab: welche Kategorie hat den geringsten\nDurchschnittspreis?\n\nLassen Sie für diese Kategorie die Anzahl der Artikel bestimmen. Visualisieren\nSie dann den Preis in Abhängigkeit des Namens als Scatterplot. Bei welchem Namen\ngibt es die größte Spannweite an Verkaufspreisen? Welches ist der teuerste\nArtikel (ID?) in dieser Kategorie und wie wird er beschrieben?\n\nLösungimport pandas as pd\n\ndata = pd.read_csv('kaggle_ikea.csv', index_col=0)\ndata.info()\n\nDie Daten bestehen aus 3694 Einträgen mit Waren, die von Ikea verkauft werden.\nDie Gegenstände werden durch 12 Eigenschaften beschrieben. Dabei sind die\nmeisten Eigenschaften Objekte (name, category, old_price, link, other_colors,\nshort_description, designer). Numerische Eigenschaften sind price, depth, height\nund width. Ob der Gegenstand auch Online gekauft werden kann, verbirgt sich in\nder Spalte sellable_online mit boolschen Werten. Nicht alle Einträge sind\ngültig. Bei den Eigenschaften depth, height und width fehlen Angaben.data.head(10)data.describe()\n\nBei den Daten ist vor allem der Preis interessant. Der billigste Artikel kostet\n3 Euro, der teuerste Artikel 9585 Euro. Aber 50 % aller Artikel kosten weniger\nals 544 Euro. Die Größenangaben sind nicht vollständig und etwas schwieriger zu\ninterpretieren. Offensichtlich ist 1 cm die kleinste Größenangabe bei allen drei\nAngaben und 2.57 m, 7 m und 4.2 m die maximalen Größen bei Tiefe, Höhe und\nBreite.data['category'].unique()\n\nEs gibt 17 verschiedene Kategorien.\n\nAls nächstes werden die Verkaufspreise pro Kategorie statistisch ausgewertet.categories = data.groupby('category')\ncategories['price'].describe()import plotly.express as px\n\nfig1 = px.box(data, x = 'category', y = 'price')\nfig1.show()\n\nAm teuersten sind Artikel in der Kategorie Betten (Beds) und Sofas (Sofas &\nArmchairs). Es ist interessant zu sehen, dass in beiden Kategorien der teuerste\nPreis 9585 EUR ist. Vielleicht ist das ein Artikel, der sowohl als Bett als auch\nSofa kategorisiert wurde, ein Schlafsofa zum Beispiel.fig = px.bar(categories['price'].mean(), title='Durchnittspreis IKEA',\n             labels={'value': 'Preis in Euro', 'variable': 'Legende'})\nfig.show()\n\nDen geringsten Durchschnittspreis hat die Kategorie Kindermöbel (Children’s furniture).children_furnitures = categories.get_group(\"Children's furniture\")\nchildren_furnitures.info()\n\nIn der Kategorie Kindermöbel werden 124 Artikel verkauft.fig = px.scatter(children_furnitures, x = 'name', y = 'price')\nfig.show()\n\nDie größte Spannweite an Preisen gibt es bei Artikel mit dem Namen Stuva bzw.\nFritids. Er kostet 1545 EUR.most_expensive_children_furniture = children_furnitures[ children_furnitures['price'] == 1545] \nmost_expensive_children_furniture.head()\n\nBei dem teuersten Möbel in der Kategorie der Kindermöbel handelt es sich um\neinen Schrank mit der Artikel-ID “19275075” und der Beschreibung “Wardrobe,\n120x50x192 cm”.print(most_expensive_children_furniture['link'])\n\nÜbung 5.3\n\nLesen Sie die csv-Datei\n‘statistic_id1301764_formel1-fahrerwertung-saison-2022.csv’ (Formel 1\nFahrerwertung, Stand 30.10.2022, Quelle:\n\n\nhttps://​de​.statista​.com​/statistik​/daten​/studie​/1301764​/umfrage​/formel​-1​-wm​-stand/)\nein.\n\nFühren Sie eine statistische Datenanalyse inklusive Visualisierung durch.\nVisualisieren Sie zusätzlich die Fahrerwertung.\n\nHinweis: Diese Übung ist bewusst offener formuliert. Nutzen Sie die Methoden aus\nden vorherigen Kapiteln bzw. Übungen selbstständig.\n\nLösungimport pandas as pd\n\ndata = pd.read_csv('statistic_id1301764_formel1-fahrerwertung-saison-2022.csv', index_col=0, skiprows=2)\ndata.info()\n\nDie Datei enthält 21 Zeilen mit den Fahrernamen und eine Spalte mit der Wertung. Der Datentyp der Spalte Wertung ist Integer. Alle Einträge sind gültig.data.head()\n\nStatistische Analyse:data.describe()import plotly.express as px\n\nfig = px.box(data, title='Formel-1-Saison 2022', labels={'value': 'Punkte', 'variable': ''})\nfig.show()\n\nDer Mittelwert der erreichten Punkte in der Fahrerwertung liegt bei 100.5\nPunkten, der Median jedoch bei 36 Punkten. Es liegt also eine deutlich schiefe\nVerteilung der Punkte vor. 50 % der Fahrer haben weniger als 36 Punkte erreicht,\ndie besten 25 % der Fahrer haben mehr als 213 Punkte. Diese ungleiche Verteilung\nder Punkte zeigt sich auch in einer hohen Standardabweichung von 120 Punkten,\ndie größer als der Mittelwert ist.\n\nVerteilung der Wertung auf die einzelnen Fahrer:fig = px.bar(data, title='Formel-1-Saison 2022',\n             labels={'value':'Punkte', 'variable': 'Legende'})\nfig.show()\n\nDie Visualisierung der Wertung der Fahrer zeigt, das Max Verstappen mit 416 Punkten die Saison gewonnen hat. Dabei hatte er einen deutlichen Vorsprung vor Sergio Perez, der nur 280 Punkte erreichte. Somit hat der Zweitplatzierte nur ca. 67 % der Punkte von Verstappen erreicht.","type":"content","url":"/chapter05-sec04","position":1},{"hierarchy":{"lvl1":"6.1 Was ist ein Entscheidungsbaum?"},"type":"lvl1","url":"/chapter06-sec01","position":0},{"hierarchy":{"lvl1":"6.1 Was ist ein Entscheidungsbaum?"},"content":"Ein beliebtes Partyspiel ist das Spiel “Wer bin ich?”. Die Spielregeln sind\nsimpel. Eine Person wählt eine berühmte Person oder eine Figur aus einem Film\noder Buch, die die Mitspieler:innen erraten müssen. Die Mitspieler:innen\ndürfen jedoch nur Fragen stellen, die mit “Ja” oder “Nein” beantwortet werden.\n\nHier ist ein Beispiel, wie eine typische Runde von “Wer bin ich?” ablaufen\nkönnte:\n\nSpieler 1: Bin ich männlich?\n\nSpieler 2: Ja.\n\nSpieler 3: Bist du ein Schauspieler?\n\nSpieler 1: Nein.\n\nSpieler 4: Bist du ein Musiker?\n\nSpieler 1: Ja.\n\nSpieler 5: Bist du Michael Jackson?\n\nSpieler 1: Ja! Richtig!\n\nAls nächstes wäre jetzt Spieler 5 an der Reihe, sich eine Person oder Figur\nauszuwählen, die die anderen Spieler erraten sollen. Vielleicht kennen Sie auch\ndie umgekehrte Variante. Der Name der zu ratenden Person/Figur wird der Person\nmit einem Zettel auf die Stirn geklebt. Und nun muss die Person raten, während\ndie Mitspieler:innen mit Ja/Nein antworten.\n\nDieser Partyklassiker lässt sich auch auf das maschinelle Lernen übertragen.","type":"content","url":"/chapter06-sec01","position":1},{"hierarchy":{"lvl1":"6.1 Was ist ein Entscheidungsbaum?","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter06-sec01#lernziele","position":2},{"hierarchy":{"lvl1":"6.1 Was ist ein Entscheidungsbaum?","lvl2":"Lernziele"},"content":"Lernziele\n\nSie wissen, was ein Entscheidungsbaum (Decision Tree) ist.\n\nSie kennen die Bestandteile eines Entscheidungsbaumes:\n\nWurzelknoten (Root Node)\n\nKnoten (Node)\n\nZweig oder Kante (Branch)\n\nBlatt (Leaf)\n\nSie können einen Entscheidungsbaum mit Scikit-Learn trainieren.\n\nSie können mit Hilfe eines Entscheidungsbaumes Prognosen treffen.","type":"content","url":"/chapter06-sec01#lernziele","position":3},{"hierarchy":{"lvl1":"6.1 Was ist ein Entscheidungsbaum?","lvl2":"Ein Entscheidungsbaum im Autohaus"},"type":"lvl2","url":"/chapter06-sec01#ein-entscheidungsbaum-im-autohaus","position":4},{"hierarchy":{"lvl1":"6.1 Was ist ein Entscheidungsbaum?","lvl2":"Ein Entscheidungsbaum im Autohaus"},"content":"Ein Entscheidungsbaum gehört zu den überwachten Lernverfahren (Supervised\nLearning). Es ist auch üblich, die englische Bezeichnung Decision Tree\nanstatt des deutschen Begriffes zu nutzen. Ein großer Vorteil von\nEntscheidungsbäumen ist ihre Flexibilität, denn sie können sowohl für\nKlassifikations- als auch Regressionsaufgaben eingesetzt werden. Im Folgenden\nbetrachten wir als Beispiel eine Klassifikationsaufgabe. In einem Autohaus\nvereinbaren zehn Personen eine Probefahrt. In der folgenden Tabelle ist für\njedes Auto notiert, welchen\n\nKilometerstand [in km] und\n\nPreis [in EUR]\n\nes hat. In der dritten Spalte verkauft ist vermerkt, ob das Auto nach der\nProbefahrt verkauft wurde (True) oder nicht (False). Diese Information ist\ndie Zielgröße. Die Tabelle mit den Daten lässt sich effizient mit einem\nPandas-DataFrame organisieren:\n\nimport pandas as pd \n\ndaten = pd.DataFrame({\n      'Kilometerstand [km]': [32908, 20328, 13285, 17162, 27449, 13715, 32889,  3111, 15607, 18295],\n      'Preis [EUR]': [15960, 20495, 17227, 17851, 5428, 22772, 13581, 16793, 23253, 11382],\n      'verkauft': [False, True, False, True, False, True, False, True, True, False],\n    },\n    index=['Auto 1', 'Auto 2', 'Auto 3', 'Auto 4', 'Auto 5', 'Auto 6', 'Auto 7', 'Auto 8', 'Auto 9', 'Auto 10'])\ndaten.head(10)\n\n\n\nDa in unserem Beispiel von den Autos nur die beiden Eigenschaften\nKilometerstand [km] und Preis [EUR] erfasst wurden, können wir die\nDatenpunkte anschaulich in einem zweidimensionalen Streudiagramm (Scatterplot)\nvisualisieren. Dabei wird der Kilometerstand auf der x-Achse und der Preis auf\nder y-Achse abgetragen. Die Zielgröße verkauft kennzeichnen wir durch die\nFarbe. Dabei steht die Farbe Rot für »verkauft« (True) und Blau für »nicht\nverkauft« (False).\n\nimport plotly.express as px\n\nfig = px.scatter(daten, x='Kilometerstand [km]', y='Preis [EUR]', \n                 color='verkauft', title='Künstliche Daten: Verkaufsaktion im Autohaus')\nfig.show()\n\n\n\nAls nächstes zeigen wir, wie die Autos anhand von Fragen in die beiden Klassen\n»verkauft« und »nicht verkauft« sortiert werden können. Im Streudiagramm\nvisualisieren wir die Autos mit ihren Eigenschaften Kilometerstand [km] und\nPreis [EUR] als Punkte. Dazu passend werden wir schrittweise den\nEntscheidungsbaum entwickeln. Ein Entscheidungsbaum visualisiert\nEntscheidungsregeln in Form einer Baumstruktur. Zu Beginn wurde noch keine Frage\ngestellt und alle Autos befinden sich gemeinsam in einem Knoten (Node) des\nEntscheidungsbaumes, der visuell durch einen rechteckigen Kasten symbolisiert\nwird. Dieser erste Knoten wird als Wurzelknoten (Root Node) bezeichnet, da\ner die Wurzel des Entscheidungsbaumes darstellt.\n\nDann wird eine erste Frage gestellt. Ist der Verkaufspreis kleiner oder gleich\n16376.50 EUR? Entsprechend dieser Entscheidung werden die Autos in zwei Gruppen\naufgeteilt. Wenn ja, wandern die Autos nach links und ansonsten nach rechts.\nWarum wir die Grenze 16376.50 gewählt haben, werden wir in einem späteren\nKapitel besprechen. Im Entscheidungsbaum wird diese Aufteilung durch einen\nZweig (Branch) nach links und einen Zweig nach rechts symbolisiert. Ein\nalternativer Name für Zweig ist Kante. Die Autos »rutschen« die\nZweige/Kanten entlang und landen in zwei separaten Knoten. Im Streudiagramm\n(Scatterplot) entspricht diese Fragestellung dem Vergleich mit einer\nhorizontalen Linie bei y = 16376.5. Da alle Autos mit einem Verkaufspreis\nkleiner/gleich 16376.5 EUR blau sind, also »nicht verkauft« wurden, wird im\nStreudiagramm (Scatterplot) alles unterhalb der horizontalen Linie blau\neingefärbt.\n\nBei den Autos mit einem Preis kleiner oder gleich 16376.50 EUR müssen wir nicht\nweiter sortieren bzw. weitere Fragen stellen. Da aus diesem Knoten keine Zweige\nmehr wachsen, wird dieser Knoten auch Blatt (Leaf) genannt. Aber in dem\nKnoten des rechten Zweiges befinden sich fünf rote (also verkaufte) Autos und\nein blaues (also nicht verkauftes) Auto. Wir wollen diese Autos durch weitere\nFragen sortieren. Doch obwohl nur ein Auto (nämlich Auto 3) aus dieser Gruppe\nsepariert werden soll, ist dies nicht durch nur eine einzige Frage möglich.\nLautet die Frage: »Ist der Preis kleiner oder gleich 17300 EUR?«, dann wandern\ndas rote Auto 8 und das blaue Auto 3 nach links. Wählen wir die Frage: »Ist der\nKilometerstand kleiner oder gleich 13500 km?«, dann wandern ebenfalls Auto 3 und\nAuto 8 nach links. Beide Fragen sind also gleichwertig, welche sollen wir\nnehmen? In diesem vereinfachten Beispiel wählen wir willkürlich den\nKilometerstand. Der Algorithmus brauchte jedoch Kriterien, um die beste Trennung\nzu finden. Darauf gehen wir im nächsten Kapitel ein.\n\nIm Streudiagramm (Scatterplot) wird die noch nicht eingefärbte Fläche rechts der\nvertikalen Linie 13500 km rot gefärbt. Im linken Knoten (Node) sind aber nur\nnoch zwei Autos, so dass diesmal eine weitere Frage ausreicht, die beiden Autos\nin zwei Klassen zu sortieren. Wir fragen: »Ist der Kilometerstand kleiner oder\ngleich 8198 km?«\n\nAlle Autos sind nun durch die Fragen sortiert und befinden sich in Blättern\n(Leaves). Im Streudiagramm (Scatterplot) wird dieser Zustand kenntlich gemacht,\nindem auch die letzte verbleibende Fläche (oberhalb eines Preises von 16376.50\nEUR) links von Kilometerstand 8198 km rot und rechts davon blau eingefärbt wird.\n\nWas ist ... ein Entscheidungsbaum?\n\nEin Entscheidungsbaum (Decision Tree) ist ein Modell zur Entscheidungsfindung,\ndas Daten mit Hilfe einer Baumstruktur sortiert. Die Datenobjekte beginnen beim\nWurzelknoten (= Ausgangssituation) und werden dann über Knoten (=\nEntscheidungsfrage) und Zweige/Kanten (= Ergebnis der Entscheidung) in Blätter\n(= Endzustand des Entscheidungsprozesses) sortiert.\n\nVideo “Decision Tree Classification Clearly Explained!” von Normalized Nerd\n\nVideo “Maschinelles Lernen - Beispiel Entscheidungsbaum”\n\nVideo zu “Entscheidungsbäume #1” von The Morpheus Tutorials","type":"content","url":"/chapter06-sec01#ein-entscheidungsbaum-im-autohaus","position":5},{"hierarchy":{"lvl1":"6.1 Was ist ein Entscheidungsbaum?","lvl2":"Entscheidungsbäume mit Scikit-Learn trainieren"},"type":"lvl2","url":"/chapter06-sec01#entscheidungsb-ume-mit-scikit-learn-trainieren","position":6},{"hierarchy":{"lvl1":"6.1 Was ist ein Entscheidungsbaum?","lvl2":"Entscheidungsbäume mit Scikit-Learn trainieren"},"content":"In der Praxis verwenden wir die ML-Bibliothek Scikit-Learn, um einen\nEntscheidungsbaum zu trainieren. Das Modul \n\nScikit-Learn →\nTree stellt sowohl einen\nEntscheidungsbaum-Algorithmus für Klassifikationsprobleme als auch einen\nAlgorithmus für Regressionsprobleme zur Verfügung. Für das obige Beispiel\nAutohaus importieren wir den Algorithmus für Klassifikationsprobleme namens\nDecisionTreeClassifier:\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n\nDann erzeugen wir ein noch untrainiertes Entscheidungsbaum-Modell und weisen es\nder Variable modell zu:\n\nmodell = DecisionTreeClassifier()\n\n\n\nBei der Erzeugung könnten wir noch verschiedene Optionen einstellen, die in der\n\n\nDokumentation Scikit-Learn →\nDecisionTreeClassifier\nnachgelesen werden können. Zunächst belassen wir es aber bei den\nStandardeinstellungen.\n\nAls nächstes adaptieren wir die Daten aus dem Pandas-DataFrame so, dass das\nEntscheidungsbaum-Modell trainiert werden kann. Der DecisionTreeClassifier\nerwartet für das Training zwei Argumente. Als erstes Argument müssen die\nEingabedaten übergeben werden, also die Eigenschaften der Autos. Als zweites\nArgument erwartet der DecisionTreeClassifier die Zielgröße, also den Status\n»nicht verkauft« oder »verkauft«. Wir trennen daher den Pandas-DataFrame daten\nauf und verwenden die Bezeichnung X für die Eingabedaten und y für die\nZielgröße.\n\nX = daten[['Kilometerstand [km]', 'Preis [EUR]']]\ny = daten['verkauft']\n\n\n\nAls nächstes wird der Entscheidungsbaum trainiert. Dazu wird die Methode\n.fit() mit den beiden Argumenten X und y aufgerufen.\n\nmodell.fit(X,y)\n\n\n\nJetzt ist zwar der Entscheidungsbaum trainiert, doch wir sehen nichts. Als\nerstes überprüfen wir mit der Methode .score(), wie gut die Prognose des\nEntscheidungsbaumes ist.\n\nscore = modell.score(X,y)\nprint(score)\n\n\n\nEine 1 steht für 100 %, also alle 10 Autos werden korrekt klassifiziert. Dazu\nhat der DecisionTreeClassifier basierend auf den Eingabedaten X eine\nPrognose erstellt und diese Prognose mit den echten Daten in y verglichen. Für\ndie Trainingsdaten funktioniert der Entscheidungsbaum also perfekt. Ob der\nEntscheidungsbaum ein neues, elftes Auto korrekt klassifizieren würde, kann so\nerst einmal nicht entschieden werden. Möglicherweise hat der Entscheidungsbaum\ndie Trainingsdaten auswendig gelernt, anstatt allgemeine Muster zu erkennen. Es\nbesteht die Gefahr des sogenannten Overfittings, auf die wir im übernächsten\nKapitel noch eingehen werden.","type":"content","url":"/chapter06-sec01#entscheidungsb-ume-mit-scikit-learn-trainieren","position":7},{"hierarchy":{"lvl1":"6.1 Was ist ein Entscheidungsbaum?","lvl2":"Prognosen mit Entscheidungsbäumen treffen"},"type":"lvl2","url":"/chapter06-sec01#prognosen-mit-entscheidungsb-umen-treffen","position":8},{"hierarchy":{"lvl1":"6.1 Was ist ein Entscheidungsbaum?","lvl2":"Prognosen mit Entscheidungsbäumen treffen"},"content":"Soll für neue Autos eine Prognose abgegeben werden, ob sie sich eher verkaufen\nlassen oder nicht, müssen die neuen Daten die gleiche Struktur wie die\nEingangsdaten haben. Wir erzeugen daher einen neuen Pandas-DataFrame, bei dem\ndie erste Eigenschaft der Kilometerstand der neuen Autos ist und die zweite\nEigenschaft ihr Preis.\n\nneue_autos = pd.DataFrame({\n    'Kilometerstand [km]': [11300, 20000, 7580],\n    'Preis [EUR]': [12000, 14999, 20999]\n    },\n    index=['Auto 11', 'Auto 12', 'Auto 13'])\n\n\n\nMit Hilfe der predict()-Methode kann dann der Entscheidungsbaum\nprognostizieren, ob die Autos verkauft werden oder nicht.\n\nprognose = modell.predict(neue_autos)\nprint(prognose)\n\n\n\nUm für ein neues Auto eine Prognose abzugeben, werden zunächst den Blättern\nKlassen zugeordnet. Dazu wird die sogenannte Reinheit der Blätter untersucht.\nBefinden sich nur Autos einer einzigen Klasse in einem Blatt, so nennen wir das\nBlatt rein und es bekommt diese Klasse zugeordnet. Ist ein Blatt nicht rein,\nsondern enthält noch Autos mit unterschiedlichen Klassen »verkauft« oder »nicht\nverkauft«, so wird diesem Blatt diejenige Klasse zugeordnet, die am häufigsten\nauftritt. Um diese Idee zu visualisieren, färben wir im Entscheidungsbaum die\nBlätter entsprechend rot und blau ein.\n\nJedes neue Auto durchläuft jetzt die Entscheidungen, bis es in einem Blatt\nangekommen ist. Die Klasse des Blattes ist dann die Prognose für dieses Auto.\n\nDer Entscheidungsbaum prognostiziert, dass Auto 11 und Auto 12 nicht verkauft\nwerden, aber Auto 13 könnte verkaufbar sein.\n\nMini-Übung\n\nTrainieren Sie einen Entscheidungsbaum für die folgenden Daten:test_daten = pd.DataFrame({\n    'Kilometerstand [km]': [5000, 25000, 15000, 30000],\n    'Preis [EUR]': [18000, 14000, 19000, 12000],\n    'verkauft': [True, False, True, False]\n})\n\nGeben Sie eine Prognose für ein Auto mit 10.000 km und 16.000 EUR ab.\n\n# Hier Ihr Code\n\n\n\nLösungtest_daten = pd.DataFrame({\n    'Kilometerstand [km]': [5000, 25000, 15000, 30000],\n    'Preis [EUR]': [18000, 14000, 19000, 12000],\n    'verkauft': [True, False, True, False]\n})\n\nX_test = test_daten[['Kilometerstand [km]', 'Preis [EUR]']]\ny_test = test_daten['verkauft']\n\nmodell_test = DecisionTreeClassifier()\nmodell_test.fit(X_test, y_test)\n\nneues_auto = pd.DataFrame({\n    'Kilometerstand [km]': [10000],\n    'Preis [EUR]': [16000]\n})\n\nprognose = modell_test.predict(neues_auto)\nprint(f\"Prognose für das neue Auto: {prognose[0]}\")","type":"content","url":"/chapter06-sec01#prognosen-mit-entscheidungsb-umen-treffen","position":9},{"hierarchy":{"lvl1":"6.1 Was ist ein Entscheidungsbaum?","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter06-sec01#zusammenfassung-und-ausblick","position":10},{"hierarchy":{"lvl1":"6.1 Was ist ein Entscheidungsbaum?","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Kapitel haben Sie den Entscheidungsbaum (Decision Tree) anhand einer\nKlassifikationsaufgabe kennengelernt. Mit Hilfe von Scikit-Learn wurde ein\nEntscheidungsbaum trainiert und dazu benutzt, eine Prognose für neue Daten\nabzugeben. Im nächsten Kapitel werden wir uns damit beschäftigen, weitere\nEinstellmöglichkeiten beim Training des Entscheidungsbaumes zu nutzen und\nEntscheidungsbäume durch Scikit-Learn visualisieren zu lassen.","type":"content","url":"/chapter06-sec01#zusammenfassung-und-ausblick","position":11},{"hierarchy":{"lvl1":"6.2 Entscheidungsbäume visualisieren und trainieren"},"type":"lvl1","url":"/chapter06-sec02","position":0},{"hierarchy":{"lvl1":"6.2 Entscheidungsbäume visualisieren und trainieren"},"content":"Im letzten Kapitel haben wir gelernt, wie mit Scikit-Learn ein Entscheidungsbaum\nfür binäre Klassifikationsaufgaben trainiert wird. In diesem Kapitel werden wir\nuns damit beschäftigen, den trainierten Entscheidungsbaum von Scikit-Learn\nvisualisieren zu lassen. Darüber hinaus lernen wir, was das\nGini-Impurity-Kriterion ist und welche weiteren Einstellmöglichkeiten es für\nEntscheidungsbäume in Scikit-Learn gibt.","type":"content","url":"/chapter06-sec02","position":1},{"hierarchy":{"lvl1":"6.2 Entscheidungsbäume visualisieren und trainieren","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter06-sec02#lernziele","position":2},{"hierarchy":{"lvl1":"6.2 Entscheidungsbäume visualisieren und trainieren","lvl2":"Lernziele"},"content":"Lernziele\n\nSie können einen Entscheidungsbaum mit plot_tree visualisieren.\n\nSie wissen, was die Angaben samples und value bei der Visualisierung des\nEntscheidungsbaumes bedeuten.\n\nSie wissen, was das Gini-Impurity-Kriterium ist.\n\nSie kennen weitere Parameter für Entscheidungsbäume wie random_state= oder\ncriterion=.","type":"content","url":"/chapter06-sec02#lernziele","position":3},{"hierarchy":{"lvl1":"6.2 Entscheidungsbäume visualisieren und trainieren","lvl2":"Entscheidungsbäume visualisieren"},"type":"lvl2","url":"/chapter06-sec02#entscheidungsb-ume-visualisieren","position":4},{"hierarchy":{"lvl1":"6.2 Entscheidungsbäume visualisieren und trainieren","lvl2":"Entscheidungsbäume visualisieren"},"content":"Im letzten Kapitel haben wir den Entscheidungsbaum für das Autohaus mit Hilfe\ndes Moduls Scikit-Learn trainiert. Scikit-Learn bietet in dem Untermodul\nsklearn.tree nicht nur Algorithmen für Entscheidungsbäume an, sondern auch ein\ndazu passendes Visualisierungswerkzeug. Die Funktion plot_tree zeichnet den\nEntscheidungsbaum. Um diese Funktion auszuprobieren, wird zunächst der Datensatz\nmit den Autodaten erneut geladen, das Modell Entscheidungsbaum gewählt und\nanschließend trainiert.\n\nimport pandas as pd \nfrom sklearn.tree import DecisionTreeClassifier\n\n# Sammlung der Daten \ndaten = pd.DataFrame({\n    'Kilometerstand [km]': [32908, 20328, 13285, 17162, 27449, 13715, 32889,  3111, 15607, 18295],\n    'Preis [EUR]': [15960, 20495, 17227, 17851, 5428, 22772, 13581, 16793, 23253, 11382],\n    'verkauft': [False, True, False, True, False, True, False, True, True, False],\n    },\n    index=['Auto 1', 'Auto 2', 'Auto 3', 'Auto 4', 'Auto 5', 'Auto 6', 'Auto 7', 'Auto 8', 'Auto 9', 'Auto 10'])\ndaten.head(10)\n\n# Auswahl des Modells: Entscheidungsbaum für Klassifikation\nmodell = DecisionTreeClassifier(random_state=0)\n\n# Adaption der Daten\nX = daten[['Kilometerstand [km]', 'Preis [EUR]']]\ny = daten['verkauft']\n\n# Training des Modells\nmodell.fit(X,y)\n\n\n\nNun können wir die Funktion plot_tree importieren und das trainierte Modell\nvisualisieren lassen.\n\nfrom sklearn.tree import plot_tree\n\nplot_tree(modell)\n\n\n\n\n\nplot_tree produziert eine Textausgabe und ein Diagramm. Die Textausgabe kann\nunterdrückt werden, indem hinter den Funktionsaufruf plot_tree(modell) ein\nSemikolon ; gesetzt wird. Das Diagramm zeichnet wie erwartet die Baumstruktur\nvom Wurzelknoten über die Knoten und Zweige bis hin zu den Blättern. Die\nEntscheidungsfragen stehen in der ersten Zeile der Knoten. Danach folgen weitere\nAngaben wie gini, samples und value. Um diese Angaben zu erklären,\nergänzen wir zunächst weitere Angaben. Mit der Option feature_names= wird eine\nListe mit den Eigenschaften ergänzt, die Option class_names= ergänzt die\nKlassenbezeichnungen. So erhalten wir folgendes Diagramm:\n\nplot_tree(modell, \n    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n    class_names=['nicht verkauft', 'verkauft']);\n\n\n\nWas gini bedeuten könnte, erschließt sich so immer noch nicht, aber die\nAngaben samples und values können so leichter von ihrer Bedeutung her\neingeordnet werden. samples gibt die Anzahl der Datenobjekte an, die sich in\ndiesem Knoten befinden. values listet auf, wie viele Datenobjekte die\nZielgröße nicht verkauft (= False bzw. 0) haben und wie viele zu der Klasse\nverkauft (= True bzw. 1) gehören.\n\nWeitere Details zu den Optionen der plot_tree-Funktion finden Sie in der\n\n\nDokumentation Scikit-Learn →\nplot_tree.\n\nAls nächstes widmen wir uns der Bedeutung von gini.","type":"content","url":"/chapter06-sec02#entscheidungsb-ume-visualisieren","position":5},{"hierarchy":{"lvl1":"6.2 Entscheidungsbäume visualisieren und trainieren","lvl2":"Was ist das Gini-Impurity-Kriterium?"},"type":"lvl2","url":"/chapter06-sec02#was-ist-das-gini-impurity-kriterium","position":6},{"hierarchy":{"lvl1":"6.2 Entscheidungsbäume visualisieren und trainieren","lvl2":"Was ist das Gini-Impurity-Kriterium?"},"content":"Das Gini-Impurity-Kriterium ist ein Maß für die Unreinheit eines Datensatzes.\nBeim Beispiel mit dem Autohaus sind im Wurzelknoten fünf Autos, die nicht\nverkauft wurden, und fünf verkaufte Autos. Bei zwei Klassen mit je 50 % Anteil\nist das die maximale Unreinheit, die auftreten kann. Der Anteil der verkauften\nAutos ist genau 50 %. Bei dieser Verteilung beträgt das Gini-Impurity-Kriterium\n0.5. Es gibt zwei weitere Extremfälle. Entweder sind nur verkaufte Autos im\nDatensatz (100 % verkaufte Autos) oder gar keine verkaufte Autos (0 % verkaufte\nAutos). In beiden Fällen ist der Datensatz rein, das Gini-Impurity-Kriterium ist\n0. In allen anderen Fällen liegt das Gini-Impurity-Kriterium zwischen 0 und 0.5.\nDie Formel zur Berechnung des genauen Wertes des Gini-Impurity-Kriteriums lautet\\text{GI} = 1 - p^2 - (1-p)^2,\n\nwenn p der prozentuale Anteil der verkauften Autos ist (das gilt natürlich\nallgemein für binäre Klassifikationsaufgaben und nicht nur das\nAutohaus-Beispiel).\n\nDie folgende Abbildung zeigt die konkreten Werte des Gini-Impurity-Kriteriums\nfür den prozentualen Anteil an verkauften Autos.\n\nfrom numpy import linspace\n\np = linspace(0,1)\ngini = 1 - p**2 - (1-p)**2\n\nimport plotly.express as px\n\nfig = px.line(x = p, y = gini,\n        title='Gini-Impurity-Kriterium',\n        labels={'x': 'prozentualer Anteil', 'y': 'Wert des Gini-Impurity-Kriteriums'})\nfig.show()\n\n\n\nIm Diagramm können wir direkt ablesen, dass bei einem nicht verkauften Auto und\nfünf verkauften Autos (p = 0.8\\bar{3}) das Gini-Impurity-Kriterium den Wert\n0.27\\bar{7} \\approx 0.278 hat.\n\nDas Gini-Impurity-Kriterium ist sehr wichtig für das Training eines\nEntscheidungsbaumes. Der Algorithmus probiert im Hintergrund verschiedene\nMöglichkeiten durch, mit Hilfe der Entscheidungsfragen den Datensatz zu\nsplitten. Zu jedem Split werden dann die zugehörigen Werte des\nGini-Impurity-Kriteriums berechnet. Dann wählt der Algorithmus den Split aus,\nder die höchste Reinheit hat (also den niedrigsten Gini-Impurity-Wert). Gilt das\nfür mehrere Splits, dann wird zufällig ein Split ausgewählt. Jedes Training kann\ndaher zu einem anderen Entscheidungsbaum führen. Ist dieses Verhalten nicht\ngewünscht, kann der optionale Parameter random_state= auf einen Integer\ngesetzt werden, um die Zufallszahlen zu fixieren. Das haben wir auch bereits im vorherigen Kapitel gemacht, damit die Ergebnisse vergleichbar waren.\n\nNeben dem Gini-Impurity-Kriterium gibt es noch weitere Bewertungsmaße, um einen\nEntscheidungsbaum zu trainieren. In Scikit-Learn sind die beiden Alternativen\nlog_loss und entropy verfügbar, die auf der Shannon-Entropie basieren\nund den Informationsgewinn durch Splits maximieren. Wir schauen uns im Folgenden\nan, wie diese ausgewählt werden können. Wer zuvor sich noch ein wenig mehr mit\nden Details von Entscheidungsbäumen beschäftigen möchte, kann sich die folgenden\nVideos ansehen.\n\nOptionales Video “Entscheidungsbäume #2 - Der ID3-Algorithmus” von The Morpheus Tutorials\n\nOptionales Video “Entscheidungsbäume #3 - Entropie und Informationsgewinn” von The Morpheus Tutorials\n\nOptionales Video “ID3 Entscheidungsbaum” von 42 Entwickler","type":"content","url":"/chapter06-sec02#was-ist-das-gini-impurity-kriterium","position":7},{"hierarchy":{"lvl1":"6.2 Entscheidungsbäume visualisieren und trainieren","lvl2":"Entscheidungsbäume trainieren"},"type":"lvl2","url":"/chapter06-sec02#entscheidungsb-ume-trainieren","position":8},{"hierarchy":{"lvl1":"6.2 Entscheidungsbäume visualisieren und trainieren","lvl2":"Entscheidungsbäume trainieren"},"content":"Der Entscheidungsbaum-Klassifikationsalgorithmus von Scikit-Learn bietet noch\nweitere Optionen an, wie in der \n\nDokumentation Scikit-Learn →\nDecisionTreeClassifier()\nnachgelesen werden kann.\n\nSowohl bei der Initialisierung des Entscheidungsbaumes können Parameter gesetzt\nwerden, als auch beim Verwenden der verschiedenen Methoden. Tatsächlich haben\nwir bereits weiter oben aus didaktischen Gründen den Parameter random_state=0\nbei der Initialisierung gesetzt, damit immer der gleiche Entscheidungsbaum\nentsteht. In einem echten Projekt würde dieser Parameter nie verwendet werden.\n\nExperimentieren Sie mit verschiedenen Werten für random_state (z.B. 0, 1, 2,\n3). Sie werden feststellen, dass sich die Struktur des Baumes ändern kann,\nobwohl die Klassifikationsgenauigkeit oft ähnlich bleibt. Testen Sie auch\nverschiedene Splitting-Kriterien (criterion='gini' vs. criterion='entropy')\nund vergleichen Sie die entstehenden Bäume.\n\nmodell = DecisionTreeClassifier(criterion='entropy', random_state=3)\nmodell.fit(X,y)\n\nplot_tree(modell, \n    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n    class_names=['nicht verkauft', 'verkauft']);\n\n\n\nDurch die Verwendung von entropy als Kriterium kann sich die Struktur des\nEntscheidungsbaums ändern. Statt gini=... wird nun entropy=... im Diagramm\nangezeigt. Die grundlegende Funktionsweise bleibt jedoch gleich: Der Algorithmus\nwählt die Splits, die die Unreinheit am stärksten reduzieren.\n\nMini-Übung\n\nGegeben sind folgende Trainingsdaten für eine Pflanzenklassifikation:pflanzen_daten = pd.DataFrame({\n    'Blattlaenge [cm]': [2.5, 4.9, 6.3, 7.1, 3.2],\n    'Blattbreite [cm]': [0.8, 1.5, 2.1, 2.5, 1.0],\n    'giftig': [False, False, True, True, False]\n})\n\nTrainieren Sie einen Entscheidungsbaum mit random_state=42.\n\nVisualisieren Sie den Baum mit aussagekräftigen Feature- und Klassennamen.\n\n# Hier Ihr Code\n\n\n\nLösungpflanzen_daten = pd.DataFrame({\n    'Blattlaenge [cm]': [2.5, 4.9, 6.3, 7.1, 3.2],\n    'Blattbreite [cm]': [0.8, 1.5, 2.1, 2.5, 1.0],\n    'giftig': [False, False, True, True, False]\n})\n\n# 1. Training\nX_pflanzen = pflanzen_daten[['Blattlaenge [cm]', 'Blattbreite [cm]']]\ny_pflanzen = pflanzen_daten['giftig']\n\nmodell_pflanzen = DecisionTreeClassifier(random_state=42)\nmodell_pflanzen.fit(X_pflanzen, y_pflanzen)\n\n# 2. Visualisierung\nplot_tree(modell_pflanzen,\n    feature_names=['Blattlänge [cm]', 'Blattbreite [cm]'],\n    class_names=['nicht giftig', 'giftig']);","type":"content","url":"/chapter06-sec02#entscheidungsb-ume-trainieren","position":9},{"hierarchy":{"lvl1":"6.2 Entscheidungsbäume visualisieren und trainieren","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter06-sec02#zusammenfassung-und-ausblick","position":10},{"hierarchy":{"lvl1":"6.2 Entscheidungsbäume visualisieren und trainieren","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Kapitel haben wir das Training von Entscheidungsbäumen mit Hilfe der\nBibliothek Scikit-Learn vertieft. Im nächsten Kapitel widmen wir uns den Vor-,\naber auch den Nachteilen von Entscheidungsbäumen.","type":"content","url":"/chapter06-sec02#zusammenfassung-und-ausblick","position":11},{"hierarchy":{"lvl1":"6.3 Entscheidungsbäume in der Praxis"},"type":"lvl1","url":"/chapter06-sec03","position":0},{"hierarchy":{"lvl1":"6.3 Entscheidungsbäume in der Praxis"},"content":"Entscheidungsbäume bieten viele Vorteile, haben aber auch Nachteile, die wir in\ndiesem Kapitel diskutieren werden. Darüber hinaus lernen wir Methoden kennen,\num bei Entscheidungsbäumen diese Nachteile zu reduzieren.","type":"content","url":"/chapter06-sec03","position":1},{"hierarchy":{"lvl1":"6.3 Entscheidungsbäume in der Praxis","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter06-sec03#lernziele","position":2},{"hierarchy":{"lvl1":"6.3 Entscheidungsbäume in der Praxis","lvl2":"Lernziele"},"content":"Lernziele\n\nSie können in eigenen Worten erklären, was Overfitting (deutsch:\nÜberanpassung) ist.\n\nSie wissen, was Underfitting bedeutet.\n\nSie wissen, dass Entscheidungsbäume eine Tendenz zu Overfitting haben und\nMaßnahmen zur Reduzierung von Overfitting ergriffen werden müssen.\n\nSie wissen, was Hyperparameter sind.\n\nSie kennen Hyperparameter der Entscheidungsbäume wie beispielsweise\n\nmaximale Baumtiefe,\n\nminimale Anzahl an Datenpunkten in Knoten oder\n\nminimale Anzahl an Datenpunkten in Blättern.\n\nSie können die Hyperparameter zum Prä-Pruning (deutsch: vorab\nZurechtschneiden) geeignet wählen.","type":"content","url":"/chapter06-sec03#lernziele","position":3},{"hierarchy":{"lvl1":"6.3 Entscheidungsbäume in der Praxis","lvl2":"Die Tendenz von Entscheidungsbäumen zum Overfitting"},"type":"lvl2","url":"/chapter06-sec03#die-tendenz-von-entscheidungsb-umen-zum-overfitting","position":4},{"hierarchy":{"lvl1":"6.3 Entscheidungsbäume in der Praxis","lvl2":"Die Tendenz von Entscheidungsbäumen zum Overfitting"},"content":"Entscheidungsbaummodelle bieten zahlreiche Vorteile. Ein wesentlicher Vorzug ist\ndie Möglichkeit, den trainierten Entscheidungsbaum zu visualisieren, wodurch es\nleicht nachvollziehbar wird, welche Merkmale einen signifikanten Einfluss haben.\nEin weiterer Vorteil ist ihre Effizienz bei heterogenen Daten; sowohl numerische\nals auch kategoriale Eigenschaften können problemlos verarbeitet werden.\nEntscheidungsbäume sind selbst bei unterschiedlichen Datenskalen robust und\nerfordern nur wenig Vorverarbeitung.\n\nTrotz dieser Stärken besitzen Entscheidungsbäume eine Neigung zum\nOverfitting. Overfitting, auch als Überanpassung bekannt, beschreibt ein\nProblem im maschinellen Lernen, bei dem ein Modell die Trainingsdaten zu genau\nlernt. Das klingt zunächst gut, aber das Modell kann dadurch seine Fähigkeit\nverlieren, Vorhersagen für neue, unbekannte Daten zu treffen. Im Gegensatz dazu\nsteht das Underfitting, das eine zu geringe Anpassung an die Daten bedeutet\nund ebenfalls unerwünscht ist.\n\nUm uns das Problem des Overfittings zu veranschaulichen, betrachten wir erneut\ndas Autohaus-Beispiel, aber diesmal mit mehr Autos. Wir lassen die Autos diesmal\nmit einer in Scikit-Learn eingebauten Funktion zur Generierung von künstlichen\nDaten erzeugen, der sogenannten make_moons-Funktion (siehe \n\nDokumentation\nScikit-Learn →\nmake_moons)\naus dem Module sklearn.datasets.\n\nfrom sklearn.datasets import make_moons \n\nX_array, y_array = make_moons(noise = 0.5, n_samples=50, random_state=3)\n\n\n\nDamit die künstlichen Daten besser zu dem Autohaus-Beispiel passen,\ntransformieren wir sie und nutzen die Pandas-Datenstrukturen, um sie effizient\nzu verwalten.\n\nimport numpy as np\nimport pandas as pd\n\n# Transformation der Merkmalswerte in einen positiven Bereich und \n# Umwandlung in eine Integer-Matrix\nX_array = X_array + 1.2 * np.abs(np.min(X_array))\nX_array[:,0] = np.ceil(X_array[:,0] * 30000)\nX_array[:,1] = np.ceil(X_array[:,1] * 10000)\nX = pd.DataFrame(X_array, columns=['Kilometerstand [km]', 'Preis [EUR]'], dtype=(int, int))\n\n# Zuweisung von True/False basierend auf den Kategorien 1 bzw. 0\ny_array = (y_array - 1.0) * (-1)\ny = pd.Series(y_array, name='verkauft', dtype='bool')\n\n\n\nNach der Datenvorbereitung visualisieren wir diese:\n\nimport plotly.express as px\n\nfig = px.scatter(x = X['Kilometerstand [km]'], y = X['Preis [EUR]'], color=y,\n    title='Künstliche Daten Autohaus',\n    labels={'x': 'Kilometerstand [km]', 'y': 'Preis [EUR]', 'color': 'verkauft'})\nfig.show()\n\n\n\nDas Training des Entscheidungsbaumes und dessen Visualisierung erledigt der\nfolgende Code.\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\nmodell = DecisionTreeClassifier(random_state=0)\nmodell.fit(X,y)\n\nplot_tree(modell,\n    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n    class_names=['nicht verkauft', 'verkauft']);\n\n\n\nDie Visualisierung offenbart zahlreiche Verzweigungen und eine schwer lesbare\nBeschriftung. Die Entscheidungsgrenzen, die im Folgenden mit\nDecisionBoundaryDisplay visualisiert werden, zeigen eine zu starke Anpassung\nan die Trainingsdaten.\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nfig = DecisionBoundaryDisplay.from_estimator(modell, X, cmap=ListedColormap(['#EF553B33', '#636EFA33']), grid_resolution=1000)\nfig.ax_.scatter(X['Kilometerstand [km]'], X['Preis [EUR]'], c=y, cmap=ListedColormap(['#EF553B', '#636EFA']))\nfig.ax_.set_title('Entscheidungsgrenzen');\n\n\n\nEs ist fraglich, ob dieser Entscheidungsbaum nicht zu genau an die\nTrainingsdaten angepasst wurde. Der dünne blaue vertikale Streifen bei ungefähr\n97000 km ist wahrscheinlich keine sinnvolle Entscheidung, sondern eher einem\nAusreißer geschuldet (dem Auto mit einem Kilometerstand von 97098 km und einem\nPreis von 28229 EUR). Der Entscheidungsbaum hat sich zu stark an die Daten\nangepasst. Es ist wahrscheinlich, dass dieser Entscheidungsbaum für Autos mit\neinem Kilometerstand von ungefähr 97000 km falsche Prognosen treffen wird. Wenn\nwir mit den gleichen Daten erneut einen Entscheidungsbaum trainieren lassen und\nden Zufallszahlengenerator mit dem Zustand random_state=1 initialisieren,\nerhalten wir ein völlig anderes Ergebnis.\n\nmodell_alternative = DecisionTreeClassifier(random_state=1)\nmodell_alternative.fit(X,y)\n\nfig = DecisionBoundaryDisplay.from_estimator(modell_alternative, X, cmap=ListedColormap(['#EF553B33', '#636EFA33']), grid_resolution=1000)\nfig.ax_.scatter(X['Kilometerstand [km]'], X['Preis [EUR]'], c=y, cmap=ListedColormap(['#EF553B', '#636EFA']))\nfig.ax_.set_title('Entscheidungsgrenzen des alternativen Modells');\n\n\n\nEine Möglichkeit, das Overfitting (Überanpassung) an die Daten zu bekämpfen, ist\ndas Zurechtschneiden (Pruning) der Entscheidungsbäume. Eine andere ist, aus\nmehreren Entscheidungbäumen einen »durchschnittlichen« Entscheidungsbaum zu\nbilden. Dieses Verfahren heißt Zufallswald (Random Forest) und wird ausführlich\nin einem eigenen Kapitel behandelt werden. In diesem Kapitel betrachten wir nur\ndas Zurechtschneiden der Entscheidungsbäume.","type":"content","url":"/chapter06-sec03#die-tendenz-von-entscheidungsb-umen-zum-overfitting","position":5},{"hierarchy":{"lvl1":"6.3 Entscheidungsbäume in der Praxis","lvl2":"Zurechtschneiden von Entscheidungsbäumen"},"type":"lvl2","url":"/chapter06-sec03#zurechtschneiden-von-entscheidungsb-umen","position":6},{"hierarchy":{"lvl1":"6.3 Entscheidungsbäume in der Praxis","lvl2":"Zurechtschneiden von Entscheidungsbäumen"},"content":"Eine effektive Strategie zur Bekämpfung des Overfittings bei Entscheidungsbäumen\nist das sogenannte Pruning, also das Beschneiden des Baumes. Pruning hilft,\ndie Komplexität des Modells zu reduzieren, indem weniger relevante\nEntscheidungszweige nach bestimmten Kriterien entfernt werden. Im Kontext\nunseres Autohaus-Beispiels würde dies bedeuten, dass Entscheidungszweige, die\nbeispielsweise aufgrund von Ausreißern entstanden sind, abgeschnitten werden.\nDies könnte beispielsweise den zuvor erwähnten dünnen blauen Streifen bei einem\nKilometerstand von ungefähr 97000 km betreffen, der wahrscheinlich durch einen\nAusreißer entstanden ist. Durch das Entfernen solcher spezifischen Anpassungen\nkann der Entscheidungsbaum besser verallgemeinern und wird robuster gegenüber\nneuen, unbekannten Daten. Das Ergebnis ist ein Modell, das eine bessere Balance\nzwischen Anpassung an die Trainingsdaten und Generalisierungsfähigkeit aufweist.\n\nFür Entscheidungsbäume gibt es prinzipiell zwei Methoden des Prunings:\nPrä-Pruning und Post-Pruning. Das Prä-Pruning findet vor dem Training\ndes Entscheidungsbaumes statt, das Post-Pruning nach dem Training. Die beiden\nwichtigsten Prä-Pruning-Maßnahmen sind\n\ndie Begrenzung der maximalen Tiefe des Baumes und\n\ndie Forderung nach einer Mindestanzahl von Datenpunkten (entweder pro Knoten\noder pro Blatt).\n\nBeim Post-Pruning werden im Nachhinein Knoten mit wenig Informationen aus dem\nEntscheidungsbaum entfernt oder es werden Knoten zusammengelegt. Scikit-Learn\nhat nur Prä-Pruning implementiert, so dass wir hier nicht weiter auf\nPost-Pruning eingehen.","type":"content","url":"/chapter06-sec03#zurechtschneiden-von-entscheidungsb-umen","position":7},{"hierarchy":{"lvl1":"6.3 Entscheidungsbäume in der Praxis","lvl3":"Prä-Pruning: Baumtiefe","lvl2":"Zurechtschneiden von Entscheidungsbäumen"},"type":"lvl3","url":"/chapter06-sec03#pr-pruning-baumtiefe","position":8},{"hierarchy":{"lvl1":"6.3 Entscheidungsbäume in der Praxis","lvl3":"Prä-Pruning: Baumtiefe","lvl2":"Zurechtschneiden von Entscheidungsbäumen"},"content":"Wir schauen uns zunächst an, wie bei Scikit-Learn-Entscheidungsbäumen die\nmaximale Tiefe festgelegt wird. Bisher haben wir das Modell ohne weitere\nParameter initialisiert (einzige Ausnahme: wir haben ggf. den\nZufallszahlengenerator aus didaktischen Gründen fixiert, damit die Ergebnisse\nvergleichbar sind). Nun verwenden wir bei der Initialisierung des\nDecisionTreeClassifiers das optionale Argument max_depth= und setzen es auf\n1.\n\nmodell_tiefe1 = DecisionTreeClassifier(random_state=0, max_depth=1)\nmodell_tiefe1.fit(X,y)\n\nplot_tree(modell_tiefe1,\n    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n    class_names=['nicht verkauft', 'verkauft']);\n\n\n\nEine Tiefe von 1 bedeutet, dass nur noch eine einzige Entscheidungsfrage\ngestellt wird. Das reicht nicht mehr, um die Autos in reine Blätter zu\nsortieren. Im linken Blatt sind 13 nicht verkaufte Autos und 24 verkaufte Autos,\nweshalb diesem Blatt die Kategorie »verkauft« zugeordnet wird. Im rechten Blatt\nsind 12 nicht verkaufte Autos und ein verkauftes Auto, so dass dieses Blatt\ninsgesamt als »nicht verkauft« gilt. Die Visualisierung der Entscheidungsgrenzen\nzeigt, um welche Autos es sich handelt.\n\nfig = DecisionBoundaryDisplay.from_estimator(modell_tiefe1, X, cmap=ListedColormap(['#EF553B33', '#636EFA33']), grid_resolution=1000)\nfig.ax_.scatter(X['Kilometerstand [km]'], X['Preis [EUR]'], c=y, cmap=ListedColormap(['#EF553B', '#636EFA']))\nfig.ax_.set_title('Entscheidungsgrenzen');\n\n\n\nInsbesondere die Visualisierung der Entscheidungsgrenzen zeigt aber auch, dass\ndieser Entscheidungsbaum nicht besonders gut die Daten erklärt. Der Score ist\nmit\n\nprint(f'Score des Entscheidungsbaumes mit Tiefe 1: {modell_tiefe1.score(X,y)}')\n\n\n\nauch nicht so gut. Daher verwenden wir nun als maximale Tiefe des Entscheidungsbaumes einen Wert von 2.\n\nmodell_tiefe2 = DecisionTreeClassifier(random_state=0, max_depth=2)\nmodell_tiefe2.fit(X,y)\n\nplot_tree(modell_tiefe2,\n    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n    class_names=['nicht verkauft', 'verkauft']);\n\nprint(f'Score des Entscheidungsbaumes mit Tiefe 2: {modell_tiefe2.score(X,y)}')\n\n\n\n\n\nMit einem Score von 0.78 ist der Entscheidungsbaum mit einer maximalen Tiefe von\n2 zwar besser als der Baum mit einer maximalen Tiefe von 1, aber deutlich\nentfernt von dem Score 1.0 bei einer Baumtiefe von 7. Die Entscheidungsgrenzen\nsehen folgendermaßen aus:\n\nfig = DecisionBoundaryDisplay.from_estimator(modell_tiefe2, X, cmap=ListedColormap(['#EF553B33', '#636EFA33']), grid_resolution=1000)\nfig.ax_.scatter(X['Kilometerstand [km]'], X['Preis [EUR]'], c=y, cmap=ListedColormap(['#EF553B', '#636EFA']))\nfig.ax_.set_title('Entscheidungsgrenzen');\n\n\n\nWas ist jetzt besser, eine maximale Tiefe von 1 oder 2? Oder doch 3 vielleicht?\nDie Einführung der maximalen Tiefe bietet den Vorteil, das Overfitting zu\nbekämpfen. Der Nachteil davon ist, dass wir jetzt einen neuen Parameter haben,\nder das Training und die Prognose des Modells bestimmt. Und für diesen Parameter\nmuss ein passender Wert eingestellt werden. Solche Parameter nennt man\nHyperparameter.\n\nWas ist ... ein Hyperparameter?\n\nEin Hyperparameter ist ein Parameter, der vor dem Training eines Modells\nfestgelegt wird und nicht aus den Daten während des Trainings gelernt wird. Die\nHyperparameter steuern den gesamten Lernprozess und haben einen wesentlichen\nEinfluss auf die Leistung des Modells.\n\nEin Score von 1.0 auf den Trainingsdaten deutet auf Overfitting hin, d.h. das\nModell hat die Daten auswendig gelernt. Ein sehr niedriger Score (z.B. 0.72)\ndeutet auf Underfitting hin, d.h. das Modell ist zu einfach. Das Ziel ist ein\nGleichgewicht: ein Score, der hoch genug ist, um die Daten gut zu beschreiben,\naber nicht 1.0, um Generalisierung zu ermöglichen. Werte zwischen 0.8 und 0.95\nsind oft ein guter Kompromiss, aber dies muss mit separaten Testdaten validiert\nwerden.\n\nKommen wir nun zu einem anderen Hyperparameter der Entscheidungsbäume, der\nMindestanzahl von Datenpunkten.","type":"content","url":"/chapter06-sec03#pr-pruning-baumtiefe","position":9},{"hierarchy":{"lvl1":"6.3 Entscheidungsbäume in der Praxis","lvl3":"Prä-Pruning: Mindestanzahl Datenpunkte","lvl2":"Zurechtschneiden von Entscheidungsbäumen"},"type":"lvl3","url":"/chapter06-sec03#pr-pruning-mindestanzahl-datenpunkte","position":10},{"hierarchy":{"lvl1":"6.3 Entscheidungsbäume in der Praxis","lvl3":"Prä-Pruning: Mindestanzahl Datenpunkte","lvl2":"Zurechtschneiden von Entscheidungsbäumen"},"content":"Genau wie der Hyperparameter zur Begrenzung der Baumtiefe wird die Mindestanzahl\nder Datenpunkte vorab bei der Initialisierung des Entscheidungsbaumes\nfestgelegt. Scikit-Learn bietet wiederum zwei Möglichkeiten, über die minimale\nAnzahl von Datenpunkten den Entscheidungsbaum zurechtzuschneiden. Zum einen kann\nfür die Knoten eine minimal erforderliche Anzahl von Datenpunkten festgelegt\nwerden, ab der es erlaubt ist, durch Entscheidungsfragen weiter zu verzweigen.\nZum anderen kann eine minimale Anzahl an Datenpunkten für jedes Blatt\nfestgelegt werden, das am Ende der Verzweigungen erreicht werden muss.\n\nWir probieren beide Möglichkeiten aus und vergleichen die Ergebnisse\nmiteinander. Die Option zur Einstellung der Mindestanzahl pro Knoten heißt\nmin_samples_split und die Option zur Einstellung des Mindestanzahl Datenpunkte\npro Blatt heißt min_samples_leaf. Beiden optionalen Argumenten kann entweder\nein Integer übergeben werden oder ein Float. Wird ein Integer übergeben, so ist\ndamit die tatsächliche minimale Anzahl an Datenpunkten gemeint. Ein Float wird\nals Bruch interpretiert und meint die relative Anzahl der Datenpunkte. Der Bruch\nwird mit der Gesamtzahl der Datenpunkte multipliziert und dann wird auf die\nnächste ganze Zahl aufgerundet.\n\nSchauen wir uns beide Varianten an. Zunächst begrenzen wir die Knoten und\nfordern, dass sich in jedem Entscheidungsknoten mindestens sechs Datenpunkte\nbefinden müssen.\n\nmodell_knotenbegrenzung = DecisionTreeClassifier(random_state=0, min_samples_split=6)\nmodell_knotenbegrenzung.fit(X,y)\n\nplot_tree(modell_knotenbegrenzung,\n    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n    class_names=['nicht verkauft', 'verkauft']);\n\nprint(f'Score des Entscheidungsbaumes mit Prä-Pruning Mindestanzahl Datenpunkte pro Knoten: {modell_knotenbegrenzung.score(X,y)}')\n\n\n\n\n\nDer Score ist 0.92. Nun fordern wir, dass in jedem Blatt mindestens sechs\nDatenpunkte verbleiben müssen.\n\nmodell_blattbegrenzung = DecisionTreeClassifier(random_state=0, min_samples_leaf=6)\nmodell_blattbegrenzung.fit(X,y)\n\nplot_tree(modell_blattbegrenzung,\n    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n    class_names=['nicht verkauft', 'verkauft']);\n\nprint(f'Score des Entscheidungsbaumes mit Prä-Pruning Mindestanzahl Datenpunkte pro Blatt: {modell_blattbegrenzung.score(X,y)}')\n\n\n\n\n\nIn diesem Fall erhalten wir einen Entscheidungsbaum mit einem Score von 0.82.\nWas jetzt die bessere Wahl ist -- Begrenzung der Baumtiefe oder Festlegung einer\nMindestanzahl von Datenpunkten Knoten/Blatt -- und vor allem welchen Wert der\nHyperparameter haben soll, ist eine zentrale Herausforderung im maschinellen\nLernen. In späteren Kapiteln werden wir systematische Methoden wie Grid Search\nund Cross-Validation kennenlernen, um die besten Hyperparameter-Werte zu finden.\n\nMini-Übung\n\nWelcher Entscheidungsbaum zeigt vermutlich die stärkste Tendenz zum Overfitting?\nStellen Sie eine Vermuting an und überprüfen Sie Ihre Vermutung durch Ausprobieren.\n\nA) DecisionTreeClassifier(max_depth=2)B) DecisionTreeClassifier(max_depth=10)C) DecisionTreeClassifier(min_samples_leaf=20)\n\n# Hier Ihr Code\n\n\n\nLösung\n\nAntwort B, denn eine große maximale Tiefe erlaubt sehr komplexe Bäume.\n\nÜberprüfung durch Code:# Die drei Modelle trainieren und Scores vergleichen\nmodell_a = DecisionTreeClassifier(max_depth=2, random_state=0)\nmodell_a.fit(X, y)\nprint(f'Score A (max_depth=2): {modell_a.score(X, y):.3f}')\n\nmodell_b = DecisionTreeClassifier(max_depth=10, random_state=0)\nmodell_b.fit(X, y)\nprint(f'Score B (max_depth=10): {modell_b.score(X, y):.3f}')\n\nmodell_c = DecisionTreeClassifier(min_samples_leaf=20, random_state=0)\nmodell_c.fit(X, y)\nprint(f'Score C (min_samples_leaf=20): {modell_c.score(X, y):.3f}')\n\n# Modell B hat vermutlich den höchsten Score (nahe 1.0) → Overfitting!\n\nVideo “How to Implement Decision Trees in Python / Scikit-Learn” von Misra Turp","type":"content","url":"/chapter06-sec03#pr-pruning-mindestanzahl-datenpunkte","position":11},{"hierarchy":{"lvl1":"6.3 Entscheidungsbäume in der Praxis","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter06-sec03#zusammenfassung-und-ausblick","position":12},{"hierarchy":{"lvl1":"6.3 Entscheidungsbäume in der Praxis","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Kapitel haben wir die Tendenz der Entscheidungsbäume zum Overfitting\ndiskutiert. Um dem Problem des Overfittings zu begegnen, bietet Scikit-Learn die\nMöglichkeit des Prä-Prunings. Durch die Begrenzung der maximalen Baumtiefe oder\ndie Festlegung einer Mindestanzahl von Datenpunkten in Knoten oder Blättern kann\nOverfitting reduziert werden. Diese zusätzlichen Parameter des\nEntscheidungsbaums werden Hyperparameter genannt und müssen angepasst werden.\nEine weitere Alternative, das Overfitting von Entscheidungsbäumen zu minimieren,\nbieten die Random Forests, die wir in einem späteren Kapitel kennenlernen\nwerden.","type":"content","url":"/chapter06-sec03#zusammenfassung-und-ausblick","position":13},{"hierarchy":{"lvl1":"Übung"},"type":"lvl1","url":"/chapter06-sec04","position":0},{"hierarchy":{"lvl1":"Übung"},"content":"","type":"content","url":"/chapter06-sec04","position":1},{"hierarchy":{"lvl1":"Übung","lvl2":"Aufgabe 6.1"},"type":"lvl2","url":"/chapter06-sec04#aufgabe-6-1","position":2},{"hierarchy":{"lvl1":"Übung","lvl2":"Aufgabe 6.1"},"content":"Das Schiff Titanic galt bei seiner Fertigstellung als unsinkbar. 1912\nkollidierte die Titanic mit einem Eisberg und sank. Bei dem Unglück kamen 1514\nvon 2220 Personen ums Leben, so dass der Titanic-Untergang zu den größten\nUnglücken der Schifffahrt zählt. Mehr Informationen zu der Titanic finden Sie\nbei Wikipedia\n\n\nTitanic (Schiff).\n\nIn der folgenden Übung werden Passagierlisten der Titanic benutzt, um die\nÜberlebenswahrscheinlichkeit zu prognostizieren (0 = gestorben, 1 = überlebt),\nderen Quelle hier ist:\n\n\nhttps://​www​.kaggle​.com​/c​/titanic.\n\nLaden sie den Datensatz ‘titanic_DE_cleaned.csv’.","type":"content","url":"/chapter06-sec04#aufgabe-6-1","position":3},{"hierarchy":{"lvl1":"Übung","lvl3":"EDA Titanic","lvl2":"Aufgabe 6.1"},"type":"lvl3","url":"/chapter06-sec04#eda-titanic","position":4},{"hierarchy":{"lvl1":"Übung","lvl3":"EDA Titanic","lvl2":"Aufgabe 6.1"},"content":"Führen Sie eine explorative Datenanalyse (EDA) durch, indem  Sie Python-Code in\nCode-Zellen ausführen und schreiben Sie in Markdown-Zellen Ihre Antworten.\n\nÜberblick über die Daten\n\nWelche Daten enthält der Datensatz? Wie viele Personen sind in der Tabelle\nenthalten? Wie viele Merkmale werden dort beschrieben? Sind die Daten\nvollständig?\n\nLösungimport pandas as pd \ndaten = pd.read_csv('titanic_DE_cleaned.csv')\n\ndaten.info()\n\nDer Datensatz enthält 183 Einträge, also 183 Personen. Es gibt 11 Merkmale. Die\nDaten sind vollständig. Für jedes Merkmal werden 183 non-null Einträge\nangezeigt.\n\nDatentypen\n\nWelchen Datentyp haben die Merkmale? Welche Merkmale sind numerisch und welche\nsind kategorial?\n\nLösung\n\nDie Merkmale ueberlebt, Klasse, Anzahl_Geschwister_Partner, Anzahl_Eltern_Kinder sind Integer. Die Merkmale Alter und Ticketpreis sind Floats. Die Merkmale Name, Geschlecht, Ticket, Kabine und Einstiegshafen sind Objekte. Mit .head() schauen wir uns die ersten fünf Zeilen an:daten.head()\n\nName, Tickets und Kabine sind Strings. Geschlecht und Einstiegshafen sind zwar vom Datentyp her Strings, könnten aber hier auch für Klassen stehen.\n\nStatistik und Ausreißer\n\nErstellen Sie eine Übersicht der statistischen Merkmale für die numerischen\nDaten. Visualisieren Sie anschließend die statistischen Merkmale mit Boxplots.\nInterpretieren Sie die statistischen Merkmale. Gibt es Ausreißer? Sind die Werte\nplausibel?\n\nLösungdaten.describe()\n\nDie statistischen Daten zu 'ueberlebt' sind unplausibel. Auch wenn hier Integer verwendet wurden, um überlebt/nicht überlebt zu klassifizieren, sind es eigentlich Klassen und sollten daher nicht statistisch ausgewertet werden.\n\nAuf der Titanic gab es drei Preisklassen von 1 bis 3. Minimum und Maximum sind plausibel, aber dass 75 % der Passagiere in Klasse 1 (der teuersten Klasse) mitgereist sind, erscheint unwahrscheinlich.\n\nBeim Alter fällt auf, dass das minimale Alter 0.92 ist. Da Jahre normalerweise als ganze Zahlen angegeben werden, ist das ungewöhnlich, aber nicht unplausibel. Die älteste Person war 80 Jahre alt. Der Durchschnitt lag bei 35.6 Jahren und der Median bei 36. 75 % der Passagiere waren jünger als 47.5 Jahre. Es erscheint plausibel, dass vor allem jüngere Passagiere die Strapazen der Schifffahrt auf sich genommen haben.\n\n50 % der Passagiere reisten alleine, nur sehr wenige in Familien.\n\nOffensichtlich wurden Passagiere auch kostenlos mitgenommen, denn der minimale Ticketpreis ist 0. Das Maximum verwundert, vielleicht eine Umrechnung der Währungen, denn normalerweise werden nur 2 Nachkommastellen angegeben.import plotly.express as px \n\nkastendiagramm = px.box(daten[['Klasse', 'Alter', 'Anzahl_Geschwister_Partner', 'Anzahl_Eltern_Kinder', 'Ticketpreis']],\n                       labels={'value': 'Wert', 'variable': 'Merkmal'},\n                       title='Boxplot der numerischen Werte des Titanic-Datensatzes')\nkastendiagramm.show()\n\nBeim Ticketpreis gibt es deutliche Ausreißer, bei den anderen Merkmalen gibt es vereinzelte Ausreißer.\n\nAnalyse der kategorialen Daten\n\nUntersuchen Sie die kategorialen Daten. Sind es wirklich kategoriale Daten?\nPrüfen Sie für jedes kategoriale Merkmal die Einzigartigkeit der auftretenden\nWerte und erstellen Sie ein Balkendiagramm mit den Häufigkeiten.\n\nLösungmerkmale = ['Name', 'Geschlecht', 'Ticket', 'Kabine', 'Einstiegshafen']\nfor m in merkmale:\n    einzigartige_eintraege = daten[m].unique()\n    anzahl = len(einzigartige_eintraege)\n    print(f'Merkmal {m} hat {anzahl} einzigartige Einträge.')\n\nBeim Merkmal Geschlecht gibt es nur zwei verschiedene Einträge, beim Einstiegshafen nur drei verschiedene Einstiegshäfen. Das sind (ungeordnete) kategoriale Daten. Die anderen Merkmale sind zu verschieden und sind damit nicht mehr als kategoriale Daten einzustufen. Es werden daher die Balkendiagramme mit den Häufigkeiten nur für die beiden Merkmale Geschlecht und Einstiegshafen erstellt.geschlecht = daten['Geschlecht']\n\nbalkendiagramm_geschlecht = px.bar(geschlecht.value_counts(),\n                                  labels={'value': 'Anzahl', 'variable': 'Geschlecht'},\n                                  title='Häufigkeit Geschlecht')\nbalkendiagramm_geschlecht.show()hafen = daten['Einstiegshafen']\n\nbalkendiagramm_hafen = px.bar(hafen.value_counts(),\n                                  labels={'value': 'Anzahl', 'variable': 'Hafen'},\n                                  title='Häufigkeit Einstiegshafen')\nbalkendiagramm_hafen.show()","type":"content","url":"/chapter06-sec04#eda-titanic","position":5},{"hierarchy":{"lvl1":"Übung","lvl3":"ML-Modell Titanic","lvl2":"Aufgabe 6.1"},"type":"lvl3","url":"/chapter06-sec04#ml-modell-titanic","position":6},{"hierarchy":{"lvl1":"Übung","lvl3":"ML-Modell Titanic","lvl2":"Aufgabe 6.1"},"content":"Entscheidungsbaum\n\nTrainieren Sie mit den numerischen Merkmalen einen Entscheidungsbaum/Decision\nTree. Visualisieren Sie den Entscheidungsbaum.\n\nLösungX = daten[['Klasse', 'Alter', 'Anzahl_Geschwister_Partner', 'Anzahl_Eltern_Kinder','Ticketpreis']]\ny = daten['ueberlebt']\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nentscheidungsbaum =  DecisionTreeClassifier()\nentscheidungsbaum.fit(X,y)\nentscheidungsbaum.score(X,y)\nscore = entscheidungsbaum.score(X,y)\nprint(f'Score: {score:.2f}')from sklearn.tree import plot_tree\n\nplot_tree(entscheidungsbaum);\n\nHyperparameter-Tuning und Interpretation\n\nSpielen Sie mit den Hyperparametern des Entscheidungsbaumes/Decision Trees.\nBegrenzen Sie die Baumtiefe auf 2, 3 und 4. Was sind die wichtigsten Merkmale,\ndie ein Überleben der Passagiere gesichert haben?\n\nLösungfor baumtiefe in [2, 3, 4]:\n    baum = DecisionTreeClassifier(max_depth=baumtiefe)\n    baum.fit(X,y)\n    score = baum.score(X,y)\n    print(f'Score für eine Baumtiefe von {baumtiefe}: {score: .2f}')\n\nTatsächlich ist der Entscheidungsbaum/Decision Tree mit einer Baumtiefe von 3\nund 4 kaum besser als der mit einer Baumtiefe von 2. Wir werten daher den\nEntscheidungsbaum mit einer Baumtiefe von 2 aus:finales_modell = DecisionTreeClassifier(max_depth=2)\nfinales_modell.fit(X,y)\n\nplot_tree(finales_modell,\n    feature_names=['Klasse', 'Alter', 'Anzahl_Geschwister_Partner', 'Anzahl_Eltern_Kinder','Ticketpreis'],\n    class_names=['nicht ueberlebt', 'ueberlebt']);\n\nZunächst einmal scheint ein jüngeres Alter die Überlebenschance erhöht zu haben. Danach wirkt es so, also ob der Ticketpreis eine wichtige Rolle gespielt haben könnte.","type":"content","url":"/chapter06-sec04#ml-modell-titanic","position":7},{"hierarchy":{"lvl1":"Übung","lvl2":"Aufgabe 6.2"},"type":"lvl2","url":"/chapter06-sec04#aufgabe-6-2","position":8},{"hierarchy":{"lvl1":"Übung","lvl2":"Aufgabe 6.2"},"content":"Der Datensatz ‘diabetes.csv’ ist eine Sammlung von medizinischen Daten, die vom\nNational Institute of Diabetes and Digestive and Kidney Diseases, erhoben\nwurden, siehe\n\n\nhttps://​www​.kaggle​.com​/datasets​/whenamancodes​/predict​-diabities​?resource​=​download.\nBei Frauen des Pima-Stammes wurden folgende medizinische Daten erhoben:\n\nPregnancies: Anzahl der Schwangerschaften\n\nGlucose: Glukose-Level im Blut\n\nBloodPressure: Messung des Blutdrucks\n\nSkinThickness: Dicke der Haut\n\nInsulin: Messung des Insulinspiegels im Blut\n\nBMI: Body-Maß-Index (Gewicht geteilt durch Körpergröße ins Quadrat)\n\nDiabetesPedigreeFunction: Wahrscheinlichkeit von Diabetes aufgrund der Familienhistorie\n\nAge: Alter\n\nEnthalten ist auch, ob bei der Person Diabetes festgestellt wurde oder nicht.\n\nOutcome: Diabetes = 1, kein Diabetes = 0","type":"content","url":"/chapter06-sec04#aufgabe-6-2","position":9},{"hierarchy":{"lvl1":"Übung","lvl3":"EDA Diabetes","lvl2":"Aufgabe 6.2"},"type":"lvl3","url":"/chapter06-sec04#eda-diabetes","position":10},{"hierarchy":{"lvl1":"Übung","lvl3":"EDA Diabetes","lvl2":"Aufgabe 6.2"},"content":"Führen Sie eine explorative Datenanalyse (EDA) durch, indem  Sie Python-Code in\nCode-Zellen ausführen und schreiben Sie in Markdown-Zellen Ihre Antworten.\n\nÜberblick\n\nWelche Daten enthält der Datensatz? Wie viele Personen sind in der Tabelle\nenthalten? Wie viele Merkmale werden dort beschrieben? Sind die Daten\nvollständig?\n\nLösungimport pandas as pd \n\ndaten = pd.read_csv('diabetes.csv')\ndaten.info()\n\nDer Datensatz enthält 768 Personen mit insgesamt 9 Merkmalen. Aufgelistet sind die Merkmale Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age und Outcome.\n\nDie Daten sind für jedes Merkmal vollständig. In jeder Spalte gibt es 768 non-null Einträge.\n\nDatentyp\n\nWelchen Datentyp haben die Merkmale? Welche Merkmale sind numerisch und welche\nsind kategorial?\n\nLösung\n\nPregnancies, Glucose, BloodPressure, SkinThickness, Insulin, Age und Outcome sind Integer. BMI und DiabetesPedigreeFunction sind Floats. Kein Merkmal wird als object eingestuft.\n\nStatistik\n\nErstellen Sie eine Übersicht der statistischen Merkmale für die numerischen\nDaten. Visualisieren Sie anschließend die statistischen Merkmale mit Boxplots.\nInterpretieren Sie die statistischen Merkmale. Gibt es Ausreißer? Sind die Werte\nplausibel?\n\nLösungdaten.describe()\n\nDie Pregnancies reichen von 0 Schwangerschaften bis hin zu 17 Schwangerschaften,\nwas ungewöhnlich hoch ist. Der Mittelwert von 3.85 Schwangerschaften erscheint\nplausibel, 50 % der Frauen waren maximal dreimal schwanger.\n\nDer Glucose-Wert reicht von 0 bis 199. An der Stelle müsste mit einem Mediziner\nRücksprache gehalten werden, ob ein Glucose-Wert von 0 plausibel ist.\n\nBeim BloodPressure, also den Blutdruck, ist der minimale Wert von 0 jedoch\nunplausibel. Eine Person mit einem Blutdruck von 0 ist tot. Diese Null-Werte sind wahrscheinlich fehlende Messwerte, die fälschlicherweise als 0 kodiert wurden. In einer vollständigen Datenanalyse müssten diese Werte entweder durch sinnvolle Methoden ersetzt oder die betroffenen Zeilen entfernt werden.\n\nSkinThickness kann erneut nur von Medizinern korrekt eingeordnet werden. Ob eine\nminimale SkinThickness von 0 und eine maximale SkinThickness von 99 sinnvolle\nWerte darstellen, können Nichtmediziner nicht sinnvoll beurteilen.\n\nAuch der minimale Insulin-Wert von 0 sowie der Body-Maß-Index BMI wirken seltsam.\nEin BMI von 0 kann nicht sein, denn beim BMI wird das Gewicht einer Person durch\ndie quadrierte Körpergröße geteilt. Ein BMI von 0 bedeutet ein Gewicht von 0,\nwas nicht sein kann. Wahrscheinlich wurden hier wiederum Werte nicht erfasst.\n\nDie DiabetesPedigree-Funktion können wir ohne medizinisches Fachwissen nicht\nbewerten.\n\nBeim Alter fällt auf, dass keine Kinder dabei waren. Die jüngste Person ist 21,\ndas mittlere Alter liegt bei 33 Jahren und 75 % aller Personen sind jünger als\n41.\n\nDas Outcome darf nicht einfach statistisch interpretiert werden. Das Outcome\ngibt an, ob eine Person Diabetes hat (1) oder nicht (0). Auch wenn hier Zahlen\nverwendet wurden, ist das Outcome eigentlich ein kategoriales Merkmal. Es ist\nsogar die kategoriale Zielgröße unserer Problemstellung.\n\nUntersuchung Ursache - Wirkung\n\nErstellen Sie eine Scatter-Matrix mit Insulin, BMI und Outcome. Welche der\nbeiden Eigeschaften Insulin oder BMI könnte ehr geeignet sein, Diabetes ja/nein\nzu prognostizieren?\n\nVisualisieren Sie Diabetes ja/nein in Abhängigkeit der gewählten Eigenschaft.\nVermuten Sie einen Zusammenhang?\n\nLösungimport plotly.express as px \n\nauswahl = ['Insulin', 'BMI', 'Outcome']\nfig = px.scatter_matrix(daten[auswahl],\n    title='Wirkung Insulin und BMI auf Diabetes')\nfig.show()\n\nBeim Insulin kann man kaum eine Wirkung des Insulins auf den Diabeteszustand\nerkennen. Beim BMI kann man erkennen, dass ein höherer BMI scheinbar mehr zu\nDiabetes führt als ein niedriger BMI.fig = px.scatter(daten, x = 'BMI', y = 'Outcome',\n    title='Wirkung BMI auf Diabetes')\nfig.show()\n\nEs ist kaum ein Zusammenhang erkennbar außer der Feststellung, dass höherer BMI\nanscheinend häufiger mit Diabetes korreliert ist als niedriger BMI.","type":"content","url":"/chapter06-sec04#eda-diabetes","position":11},{"hierarchy":{"lvl1":"Übung","lvl3":"ML-Modell Diabetes","lvl2":"Aufgabe 6.2"},"type":"lvl3","url":"/chapter06-sec04#ml-modell-diabetes","position":12},{"hierarchy":{"lvl1":"Übung","lvl3":"ML-Modell Diabetes","lvl2":"Aufgabe 6.2"},"content":"Entscheidungsbaum\n\nTrainieren Sie mit den numerischen Merkmalen einen Entscheidungsbaum/Decision\nTree. Visualisieren Sie den Entscheidungsbaum.\n\nLösungX = daten.loc[:, 'Pregnancies' : 'Age']\ny = daten['Outcome']from sklearn.tree import DecisionTreeClassifier\n\nmodell = DecisionTreeClassifier()\nmodell.fit(X,y)\n\nscore = modell.score(X,y)\nprint(f'Score: {score:.2f}')from sklearn.tree import plot_tree\n\nplot_tree(modell);\n\nHyperparameter und Interpretation\n\nSpielen Sie mit den Hyperparametern des Entscheidungsbaumes/Decision Trees.\nBegrenzen Sie die Baumtiefe auf 2, 3 und 4. Was sind die wichtigsten Merkmale,\ndie Diabetes auslösen können?\n\nLösungfor baumtiefe in [2, 3, 4]:\n    baum = DecisionTreeClassifier(max_depth=baumtiefe)\n    baum.fit(X,y)\n    score = baum.score(X,y)\n    print(f'Score für eine Baumtiefe von {baumtiefe}: {score: .2f}')\n\nWieder gibt es kaum Unterschiede im Score für die verschiedenen Baumtiefen. Wir\nbetrachten nun ein Modell der Baumtiefe 2.finales_modell = DecisionTreeClassifier(max_depth=2)\nfinales_modell.fit(X,y)\n\nplot_tree(finales_modell,\n    feature_names=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age'],\n    class_names=['kein Diabetes', 'Diabetes']);\n\nAls erstes Entscheidungskriterium wird der Glucose-Wert benutzt. Je nachdem, ob\nder Glucose-Wert kleiner 127.5 ist oder nicht, wird danach das Alter (jünger als\n28.5 bedeutet dann kein Diabetes) oder der BMI (kleiner als 29.95 kein Diabetes)\nals Entscheidungsmerkmal verwendet.","type":"content","url":"/chapter06-sec04#ml-modell-diabetes","position":13},{"hierarchy":{"lvl1":"7.1 Einfache lineare Regression"},"type":"lvl1","url":"/chapter07-sec01","position":0},{"hierarchy":{"lvl1":"7.1 Einfache lineare Regression"},"content":"Die lineare Regression gehört zu den überwachten maschinellen Lernverfahren\n(Supervised Learning). Meist ist sie das erste ML-Modell, das eingesetzt wird,\num Regressionsprobleme zu lösen. In diesem Kapitel führen wir in das Konzept\nund die Umsetzung der einfachen linearen Regression mit Scikit-Learn ein.","type":"content","url":"/chapter07-sec01","position":1},{"hierarchy":{"lvl1":"7.1 Einfache lineare Regression","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter07-sec01#lernziele","position":2},{"hierarchy":{"lvl1":"7.1 Einfache lineare Regression","lvl2":"Lernziele"},"content":"Lernziele\n\nSie kennen das lineare Regressionsmodell.\n\nSie können erklären, was die Fehlerquadratsumme ist.\n\nSie wissen, dass das Training des lineare Regressionsmodells durch die\nMinimierung der Fehlerquadratsumme (Kleinste-Quadrate-Schätzer) erfolgt.\n\nSie können mit Scikit-Learn ein lineares Regressionsmodell trainieren.\n\nSie können mit einem trainierten linearen Regressionsmodell Prognosen treffen.\n\nSie können mit dem Bestimmtheitsmaß bzw. R²-Score beurteilen, ob das\nlineare Regressionsmodell geeignet zur Erklärung der Daten ist.","type":"content","url":"/chapter07-sec01#lernziele","position":3},{"hierarchy":{"lvl1":"7.1 Einfache lineare Regression","lvl2":"Regression kommt aus der Statistik"},"type":"lvl2","url":"/chapter07-sec01#regression-kommt-aus-der-statistik","position":4},{"hierarchy":{"lvl1":"7.1 Einfache lineare Regression","lvl2":"Regression kommt aus der Statistik"},"content":"In der Statistik beschäftigen sich Mathematikerinnen und Mathematiker bereits\nseit Jahrhunderten damit, Analyseverfahren zu entwickeln, mit denen\nexperimentelle Daten gut erklärt werden können. Falls wir eine \"erklärende”\nVariable haben und wir versuchen, die Abhängigkeit einer Messgröße von der\nerklärenden Variable zu beschreiben, nennen wir das Regressionsanalyse oder kurz\nRegression. Bei vielen Problemen suchen wir nach einem linearen Zusammenhang\nund sprechen daher von linearer Regression. Mehr Details finden wir auch bei\n\n\nWikipedia → Regressionsanalyse.\n\nEtwas präziser formuliert ist lineare Regression ein Verfahren, bei dem es eine\nEinflussgröße x und eine Zielgröße y gibt. In der ML-Sprechweise wird die\nEinflussgröße x typischerweise als Merkmal (oder englisch Input oder\nFeature) bezeichnet. Die Zielgröße (manchmal auch Output oder\nTarget genannt) soll stetig sein (manchmal auch kontinuierlich, metrisch\noder quantitativ genannt). Für das Merkmal (oder die Merkmale) liegen M\nDatenpunkte mit den dazugehörigen Werten der Zielgröße vor. Diese werden\nüblicherweise als Paare (wenn nur ein Merkmal vorliegt) zusammengefasst:(x^{(1)},y^{(1)}), \\, (x^{(2)},y^{(2)}), \\, \\ldots, \\, (x^{(M)},y^{(M)}).\n\nZiel der linearen Regression ist es, zwei Parameter w_0 und w_1 so zu\nbestimmen, so dass die lineare Gleichungy^{(i)} \\approx w_0 + w_1 x^{(i)}\n\nmöglichst für alle Datenpunkte (x^{(i)}, y^{(i)}) gilt. Geometrisch\nausgedrückt: durch die Daten soll eine Gerade gelegt werden, wie die folgende\nAbbildung zeigt. Die Datenpunkte sind blau, die Regressionsgerade rot.\n\n\n\nFigure 1:Lineare Regression: die erklärende Variable (= Input oder unabhängige Variable\noder Ursache) ist auf der x-Achse, die abhängige Variable (= Output oder\nWirkung) ist auf der y-Achse aufgetragen, Paare von Messungen sind in blau\ngekennzeichnet, das Modell in rot.\n(Quelle:\n\nWikimedia\nvon Sewaqu. Lizenz: Public domain))\n\nIn der Praxis werden die Daten nicht perfekt auf der Geraden liegen. Die Fehler\nzwischen dem echten y^{(i)} und dem Funktionswert der Gerade f(x^{(i)}) =\nw_0 + w_1 x^{(i)} werden unterschiedlich groß sein, je nachdem, welche Parameter\nw_0 und w_1 gewählt werden. Wie finden wir jetzt die beste Kombination w_0\nund w_1, so dass diese Fehler möglichst klein sind?","type":"content","url":"/chapter07-sec01#regression-kommt-aus-der-statistik","position":5},{"hierarchy":{"lvl1":"7.1 Einfache lineare Regression","lvl2":"Wie groß ist der Fehler?"},"type":"lvl2","url":"/chapter07-sec01#wie-gro-ist-der-fehler","position":6},{"hierarchy":{"lvl1":"7.1 Einfache lineare Regression","lvl2":"Wie groß ist der Fehler?"},"content":"Das Prinzip für das lineare Regressionsmodell und auch die folgenden ML-Modelle\nist jedesmal gleich. Das Modell ist eine mathematische Funktion, die aber noch\nParameter (hier beispielsweise die Koeffizienten der Gerade) enthält. Dann wird\nfestgelegt, was eine gute Prognose ist, also wie Fehler berechnet und beurteilt\nwerden sollen. Das hängt jeweils von dem betrachteten Problem ab. Sobald das\nsogenannte Fehlermaß feststeht, werden die Parameter der Modellfunktion so\nberechnet, dass das Fehlermaß (z.B. Summe der Fehler oder Mittelwert der Fehler)\nmöglichst klein wird. In der Mathematik sagt man dazu Minimierungsproblem.\n\nFür die lineare Regression wird als Fehlermaß die Kleinste-Quadrate-Schätzung\nverwendet (siehe \n\nWikipedia  → Methode der kleinsten\nQuadrate). Dazu\nberechnen wir, wie weit weg die Gerade von den Messpunkten ist. Wie das geht,\nveranschaulichen wir uns mit der folgenden Grafik.\n\n\n\nFigure 2:Messpunkte (blau) und der Abstand (grün) zu einer Modellfunktion (rot)\n(\n\nQuelle: Autor: Christian Schirm, Lizenz: CC0)\n\nUnsere rote Modellfunktion trifft die Messpunkte mal mehr und mal weniger gut.\nWir können jetzt für jeden Messpunkt berechnen, wie weit die rote Kurve von ihm\nweg ist (= grüne Strecke), indem wir die Differenz der y-Koordinaten errechnen:\nr = y_{\\text{blau}}-y_{\\text{rot}}. Diese Differenz nennt man Residuum.\nDanach summieren wir die Fehler (also die Residuen) auf und erhalten den\nGesamtfehler. Dabei kann es passieren, dass am Ende als Gesamtfehler 0\nherauskommt, weil beispielsweise für den 1. Messpunkt die blaue y-Koordinate\nunter der roten y-Koordinate liegt und damit ein negatives Residuum herauskommt,\naber für den 5. Messpunkt ein positives Residuum. Daher quadrieren wir die\nResiduen, was noch weitere Vorteile bietet (Differenzierbarkeit, eindeutiges\nMinimum). Dann wird diese Fehlerquadratsumme minimiert, um die Koeffizienten\ndes Regressionsmodells zu berechnen.","type":"content","url":"/chapter07-sec01#wie-gro-ist-der-fehler","position":7},{"hierarchy":{"lvl1":"7.1 Einfache lineare Regression","lvl2":"Einfache lineare Regression mit Scikit-Learn"},"type":"lvl2","url":"/chapter07-sec01#einfache-lineare-regression-mit-scikit-learn","position":8},{"hierarchy":{"lvl1":"7.1 Einfache lineare Regression","lvl2":"Einfache lineare Regression mit Scikit-Learn"},"content":"Nach diesem theoretischen Exkurs möchten wir Scikit-Learn nutzen, um eine\neinfache lineare Regression durchzuführen. Aus didaktischen Gründen erzeugen wir\nuns dazu künstliche Daten mit der Funktion make_regression des Moduls\nsklearn.datasets. Wir transformieren die zufällig erzeugten Zahlen und packen\nsie in ein Pandas-DataFrame mit den Merkmalen »Leistung [PS]« eines Autos und\ndem »Preis [EUR]« eines Autos.\n\nimport numpy as np \nimport pandas as pd \nfrom sklearn.datasets import make_regression\n\nX_array, y_array = make_regression(n_samples=100, n_features=1, noise=10, random_state=0)\n\ndaten = pd.DataFrame({\n    'Leistung [PS]': np.floor(50*(X_array[:,0] + 3)),\n    'Preis [EUR]': 100*(y_array+150)\n    })\n\n\n\nMehr Details zu der Funktion make_regression gibt es in der \n\nDokumentation\nScikit-Learn →\nmake_regression.\nWir visualisieren nun den Preis in Abhängigkeit von der Leistung des Autos.\n\nimport plotly.express as px \n\nfig = px.scatter(daten, x = 'Leistung [PS]', y = 'Preis [EUR]',\n    title='Künstliche Daten: Verkaufspreise Autos')\nfig.show()\n\n\n\nEs drängt sich die Vermutung auf, dass der Preis eines Autos linear von der\nLeistung abhängt. Je mehr PS, desto teurer das Auto.\n\nAls nächstes trainieren wir ein lineares Regressionsmodell auf den Daten.\nLineare ML-Modelle fasst Scikit-Learn in einem Untermodul namens linear_model\nzusammen. Um also das lineare Regressionsmodell LinearRegression verwenden zu\nkönnen, müssen wir es folgendermaßen importieren und initialisieren:\n\nfrom sklearn.linear_model import LinearRegression\n\nmodell = LinearRegression()\n\n\n\nMit der Methode .fit() werden die Parameter des linearen Regressionsmodells an\ndie Daten angepasst. Dazu müssen die Daten in einem bestimmten Format vorliegen.\nBei den Inputs wird davon ausgegangen, dass mehrere Merkmale in das Modell\neingehen sollen. Die Merkmale stehen normalerweise in den Spalten des\nDatensatzes. Beim Output erwarten wir zunächst nur ein Merkmal, das durch das\nModell erklärt werden soll. Daher geht Scikit-Learn davon aus, dass der Input\neine Tabelle (Matrix) X ist, die M Zeilen und N Spalten hat. M ist die Anzahl\nan Datenpunkten, hier also die Anzahl der Autos, und N ist die Anzahl der\nMerkmale, die betrachtet werden sollen. Da wir momentan nur die Abhängigkeit des\nPreises von der PS-Zahl analysieren wollen, ist N=1. Beim Output geht\nScikit-Learn davon aus, dass eine Datenreihe (eindimensionaler Spaltenvektor)\nvorliegt, die natürlich ebenfalls M Zeilen hat. Wir müssen daher unsere\nPS-Zahlen noch in das Matrix-Format bringen. Dazu verwenden wir die Tatsache,\ndass mit [ [list] ] eine Tabelle extrahiert wird.\n\n# Adaption der Daten\nX = daten[['Leistung [PS]']]\ny = daten['Preis [EUR]']\n\n\n\nDanach können wir das lineare Regressionsmodell trainieren.\n\nmodell.fit(X,y)\n\n\n\nEs erfolgt keine Ausgabe, aber jetzt ist das lineare Regressionsmodell\ntrainiert. Die durch das Training bestimmten Parameter des Modells sind im\nModell selbst abgespeichert. Bei dem linearen Regressionsmodell sind das die\nbeiden Parameter w_0 und w_1, also Steigung .coef_ und den\ny-Achsenabschnitt .intercept_.\n\nprint(f'Steigung: {modell.coef_[0]}')\nprint(f'y-Achsenabschnitt: {modell.intercept_}')\n\n\n\nDamit lautet das (gerundete) lineare Regressionsmodell, um aus der PS-Zahl eines\nAutos x den Verkaufspreis y zu berechnen, folgendermaßen:y = 85.2 \\cdot x + 2179.","type":"content","url":"/chapter07-sec01#einfache-lineare-regression-mit-scikit-learn","position":9},{"hierarchy":{"lvl1":"7.1 Einfache lineare Regression","lvl2":"Prognosen treffen"},"type":"lvl2","url":"/chapter07-sec01#prognosen-treffen","position":10},{"hierarchy":{"lvl1":"7.1 Einfache lineare Regression","lvl2":"Prognosen treffen"},"content":"Wenn wir die Parameter des trainierten Modells ausgeben lassen bzw. die lineare\nFunktion y = 85.2 \\cdot x + 2179 verwenden, können wir mit dem linearen Modell\nPrognosen treffen. Den Umweg über das Ausgeben der trainierten Parameter und dem\nBasteln einer linearen Funktion können wir uns aber sparen, denn Scikit-Learn\nstellt für Prognosen mit dem trainierten Modell direkt eine Methode zur\nVerfügung. Mit Hilfe der predict()-Methode können für jedes Scikit-ML-Modell\nPrognosen getroffen werden.\n\nWir möchten uns den kompletten Bereich zwischen 20 PS und 270 PS ansehen und\nerzeugen daher 100 Punkte in diesem Bereich. Diese transformieren wir in ein\nPandas-DataFrame und verwenden dann die predict()-Methode.\n\ntestdaten = pd.DataFrame({\n    'Leistung [PS]': np.linspace(20, 270, 100)\n    })\nprognose = modell.predict(testdaten[['Leistung [PS]']])\n\n\n\nDiese Prognose wird dann zusammen mit den Verkaufsdaten in einem Diagramm\nvisualisiert. Dazu generieren wir zuerst den Scatter-Plot mit den Verkaufsdaten\nund fügen dann mit der Funktion add_scatter() einen zweiten Scatter-Plot zu\ndem ersten hinzu. In diesem Scatter-Plot sollen die Punkte jedoch durch eine\nLinie verbunden werden, weshalb wir die Option mode='lines' nutzen. Zusätzlich\nkennzeichnen wir die Regressionsgerade noch mit dem Namen name='Prognose'.\n\nfig = px.scatter(daten, x = 'Leistung [PS]', y = 'Preis [EUR]',\n    title='Verkaufspreise von Autos')\nfig.add_scatter(x = testdaten['Leistung [PS]'], y = prognose, mode='lines', name='Prognose')\nfig.show()\n\n\n\nDer visuelle Eindruck ist gut, aber ist diese Regressionsgerade wirklich das\nbeste Modell? Im nächsten Abschnitt sehen wir uns ein statistisches Bewertungsmaß\nan, um die Güte des Modells zu beurteilen.","type":"content","url":"/chapter07-sec01#prognosen-treffen","position":11},{"hierarchy":{"lvl1":"7.1 Einfache lineare Regression","lvl2":"Ist das beste Modell gut genug? Der R²-Score"},"type":"lvl2","url":"/chapter07-sec01#ist-das-beste-modell-gut-genug-der-r-score","position":12},{"hierarchy":{"lvl1":"7.1 Einfache lineare Regression","lvl2":"Ist das beste Modell gut genug? Der R²-Score"},"content":"Auch wenn wir mit der Minimierung der Fehlerquadratsumme bzw. der\nKleinsten-Quadrate-Methode die besten Parameter für unsere Modellfunktion\ngefunden haben, heißt das noch lange nicht, dass unser Modell gut ist. Bereits\ndie Modellfunktion kann ja völlig falsch gewählt sein. Beispielsweise könnten\nwir Messungen rund um eine sinus-förmige Wechselspannung vornehmen und dann wäre\nein lineares Regressionsmodell völlig ungeeignet, auch wenn die\nFehlerquadratsumme minimal wäre.\n\nWir brauchen daher noch ein Kriterium dafür, ob das trainierte Modell auch\nvalide ist. Für die lineare Regression nehmen wir das Bestimmtheitsmaß, das\nin der ML-Community auch R²-Score genannt wird. Der R²-Score wird dabei\nfolgendermaßen interpretiert:\n\nWenn R^2 = 1  ist, dann gibt es den perfekten linearen Zusammenhang und die\nModellfunktion ist eine sehr gute Anpassung an die Messdaten.\n\nWenn R^2 = 0 oder gar negativ ist, dann funktioniert die lineare\nModellfunktion überhaupt nicht. Dann ist das Modell schlechter als der einfache\nMittelwert.\n\nAuf der Seite \n\nhttps://mathweb.de gibt es eine Reihe von\nAufgaben und interaktiven Demonstrationen rund um die Mathematik. Insbesondere\ngibt es dort auch eine interaktive Demonstration des R²-Scores.\n\nMini-Übung\n\nDrücken Sie auf den zwei kreisförmigen Pfeile rechts oben. Dadurch wird ein\nneuer Datensatz erzeugt. Die Messdaten sind durch grüne Punkte dargestellt, das\nlineare Regressionsmodell durch eine blaue Gerade. Im Titel wird der aktuelle\nund der optimale R²-Wert angezeigt. Ziehen Sie an den weißen Punkten, um die\nGerade zu verändern. Schaffen Sie es, den optimalen R²-Score zu treffen?\nBeobachten Sie dabei, wie die Fehler (rot) kleiner werden.\n\nWie ist nun der R²-Score für das trainierte lineare Regressionsmodell? Dazu\nverwenden wir die score()-Methode.\n\nr2_score = modell.score(X,y)\nprint(f'Der R2-Score für das lineare Regressionsmodell ist: {r2_score:.2f}.')\n\n\n\nDas lineare Regressionsmodell kann für die Trainingsdaten sehr gut die\nVerkaufspreise prognostizieren. Wie gut es allerdings noch unbekannte Daten\nprognostizieren könnte, ist ungewiss. Mit dem Thema Validierung werden wir uns\neinem späteren Kapitel noch detailliert beschäftigen.","type":"content","url":"/chapter07-sec01#ist-das-beste-modell-gut-genug-der-r-score","position":13},{"hierarchy":{"lvl1":"7.1 Einfache lineare Regression","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter07-sec01#zusammenfassung-und-ausblick","position":14},{"hierarchy":{"lvl1":"7.1 Einfache lineare Regression","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Abschnitt haben wir das theoretische Modell der linearen Regression\nkennengelernt. Das Training eines linearen Regressionsmodells mit Scikit-Learn\nerfolgt wie üblich mit der fit()-Methode, die Prognose mit der\npredict()-Methode. Bewerten können wir Prognosequalität mit der\nscore()-Methode. Im nächsten Kapitel betrachten wir die lineare Regression,\nbei der die Zielgröße von mehreren Merkmalen abhängt.","type":"content","url":"/chapter07-sec01#zusammenfassung-und-ausblick","position":15},{"hierarchy":{"lvl1":"7.2 Multiple lineare Regression"},"type":"lvl1","url":"/chapter07-sec02","position":0},{"hierarchy":{"lvl1":"7.2 Multiple lineare Regression"},"content":"Bisher haben wir nur ein einzelnes Merkmal aus den gesammelten Daten\nherausgegriffen und untersucht, ob es zwischen diesem Merkmal und der Zielgröße\neinen linearen Zusammenhang gibt. So simpel ist die Welt normalerweise nicht,\noft wirken mehrere Einflussfaktoren gleichzeitig. Daher steht die multiple\nlineare Regression in diesem Kapitel im Fokus.","type":"content","url":"/chapter07-sec02","position":1},{"hierarchy":{"lvl1":"7.2 Multiple lineare Regression","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter07-sec02#lernziele","position":2},{"hierarchy":{"lvl1":"7.2 Multiple lineare Regression","lvl2":"Lernziele"},"content":"Lernziele\n\nSie wissen, was eine multiple lineare Regression ist und können sie mit\nScikit-Learn durchführen.\n\nSie wissen, was positive lineare Korrelation und negative lineare\nKorrelation bedeuten.\n\nSie können die lineare Korrelation der Merkmale miteinander mit Hilfe der\nKorrelationsmatrix beurteilen.\n\nSie können die Korrelationsmatrix als Heatmap visualisieren, um\nZusammenhänge zwischen Merkmalen schnell zu erkennen.","type":"content","url":"/chapter07-sec02#lernziele","position":3},{"hierarchy":{"lvl1":"7.2 Multiple lineare Regression","lvl2":"Zusammenhänge zwischen Merkmalen"},"type":"lvl2","url":"/chapter07-sec02#zusammenh-nge-zwischen-merkmalen","position":4},{"hierarchy":{"lvl1":"7.2 Multiple lineare Regression","lvl2":"Zusammenhänge zwischen Merkmalen"},"content":"Im vorherigen Kapitel haben wir den Zusammenhang des Merkmals Leistung [PS] und\nder Zielgröße Preis [EUR] betrachtet. Nun wollen wir noch das Merkmal Alter\ngemessen in Jahren hinzunehmen. 0 Jahre meint dabei einen Neuwagen. Aus\ndidaktischen Gründen werden wir auch hier künstlich erzeugte Daten nutzen, um\ndie multiple lineare Regression zu erklären. Als erstes erzeugen wir die Daten,\ndiesmal direkt mit Hilfsmitteln des Moduls NumPy.\n\nimport numpy as np \nimport pandas as pd \n\nnp.random.seed(0)\nanzahl_autos = 100\n\nx = np.floor( np.random.uniform(0, 11, anzahl_autos) )\ny = np.floor( np.random.uniform(50, 301, anzahl_autos) )\nz = np.floor( -2000 * x + 200 * y + 500 * np.random.normal(0, 1, anzahl_autos) + 10000 )\n\ndaten = pd.DataFrame({\n    'Alter': x,\n    'Leistung [PS]': y,\n    'Preis [EUR]': z\n    })\n\n\n\nDann visualisieren wir die Daten.\n\nimport plotly.express as px\n\nfig = px.scatter_3d(daten, x = 'Alter', y = 'Leistung [PS]', z = 'Preis [EUR]',\n  title='Künstliche Verkaufspreise für Autos')\nfig.show()\n\n\n\nDa wir mehr als drei Merkmale nicht gleichzeitig in einem Diagramm darstellen\nkönnen, nutzen wir eine Scattermatrix. Sie zeigt paarweise Zusammenhänge\nzwischen allen Merkmalen.\n\nfig = px.scatter_matrix(daten,\n    title='Künstliche Daten: Verkaufspreise Autos')\nfig.show()\n\n\n\nWir betrachten die letzte Zeile, in der der Preis auf der y-Achse aufgetragen\nist. In der ersten Spalte wird der Preis abhängig vom Alter dargestellt. Je\nälter das Auto, desto geringer der Preis. In der zweiten Spalte wird der Preis\nabhängig von der Leistung gezeigt. Je leistungsstärker ein Auto, desto höher der\nPreis. Insbesondere vermittelt das Diagramm den Eindruck, dass durch die\nPunktewolke sehr gut eine Regressionsgerade gelegt werden könnte, was bei dem\nZusammenhang Alter -- Preis eher fraglich ist. Zusätzlich stellen wir fest, dass\ndie Scattermatrix symmetrisch ist und die Hauptdiagonale die Verteilung der\nDatenpunkte visualisiert.\n\nWir trainieren jetzt ein lineares Regressionsmodelly = w_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2.\n\nDamit ist gemeint, dass wir die Gewichte w_0, w_1 und w_2 des Modells so\nbestimmen wollen, dass der Preis y möglichst gut durch die beiden Merkmale\nAlter x_1 und Leistung x_2 prognostiziert wird.\n\nIn einem ersten Schritt laden wir das lineare Regressionsmodul.\n\nfrom sklearn.linear_model import LinearRegression\n\nmodell = LinearRegression()\n\n\n\nDann adaptieren wir die Daten.\n\n# Adaption der Daten\nX = daten[['Alter', 'Leistung [PS]']]\ny = daten['Preis [EUR]']\n\n\n\nJetzt können wir das lineare Regressionsmodell von Scikit-Learn mit der\n.fit()-Methode trainieren. Wir lassen auch gleich den R²-Score mit ausgeben.\n\n# Training\nmodell.fit(X, y)\n\n# Validierung\nr2_score = modell.score(X, y)\nprint(f'Der R2-Score ist: {r2_score:.2f}')\n\n\n\nDer R²-Score ist perfekt, ob sich das Modell auf neue Daten verallgemeinern\nlässt, ist damit noch nicht geklärt. Es könnte Overfitting vorliegen. Betrachten\nwir nun, welche Koeffizienten von Scikit-Learn für unsere mehrdimensionale\nlineare Modellfunktion gefunden wurden.\n\nprint(f'Achsenabschnitt w0: {modell.intercept_:.2f}')\nprint(f'Koeffizienten (Steigungen): {modell.coef_}')\n\n\n\nDamit lautet unsere Modellfunktion abhängig von Alter und Leistung alsoy = f(x_1, x_2) = 10168 -2025\\cdot x_1 + 199\\cdot x_2.\n\nFür jedes Jahr, das das Auto altert, sinkt der Preis um 2.025 EUR (bei gleicher\nLeistung), wohingegen jedes weitere PS den Preis um 199 EUR steigert (bei\ngleichem Alter). Geometrisch können wir das Ergebnis so interpretieren, dass\neine Ebene durch die Datenpunkte gelegt wird.","type":"content","url":"/chapter07-sec02#zusammenh-nge-zwischen-merkmalen","position":5},{"hierarchy":{"lvl1":"7.2 Multiple lineare Regression","lvl2":"Korrelationsmatrix"},"type":"lvl2","url":"/chapter07-sec02#korrelationsmatrix","position":6},{"hierarchy":{"lvl1":"7.2 Multiple lineare Regression","lvl2":"Korrelationsmatrix"},"content":"Das Prognoseergebnis des multiplen linearen Regressionsmodell ist für die\nTrainingsdaten sehr gut. Beim Betrachten der Scattermatrix scheint das Merkmal\nLeistung eher einen linearen Einfluss zu haben als das Alter. Als nächstes\nwollen wir bewerten, wie stark der lineare Zusammenhang jedes einzelnen Merkmals\nauf jedes andere Merkmal ist. Dazu betrachten wir die sogenannte\nKorrelationsmatrix. Mit der Methode corr() können wir sie einfach\nberechnen lassen:\n\ndaten.corr()\n\n\n\nDabei verwendet Pandas standardmäßig den\n\n\nPearson-Korrelationskoeffizienten,\nder Werte zwischen -1 und 1 liefert.\n\nIn der ersten Zeile ‘Alter’ wird die Stärke der Korrelation von dem Merkmal\nAlter auf die Merkmale Alter, Leistung und Preis bewertet. Eine positive\nKorrelation beschreibt einen Zusammenhang nach dem Prinzip “wenn mehr, dann\nmehr”. Beispielsweise kann mehr Leistung zu einem höhren Preis führen. Der\numgekehrte Fall ist das Prinzip “wenn mehr, dann weniger” bzw. “wenn weniger,\ndann mehr”, was wir negative Korrelation nennen. Beispielsweise kann ein\nhöheres Alter zu einem geringeren Preis führen.\n\nDie Zahl 1 drückt dabei aus, dass die beiden Merkmale perfekt linear positiv\nkorreliert sind. In der ersten Zeile und der ersten Spalte wird der Zusammenhang\nzwischen Alter und Alter bewertet. Dort muss eine 1 stehen, denn hier sind ja\nbeide Merkmale identisch. In der ersten Zeile und der zweiten Spalte wird die\nlineare Korrelation zwischen Alter und Leistung bewertet. Die Zahl -0.074244 ist\nnahe bei 0 und bedeutet daher, dass es nur einen sehr, sehr schwachen\nZusammenhang zwischen Alter und Leistung gibt, wenn überhaupt. Unser technisches\nVerständnis eines Autos bestätigt, dass Alter und PS nicht zusammenhängen\n(zumindest, wenn man die Leistung des Autos nimmt, wie sie im Fahrzeugschein\neingetragen ist). Dahingegen scheint es eine schwache negative Korrelation\nzwischen Alter und Preis zu geben. Je älter ein Auto ist, desto geringer ist\nsein Preis. -1 würde bedeuten, dass die negative Korrelation perfekt ist.\n\nAm stärksten linear scheinen Leistung und Preis zusammenzuhängen. In der zweiten\nZeile und der dritten Spalte findet sich der Eintrag 0.914003. Je größer die\nLeistung des Autos, desto höher sein Preis.\n\nMini-Übung\n\nInterpretieren Sie die folgenden Korrelationswerte aus der Matrix:\n\nAlter vs. Preis -0.471406: Was bedeutet das Vorzeichen? Ist der Zusammenhang\nstark oder schwach?\n\nLeistung vs. Preis 0.914: Was sagt dieser Wert über den Zusammenhang aus?\n\nAlter vs. Leistung -0.074: Hängen diese beiden Merkmale zusammen?\n\nZusatzfrage: Wenn der Korrelationskoeffizient zwischen zwei Merkmalen bei\n0.95 liegt, bedeutet das, dass das eine das andere verursacht?\n\nLösung\n\n1. Alter vs. Preis: -0.471406\n\nDas negative Vorzeichen bedeutet, dass ein negativer Zusammenhang besteht:\nJe älter das Auto, desto niedriger tendenziell der Preis (und umgekehrt).\n\nDer Betrag von ca. 0.47 deutet auf einen mittelstarken negativen\nZusammenhang hin. Als Faustregel gilt:\n\n|r| < 0.3: schwacher Zusammenhang\n\n0.3 ≤ |r| < 0.7: mittelstarker Zusammenhang\n\n|r| ≥ 0.7: starker Zusammenhang\n\n2. Leistung vs. Preis: 0.914\n\nDieser Wert von 0.914 ist positiv und nahe bei 1, was auf einen sehr starken\npositiven linearen Zusammenhang hinweist. Autos mit mehr PS haben tendenziell\neinen deutlich höheren Preis. Von allen drei Merkmalen zeigt die Leistung die\nstärkste Korrelation mit dem Preis.\n\n3. Alter vs. Leistung: -0.074\n\nDer Wert von -0.074 liegt sehr nahe bei 0, was bedeutet, dass zwischen Alter und\nLeistung praktisch kein linearer Zusammenhang besteht. Das entspricht auch\nunserer Erwartung: Die PS-Zahl eines Autos (laut Fahrzeugschein) ändert sich\nnicht mit dem Alter des Fahrzeugs.\n\n4. Zusatzfrage: Korrelation 0.95 → Kausalität?\n\nNein! Ein Korrelationskoeffizient von 0.95 bedeutet nicht automatisch, dass ein\nMerkmal das andere verursacht. Korrelation beschreibt nur, dass zwei Merkmale\ngemeinsam variieren. Mögliche Erklärungen für hohe Korrelation ohne Kausalität:\n\nEine dritte Variable beeinflusst beide Merkmale.\n\nDer Zusammenhang ist zufällig (besonders bei kleinen Datensätzen).\n\nDie Kausalität verläuft in die umgekehrte Richtung als vermutet.\n\nEs liegt tatsächlich eine kausale Beziehung vor (muss aber separat\nnachgewiesen werden).\n\nBeispiel: Eisverkäufe und Sonnenbrand-Fälle korrelieren stark, aber Eis\nverursacht keinen Sonnenbrand. Beide werden durch eine dritte Variable (warmes\nWetter/Sonneneinstrahlung) beeinflusst.\n\nWarnung: Korrelation ist nicht Kausalität\n\nWichtig: wenn zwei Merkmale korreliert sind, heißt das nicht, dass das eine\nMerkmal das andere verursacht (Kausalität). Es kann auch eine dritte Variable\nbeide beeinflussen oder der Zusammenhang kann zufällig sein.","type":"content","url":"/chapter07-sec02#korrelationsmatrix","position":7},{"hierarchy":{"lvl1":"7.2 Multiple lineare Regression","lvl2":"Heatmaps"},"type":"lvl2","url":"/chapter07-sec02#heatmaps","position":8},{"hierarchy":{"lvl1":"7.2 Multiple lineare Regression","lvl2":"Heatmaps"},"content":"Es ist üblich, die Korrelationsmatrix als sogenanntes Heatmap-Diagramm zu\nvisualisieren. Vor allem wenn die Zusammenhänge zwischen vielen Merkmalen\nuntersucht werden sollen, wird die Korrelationsmatrix schnell unübersichtlich.\nBei einer Heatmap werden die Zahlenwerte der Matrix durch Farben visualisiert.\nPlotly Express bietet dazu die Funktion imshow() an.\n\nkorrelationsmatrix = daten.corr()\n\nfig = px.imshow(korrelationsmatrix)\nfig.show()\n\n\n\nEs ist hilfreich, die Werte der Korrelationsmatrix direkt in der Heatmap\nanzeigen zu lassen. Daher verwenden wir die zusätzliche Option text_auto=True.\n\nfig = px.imshow(korrelationsmatrix, text_auto=True)\nfig.show()\n\n\n\nWeitere Optionen zum Stylen der Heatmaps finden Sie in der \n\nPlotly Dokumentation\n→ Heatmaps in Plotly.","type":"content","url":"/chapter07-sec02#heatmaps","position":9},{"hierarchy":{"lvl1":"7.2 Multiple lineare Regression","lvl2":"Zusammenfassung"},"type":"lvl2","url":"/chapter07-sec02#zusammenfassung","position":10},{"hierarchy":{"lvl1":"7.2 Multiple lineare Regression","lvl2":"Zusammenfassung"},"content":"In diesem Kapitel haben wir uns mit der linearen multiplen Regression\nbeschäftigt. Es wird eine lineare Modellfunktion für einen oder mehrere\nEinflussfaktoren gesucht. Die Parameter der Modellfunktion, also die\nKoeffizienten der mehrdimensionalen linearen Funktion werden so an die Daten\nangepasst, dass die Fehlerquadratsumme möglichst klein wird. Um beurteilen zu\nkönnen, ob die beste gefundene Modellfunktion eine gute Prognose liefert, werten\nwir den R²-Score aus.\n\nUm zu analysieren, ob einzelne Merkmale miteinander linear korreliert sind,\nwerden die Korrelationsmatrix und die Heatmap eingesetzt.","type":"content","url":"/chapter07-sec02#zusammenfassung","position":11},{"hierarchy":{"lvl1":"7.3 Polynomiale Regression"},"type":"lvl1","url":"/chapter07-sec03","position":0},{"hierarchy":{"lvl1":"7.3 Polynomiale Regression"},"content":"In den letzten beiden Kapiteln haben wir uns mit der linearen Regression\nbefasst. Dabei haben wir die einfache lineare Regression betrachtet, bei der die\nZielgröße von einem einzelnen Merkmal abhängt, sowie die multiple lineare\nRegression, bei der die Zielgröße von mehreren Merkmalen beeinflusst wird. In\ndiesem Kapitel werden wir uns damit beschäftigen, wie eine Regression für\nquadratische, kubische oder allgemein für polynomiale Modelle durchgeführt wird.\nDarüber hinaus diskutieren wir die Probleme Overfitting und Underfitting.","type":"content","url":"/chapter07-sec03","position":1},{"hierarchy":{"lvl1":"7.3 Polynomiale Regression","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter07-sec03#lernziele","position":2},{"hierarchy":{"lvl1":"7.3 Polynomiale Regression","lvl2":"Lernziele"},"content":"Lernziele\n\nSie können eine polynomiale Regression durchführen.\n\nSie wissen, dass die Wahl des Polynomgrades entscheidend dafür ist, ob\nUnderfitting (Unteranpassung), Overfitting (Überanpassung) oder ein\ngeeignetes Modell vorliegt.\n\nSie wissen, dass der Polynomgrad ein Hyperparameter ist.","type":"content","url":"/chapter07-sec03#lernziele","position":3},{"hierarchy":{"lvl1":"7.3 Polynomiale Regression","lvl2":"Künstliches Experiment zu Bremswegen eines Autos"},"type":"lvl2","url":"/chapter07-sec03#k-nstliches-experiment-zu-bremswegen-eines-autos","position":4},{"hierarchy":{"lvl1":"7.3 Polynomiale Regression","lvl2":"Künstliches Experiment zu Bremswegen eines Autos"},"content":"Ausnahmsweise werden wir uns in diesem Kapitel nicht mit dem Verkauf von Autos\nbeschäftigen, sondern mit dem Bremsweg von Autos. Die Faustformel zur Berechnung\ndes Bremsweges s in Metern (ohne Reaktionszeit) lautets = \\frac{1}{100} \\cdot v^2,\n\nwobei die Geschwindigkeit v des Autos in km/h angegeben wird. Natürlich\nvariiert der tatsächliche Bremsweg abhängig von der Straßenoberfläche (trocken /\nnass / vereist) oder dem Fahrzeugtyp (inbesondere Leistung der Bremse). Wird die\nBremsung aufgrund eines plötzlich auftauchenden Hindernisses eingeleitet, kommt\nzum Bremsweg noch der Reaktionsweg hinzu. Mehr Details finden Sie auf den\nInternetseiten des ADAC unter \n\nBremsweg berechnen: Mit dieser Formel\ngeht’s.\n\nWir erzeugen nun künstliche Daten, die ein Experiment simulieren: Bremswege von\nAutos in Abhängigkeit von der Geschwindigkeit. In einem ersten Schritt\ngenerieren wir zufällig 50 Geschwindigkeiten zwischen 30 km/h und 150 km/h.\nGemäß der obigen Faustformel lassen wir zunächst die dazugehörigen Bremswege\nberechnen, addieren dann aber noch zufällige Schwankungen.\n\nimport numpy as np \nimport pandas as pd \n\nnp.random.seed(0)\nanzahl_experimente = 50\nv_min = 30\nv_max = 151\n\nv = np.floor( np.random.uniform(v_min, v_max, anzahl_experimente) )\nzufaellige_schwankungen = 3 * np.random.normal(0, 1, anzahl_experimente)\nbremsweg = 1/100 * v**2 \n\ndaten = pd.DataFrame({\n    'Geschwindigkeit [km/h]': v,\n    'Bremsweg [m]': bremsweg + zufaellige_schwankungen,\n    })\n\n\n\nAls nächstes lassen wir die künstlich erzeugten Bremsweg-Experimente visualisieren.\n\nimport plotly.express as px \n\nfig = px.scatter(daten, x = 'Geschwindigkeit [km/h]', y = 'Bremsweg [m]',\n    title='Künstliche Daten: Bremsweg eines Autos')\nfig.show()\n\n\n\n","type":"content","url":"/chapter07-sec03#k-nstliches-experiment-zu-bremswegen-eines-autos","position":5},{"hierarchy":{"lvl1":"7.3 Polynomiale Regression","lvl2":"Erster Versuch: lineare Regression"},"type":"lvl2","url":"/chapter07-sec03#erster-versuch-lineare-regression","position":6},{"hierarchy":{"lvl1":"7.3 Polynomiale Regression","lvl2":"Erster Versuch: lineare Regression"},"content":"Als erstes verwenden wir die lineare Regression, um ein Modell für die Messdaten\nzu finden. Wenn wir die Geschwindigkeit mit x bezeichnen und den Bremsweg mit\ny, dann lautet das lineare Regressionsmodelly = w_0 + w_1 \\cdot x.\n\nfrom sklearn.linear_model import LinearRegression\n\n# Adaption der Daten\nX = daten[['Geschwindigkeit [km/h]']]\ny = daten['Bremsweg [m]']\n\n# Training des Modells\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Bewertung des Modells für die Trainingsdaten\nr2_score = model.score(X, y)\nprint(f'R2-score Trainingsdaten: {r2_score:.4f}')\n\n\n\nDer R²-Score sieht sehr gut aus. Um uns einen Eindruck zu verschaffen, wie gut\ndas lineare Modell tatsächlich ist (wir wissen ja, dass es eigentlich\nquadratisch ist!), erzeugen wir nun systematisch Geschwindigkeiten in dem\nBereich von 30 km/h und 150 km/h und verwenden die Faustformel für die\nBerechnung der Bremswege.\n\nv_test = np.linspace(v_min, v_max, 200)\ns_test = 1/100 * v_test**2\ntestdaten = pd.DataFrame({\n    'Geschwindigkeit [km/h]': v_test,\n    'Bremsweg [m]': s_test\n    })\n\n\n\nMit Hilfe des linearen Regressionsmodells prognostizieren wir die Bremswege für\ndiese Geschwindigkeiten und lassen den R²-Score berechnen.\n\n# Bewertung des Modells für die Testdaten\nX_test = testdaten[['Geschwindigkeit [km/h]']]\ny_test = testdaten['Bremsweg [m]']\n\nr2_score = model.score(X_test, y_test)\nprint(f'R2-score Testdaten: {r2_score:.4f}')\n\n\n\nZuletzt visualisieren wir die Prognose.\n\n# Berechnung der Prognose \ny_prognose = model.predict(X_test)\n\n# Visualisierung\nfig = px.scatter(daten, x = 'Geschwindigkeit [km/h]', y = 'Bremsweg [m]',\n    title='Bremsweg eines Autos: lineares Modell')\nfig.add_scatter(x = testdaten['Geschwindigkeit [km/h]'], y = y_prognose, mode='lines', name='Prognose')\nfig.add_scatter(x = testdaten['Geschwindigkeit [km/h]'], y = y_test, mode='lines', name='Faustformel')\nfig.show()\n\n\n\nVor allem die Visualisierung zeigt die Schwächen des linearen Modells. Bei\nniedrigen Geschwindigkeiten wie in der Stadt unterschätzt das lineare Modell den\nBremsweg. Unterhalb von 40 km/h prognostiziert das Modell sogar einen negativen\nBremsweg. Zwischen 60 km/h und 120 km/h überschätzt das Modell den Bremsweg und\noberhalb von 120 km/h unterschätzt es den Bremsweg wieder. Das Modell ist zu\neinfach für die Prognose, es liegt Underfitting vor. Daher probieren wir\nals nächstes ein quadratisches Regressionsmodell aus.","type":"content","url":"/chapter07-sec03#erster-versuch-lineare-regression","position":7},{"hierarchy":{"lvl1":"7.3 Polynomiale Regression","lvl2":"Quadratische Regression"},"type":"lvl2","url":"/chapter07-sec03#quadratische-regression","position":8},{"hierarchy":{"lvl1":"7.3 Polynomiale Regression","lvl2":"Quadratische Regression"},"content":"Wenn wir in der Dokumentation von Scikit-Learn nun nach einer Funktion zur\nquadratischen Regression suchen, werden wir nicht fündig. Stattdessen nutzen wir\neinen Trick und erzeugen neue Merkmale.\n\nDas lineare Regressionsmodell, das wir eben ausprobiert haben, lautet\nmathematisch formuliert folgendermaßen:y = w_0 + w_1 \\cdot x\n\nmit nur einem Merkmal x, nämlich der Geschwindigkeit.\n\nWenn wir eine quadratische Funktion als Modellfunktion wählen möchten, erzeugen\nwir einfach ein zweites Merkmal. Wir nennen die bisherigen x-Werte x\njetzt x_1 und fügen als zweites Merkmal die neue Eigenschaftx_2 = \\left( x_1 \\right)^2\n\nhinzu. Damit wird aus dem quadratischen Regressionsmodelly = w_0 + w_1 \\cdot x + w_2 \\cdot x^2\n\ndas multiple lineare Regressionsmodelly = w_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2.\n\nScikit-Learn stellt auch hier passende Methoden bereit. Aus dem\nVorbereitungsmodul sklearn.preprocessing importieren wir PolynomialFeatures.\nMehr Details dazu finden Sie in der \n\nDokumentation Scikit-Learn →\nPolynomialFeature.\nWir erzeugen das PolynomialFeature-Objekt mit der Option degree=2, um die\nQuadrate hinzuzufügen. Dann transformieren wir die Input-Daten, indem wir die\nfit_transform()-Methode auf den Input anwenden.\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Adaption der Daten\npolynom_transformator = PolynomialFeatures(degree = 2)\nX = polynom_transformator.fit_transform(daten[['Geschwindigkeit [km/h]']])\ny = daten['Bremsweg [m]']\n\n\n\nWichtig: fit_transform() lernt die Transformation aus den Trainingsdaten und\nwendet sie an, transform() wendet nur die bereits gelernte Transformation an.\n\nDanach können wir das multiple lineare Regressionsmodell trainieren und bewerten\nlassen.\n\n# Training des Modells\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Bewertung des Modells für die Trainingsdaten\nr2_score = model.score(X, y)\nprint(f'R2-score Trainingsdaten: {r2_score:.4f}')\n\n\n\nZuletzt lassen wir das quadratische Regressionsmodell noch visualisieren.\nWichtig ist, dass nun auch die Testdaten quadriert werden müssen, da das\nML-Modell für Prognosen voraussetzt, dass die Daten in der gleichen Art und\nWeise vorliegen wie die Trainingsdaten. Wir müssen denselben Transformator\nnehmen wie zum Training der Daten und nutzen daher nur die\ntransform()-Methode.\n\n# Bewertung des Modells für die Testdaten\nX_test = polynom_transformator.transform(testdaten[['Geschwindigkeit [km/h]']])\ny_test = testdaten['Bremsweg [m]']\nr2_score = model.score(X_test, y_test)\nprint(f'R2-score Testdaten: {r2_score:.4f}')\n\n# Berechnung der Prognose Testdaten\ny_prognose = model.predict(X_test)\n\n# Visualisierung der Prognose   \nfig = px.scatter(daten, x = 'Geschwindigkeit [km/h]', y = 'Bremsweg [m]',\n    title='Bremsweg eines Autos: quadratisches Modell')\nfig.add_scatter(x = testdaten['Geschwindigkeit [km/h]'], y = y_prognose, mode='lines', name='Prognose')\nfig.add_scatter(x = testdaten['Geschwindigkeit [km/h]'], y = y_test, mode='lines', name='Faustformel')\nfig.show()\n\n\n\n\n\nDie Prognose ist so gut, dass wir die Prognose (rot) und die Faustformel (grün)\nkaum unterscheiden können. Kleinere Abweichungen gibt es bei den Bremswegen für\nGeschwindigkeiten oberhalb von 130 km/h. Wir können in den Plot hineinzoomen, um\nuns die Unterschiede anzusehen.","type":"content","url":"/chapter07-sec03#quadratische-regression","position":9},{"hierarchy":{"lvl1":"7.3 Polynomiale Regression","lvl2":"Polynomiale Regression"},"type":"lvl2","url":"/chapter07-sec03#polynomiale-regression","position":10},{"hierarchy":{"lvl1":"7.3 Polynomiale Regression","lvl2":"Polynomiale Regression"},"content":"Mit diesem Trick, die Merkmale zu quadrieren, können wir weitermachen, z.B. die\nMerkmale mit 3 potenzieren. Letztendlich können wir so jedes gewünschte Polynom\nals Regressionspolynom trainieren lassen. Dabei muss ein höheres Polynom nicht\nunbedingt besser sein, wie das folgende Beispiel zeigt. Wir wählen als\nPolynomgrad 14.\n\n# Adaption der Daten\npolynom_transformator = PolynomialFeatures(degree = 14)\nX = polynom_transformator.fit_transform(daten[['Geschwindigkeit [km/h]']])\ny = daten['Bremsweg [m]']\n\n# Training des Modells\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Bewertung des Modells für die Trainingsdaten\nr2_score = model.score(X, y)\nprint(f'R2-score Trainingsdaten: {r2_score:.4f}')\n\n# Bewertung für die Testdaten\nX_test = polynom_transformator.transform(testdaten[['Geschwindigkeit [km/h]']])\nr2_score = model.score(X_test, y_test)\nprint(f'R2-score Testdaten: {r2_score:.4f}')\n\n# Berechnung der Prognose und Visualisierung\ny_prognose = model.predict(X_test)\n\nfig = px.scatter(daten, x = 'Geschwindigkeit [km/h]', y = 'Bremsweg [m]',\n    title='Bremsweg eines Autos: Polynom 14. Grades')\nfig.add_scatter(x = testdaten['Geschwindigkeit [km/h]'], y = y_prognose, mode='lines', name='Prognose')\nfig.add_scatter(x = testdaten['Geschwindigkeit [km/h]'], y = y_test, mode='lines', name='Faustformel')\nfig.show()\n\n\n\n\n\nZwischen 30 km/h und 50 km/h ist der Brensweg konstant knapp 22 m, was natürlich\nnicht mit der Praxis übereinstimmt. Die Visualisierung der Prognose zeigt, dass\ndas ein polynomiales Regressionsmodell mit Grad 14 zu sehr an die Trainingsdaten\nangepasst ist und für neue Daten (siehe Geschwindigkeiten größer 150 km/h) nicht\ngut geeignet ist. Es liegt Overfitting vor.\n\nBei der polynomialen Regression wird der Polynomgrad zu einem\nHyperparameter. Hyperparameter haben wir auch schon bei den\nEntscheidungsbäumen (Decision Trees) kennengelernt. Zur Wiederholung geben wir\nhier erneut die Definition an.\n\nWas ist ... ein Hyperparameter?\n\nEin Hyperparameter ist ein Parameter, der vor dem Training eines Modells\nfestgelegt wird und nicht aus den Daten während des Trainings gelernt wird. Die\nHyperparameter steuern den gesamten Lernprozess und haben einen wesentlichen\nEinfluss auf die Leistung des Modells.\n\nFür ein gutes Modell, müssen die Hyperparameter sorgsam gewählt werden, damit\ndas ML-Modell weder Underfitting noch Overfitting aufweist. Wir fassen zunächst\ndie R²-Scores in einer Tabelle zusammen:\n\nPolynomgrad\n\nR² (Trainingsdaten)\n\nR² (Testdaten)\n\n1\n\n0.9641\n\n0.9698\n\n2\n\n0.9979\n\n0.9999\n\n14\n\n0.9899\n\n0.9895\n\nMini-Übung\n\nVergleichen Sie die drei trainierten Modelle anhand der Tabelle:\n\nWelches Modell hat den besten R²-Score auf den Testdaten?\n\nBeim Polynom Grad 14: Warum ist der Test-Score niedriger als der Train-Score?\n\nOrdnen Sie zu: Welches Modell zeigt Underfitting, welches ist gut angepasst,\nwelches zeigt Overfitting?\n\nWas würde vermutlich passieren, wenn wir mit dem Polynom Grad 14 Bremswege\nbei 200 km/h prognostizieren?\n\nLösung\n\nGrad 2 (quadratisch) hat mit R² ≈ 0.9999 den besten Test-Score.\n\nBeim Polynom Grad 14 ist der Test-Score niedriger als der Train-Score, weil\ndas Modell zu stark an die Trainingsdaten angepasst ist (Overfitting). Es\nlernt auch das Rauschen in den Trainingsdaten, was bei neuen Daten nicht\nhilft.\n\nUnderfitting - gute Passung - Overfitting:\n\nGrad 1 (linear) - Underfitting (zu einfaches Modell, beide Scores um 0.96)\n\nGrad 2 (quadratisch) - gut angepasst (beide Scores nahe 1.0, passt zur\nwahren Faustformel)\n\nGrad 14 - Overfitting (sehr flexibel, aber Test-Score schlechter als\nTrain-Score)\n\nDie Visualisierung zeigt bereits bei 150 km/h unrealistisches Verhalten. Bei\n200 km/h würde das Modell vermutlich viel zu lange Bremswege prognostizieren.\n\nSowohl bei den Trainingsdaten als auch bei den Testdaten hat das Modell mit\nPolynomgrad 2 den besten R²-Score. Interessant ist auch, dass beim Polynom Grad\n14 der Testdaten-Score (0.9895) niedriger ist als der Trainingsdaten-Score\n(0.9899), was ein Hinweis auf Overfitting ist. Beim quadratischen Modell sind\nbeide Scores nahezu perfekt, was zur Faustformel passt. Daher wählen wir für das\nML-Modell ein quadratisches Regressionsmodell.","type":"content","url":"/chapter07-sec03#polynomiale-regression","position":11},{"hierarchy":{"lvl1":"7.3 Polynomiale Regression","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter07-sec03#zusammenfassung-und-ausblick","position":12},{"hierarchy":{"lvl1":"7.3 Polynomiale Regression","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Kapitel haben wir die polynomiale Regression mit Scikit-Learn\nkennengelernt. Auch bei der polynomialen Regression ist die Wahl des\nHyperparameters (Polynomgrad) wichtig. Im nächsten Kapitel werden wir uns\nansehen, wie Hyperparameter systematisch gewählt werden.","type":"content","url":"/chapter07-sec03#zusammenfassung-und-ausblick","position":13},{"hierarchy":{"lvl1":"Übungen"},"type":"lvl1","url":"/chapter07-sec04","position":0},{"hierarchy":{"lvl1":"Übungen"},"content":"","type":"content","url":"/chapter07-sec04","position":1},{"hierarchy":{"lvl1":"Übungen","lvl2":"Aufgabe 7.1 Bevölkerungszahlen in Deutschland"},"type":"lvl2","url":"/chapter07-sec04#aufgabe-7-1-bev-lkerungszahlen-in-deutschland","position":2},{"hierarchy":{"lvl1":"Übungen","lvl2":"Aufgabe 7.1 Bevölkerungszahlen in Deutschland"},"content":"In dieser Aufgabe betrachten wir den Datensatz population_germany.csv. Führen\nSie zuerst eine explorative Datenanalyse durch. Geben Sie dann mit Hilfe eines\nEntscheidungsbaumes (Decision Tree) und eines linearen Regressionsmodells\njeweils eine Prognose ab, wie viele Menschen in Deutschland im Jahr 2100 leben\nwerden. Unterscheiden sich die beiden Prognosen?\n\nÜberblick über die Daten\n\nLaden Sie die csv-Datei population_germany.csv. Welche Daten enthält die\nDatei? Wie viele Datenpunkte sind vorhanden? Wie viele und welche Merkmale gibt\nes? Sind die Daten vollständig? Welche Datentypen haben die Merkmale?\n\nLösungimport pandas as pd\n\ndaten = pd.read_csv('population_germany.csv')\ndaten.info()\n\nEs liegen 222 Einträge vor und 4 Merkmale: Region, Code, Jahr, Population. Die\nersten beiden Eigenschaften Region und Code sind Objects. Die Eigenschaften Jahr\nund Population sind Integer.\n\nStatistik der numerischen Daten\n\nErstellen Sie eine Übersicht der statistischen Kennzahlen für die numerischen\nDaten. Visualisieren Sie anschließend die statistischen Kennzahlen mit Boxplots.\nInterpretieren Sie die statistischen Kennzahlen. Gibt es Ausreißer? Sind die\nWerte plausibel?\n\nLösungdaten.describe()\n\nDas erste Merkmal ist das Jahr. Wir erhalten zwar eine Statistik für die\nJahreszahlen und können ablesen, dass 50 % der Jahre vor 1910 liegen, aber\nsinnvoll ist die Statistik für die Jahreszahlen nicht. Eigentlich sind die\nJahreszahlen für diese Fragestellung ein Index und werden daher nicht weiter\nbetrachtet.\n\nDas zweite numerische Merkmal sind die Bevölkerungszahlen. Die minimale\nPopulation sind 18 Mio. Einwohner, die maximale Einwohnerzahl 83.9 Mio.\nEinwohner. Der Mittelwert von 55.4 Mio. Einwohner ist deutlich unter dem Median\nvon 62.4 Mio. Einwohner. Es gibt einige Jahre mit hohen Bevölkerungszahlen, die\nden Mittelwert nach oben ziehen. Die Verteilung könnte rechtsschief sein. Das\nbestätigt auch der Boxplot.import plotly.express as px \n\nfig = px.box(daten['Population'],\n    labels={'variable':'', 'value': 'Einwohnerzahl'},\n    title='Population in Deutschland')\nfig.show()\n\nDer Boxplot zeigt keine Ausreißer. Insgesamt erscheinen die Werte plausibel.\n\nStatistik der kategorialen Daten\n\nErstellen Sie eine Übersicht der Häufigkeiten für die kategorialen Daten.\nVisualisieren Sie anschließend die Häufigkeiten mit Barplots. Interpretieren Sie\ndie Häufigkeiten. Sind die Werte plausibel?\n\nLösung\n\nAls erstes schauen wir uns an, was sich hinter den Objekten verbirgt.daten.head(10)\n\nBei Region scheinen Länderbezeichnungen (Strings) aufgelistet zu werden und der\nCode scheint ein Länderkürzel (String) zu beinhalten. Bei den ersten 10\nEinträgen findet sich bei Region nur 'Germany' und bei Code nur 'DEU'. Mit\n.unique() überprüfen wir, wie viele verschiedene Einträge es überhaupt gibt.\nSind diese Werte als kategoriale Daten einzuordnen?for merkmal in ['Region', 'Code']:\n    einzigartige_werte = daten[merkmal].unique()\n    anzahl = len(einzigartige_werte)\n    print(f'Das Merkmal {merkmal} hat {anzahl} einzigartige Einträge.')\n\nNein, in beiden Fällen gibt es nur einen einzigen Wert, nämlich 'Germany' und\n'DEU'. Wir können diese beiden Merkmale im Folgenden ignorieren und brauchen\ndaher auch keinen Barplot für die Häufigkeiten.\n\nKorrelationen\n\nErstellen Sie einen Scatterplot mit dem Jahr auf der x-Achse und der Population\nauf der y-Achse. Beschriften Sie den Scatterplot sinnvoll. Vermuten Sie einen\nZusammenhang zwischen Jahr und Bevölkerung? Was fällt Ihnen generell auf? Können\nSie die Besonderheiten mit Geschichtswissen erklären?\n\nLösungfig = px.scatter(daten, x = 'Jahr', y = 'Population',\n    title='Population in Deutschland')\nfig.show()\n\nEs scheint einen linearen Zusammenhang zu geben.\n\nDie Bevölkerung ist seit 1800 gewachsen. Zu Beginn der 1910er Jahr gibt es einen\nBevölkerungsrückgang bis 1919/1920, dann steigt die Bevölkerung wieder an. Auch\nin den 1940er Jahren kam es zu einem Bevölkerungsrückgang. In beiden Fällen\nkönnen wir vermuten, dass diese mit den beiden Weltkriegen zu tun hat. Aber auch\nin späteren Zeiten kam es trotz des langfristigen Wachstumstrends immer\nwieder zu einem kurzen Bevölkerungsrückgang, z.B. um 1984 oder 2010.\n\nLineares Regressionsmodell\n\nAdaptieren Sie die Daten. Wählen Sie als Input das Jahr und als Output die\nPopulation. Trainieren Sie ein lineares Regressionsmodell und lassen Sie den\nR²-Score berechnen und ausgeben.\n\nLösungX = daten[['Jahr']]\ny = daten['Population']\n\nfrom sklearn.linear_model import LinearRegression\n\nmodell_linear = LinearRegression()\nmodell_linear.fit(X,y)\nr2_score_lineares_modell = modell_linear.score(X,y)\nprint(f'R2-Score lineares Modell: {r2_score_lineares_modell:.2f}')\n\nDer R²-Score ist mit 0.97 sehr gut.\n\nEntscheidungsbaum/Decision Tree\n\nLassen Sie nun einen Entscheidungsbaum/Decision Tree trainieren und den R²-Score\nausgeben.\n\nTipp: Das Scikit-Learn-Modell heißt DecisionTreeRegressor.\n\nLösungfrom sklearn.tree import DecisionTreeRegressor\n\nmodell_entscheidungsbaum = DecisionTreeRegressor()\nmodell_entscheidungsbaum.fit(X,y)\nr2_score_entscheidungsbaum = modell_entscheidungsbaum.score(X,y)\nprint(f'R2-Score Entscheidungsbaum: {r2_score_entscheidungsbaum:.2f}')\n\nDer R²-Score ist mit 1.0 scheinbar perfekt, aber das ist ein Zeichen für\nOverfitting auf den Trainingsdaten.\n\nBewertung und Prognose\n\nFür welches Modell würden Sie sich entscheiden? Begründen Sie Ihre Wahl.\n\nLassen Sie dann sowohl das lineare Regressionsmodell als auch den\nEntscheidungsbaum die Populationen von 1800 bis 2100 prognostizieren. Verwenden\nSie dazu den folgenden Datensatz:prognosedaten = pd.DataFrame({\n    'Jahr': range(1800, 2101)\n})\n\nVisualisieren Sie die Prognosen zusammen mit den gemessenen Populationen in\neinem gemeinsamen Scatterplot.\n\nTipp: Mit\n\n`fig.add_scatter(x = prognosedaten[‘Jahr’],y = prognose_linear, name=‘lineare Regression’)``\n\nkönnen Sie einen weiteren Scatterplot zu einem schon existierenden (hier in der\nVariable fig gespeichert) hinzufügen.\n\nWelches Modell würden Sie nach der Visualisierung bevorzugen?\n\nLösung\n\nZunächst einmal erscheint der Entscheidungsbaum besser zu sein als das lineare\nRegressionsmodell, da der R²-Score besser ist. Daher könnte man sich für einen\nEntscheidungsbaum/Decision Tree entscheiden.prognosedaten = pd.DataFrame({\n    'Jahr': range(1800, 2101)\n})\n\nprognose_linear = modell_linear.predict(prognosedaten)\nprognose_entscheidungsbaum = modell_entscheidungsbaum.predict(prognosedaten)\n\nfig = px.scatter(daten, x = 'Jahr', y = 'Population', title='Vergleich der Modelle')\nfig.add_scatter(x = prognosedaten['Jahr'],y = prognose_linear, name='lineare Regression')\nfig.add_scatter(x = prognosedaten['Jahr'],y = prognose_entscheidungsbaum, name='Entscheidungsbaum')\nfig.show()\n\nDie Visualisierung der beiden Modelle zeigt zunächst die bessere Performance des\nEntscheidungsbaumes. Am Ende lernt der Entscheidungsbaum jeden Punkt auswendig\nund liefert daher für die bekannten Daten die perfekte Prognose. Er ist jedoch\nnicht in der Lage, auf unbekannte Daten (also Zeitraum 2022 bis 2100) zu\nverallgemeinern. Es liegt Overfitting vor. Daher ist das lineare\nRegressionsmodell zu bevorzugen.print(f'Prognose im Jahr 2100: {prognose_linear[-1]:.1f}')","type":"content","url":"/chapter07-sec04#aufgabe-7-1-bev-lkerungszahlen-in-deutschland","position":3},{"hierarchy":{"lvl1":"Übungen","lvl2":"Aufgabe 7.2 Marketing-Budget für soziale Medien und Zeitungen"},"type":"lvl2","url":"/chapter07-sec04#aufgabe-7-2-marketing-budget-f-r-soziale-medien-und-zeitungen","position":4},{"hierarchy":{"lvl1":"Übungen","lvl2":"Aufgabe 7.2 Marketing-Budget für soziale Medien und Zeitungen"},"content":"Eine Firma erhebt statistische Daten zu ihren Verkaufszahlen (angegeben in\nTausend US-Dollar) abhängig von dem eingesetzten Marketing-Budget in den\njeweiligen Sozialen Medien (Quelle siehe\n\n\nhttps://​www​.kaggle​.com​/datasets​/fayejavad​/marketing​-linear​-multiple​-regression).\n\nErstellen Sie eine explorative Datenanalyse (EDA). Trainieren Sie dann\nML-Modelle und bewerten Sie, bei welchem sozialen Medium sich am ehesten lohnt\nzu investieren.\n\nÜberblick über die Daten\n\nLaden Sie die csv-Datei marketing_data.csv. Welche Daten enthält die\nDatei? Wie viele Datenpunkte sind vorhanden? Wie viele und welche Merkmale gibt\nes? Sind die Daten vollständig? Welche Datentypen haben die Merkmale?\n\nLösungimport pandas as pd \n\ndaten = pd.read_csv('marketing_data.csv')\ndaten.info()\n\nDie Daten enthalten 171 Einträge, die von 0 bis 170 indiziert sind. Es gibt vier\nEigenschaften, die alle als Floats gespeichert sind. Alle Einträge sind gültig.\nAufgrund der Aufgabenstellung ist klar, dass die ersten drei Eigenschaften das\ninvestierte Marketing-Budget in die sozialen Medien YouTube, Facebook und\nNewspaper darstellen. Die Spalte ‘sales’ repräsentiert dahingehend die Wirkung\nder Marketing-Budgets. Wir werfen noch einen Blick auf die ersten 10 Zeilen der\nTabelle:daten.head(10)\n\nStatistik der numerischen Daten\n\nErstellen Sie eine Übersicht der statistischen Kennzahlen für die numerischen\nDaten. Visualisieren Sie anschließend die statistischen Kennzahlen mit Boxplots.\nInterpretieren Sie die statistischen Kennzahlen. Gibt es Ausreißer? Sind die\nWerte plausibel?\n\nLösungdaten.describe()\n\nAls nächstes visualisieren wir die statistischen Kennzahlen des\nMarketing-Budgets.import plotly.express as px\n\nX = daten.loc[:, 'youtube' : 'newspaper']\nfig = px.box(X,\n             title='Marketing-Budget',\n             labels={'variable': 'Kategorie', 'value': 'Tsd. US-Dollar'})\n\nfig.show()\n\nMit großem Abstand wird am meisten in das Marketing bei YouTube investiert. Im\nMittel 178 Tsd. US-Dollar, wohingegen nur 27 Tsd. US-Dollar in Facebook und 35\nTsd. US-Dollar in Zeitungen (Newspaper) investiert werden. Bei allen drei\nKategorien ist der Median ungefähr mittig zwischen Q1 und Q3. Bei der Kategorie\nNewspaper gibt es einen Ausreißer.\n\nZuletzt betrachten wir noch den Boxplot der Wirkung ‘sales’.import plotly.express as px\n\ny = daten['sales']\nfig = px.box(y,\n             title='Verkäufe',\n             labels={'variable': 'Kategorie', 'value': 'Tsd. US-Dollar'})\n\nfig.show()\n\nAuch bei den Verkäufen gibt es keine Ausreißer. Der Median liegt näher an Q1 als\nan Q3 und unterhalb des Mittelwertes von 16 Tsd. US-Dollar.\n\nKorrelationen\n\nErstellen Sie zuerst eine Scattermatrix, um Zusammenhänge zwischen den Merkmalen\nzu analysieren. Lassen Sie dann die Korrelationsmatrix als Heatmap anzeigen und\ninterpretieren Sie das Ergebnis.\n\nLösung\n\nAlle drei Merkmale YouTube, Facebook und Newspaper könnten eine Wirkung auf die\nVerkaufszahlen haben. Am einfachsten ist zunächst die Visualisierung als\nScattermatrix.fig = px.scatter_matrix(daten)\nfig.show()\n\nYouTube und Facebook scheinen eine stärke lineare Wirkung auf die Verkaufszahlen\nzu haben als die Zeitungen. Genaueres zeigt uns die Korrelationsmatrix:korrelationsmatrix = daten.corr()\n\nfig = px.imshow(korrelationsmatrix, text_auto=True)\nfig.show()\n\nAlle drei sozialen Medien sind positiv korreliert, also je mehr\nMarketing-Budget, desto höhere Verkaufszahlen. Am stärksten korreliert YouTube\ngefolgt von Facebook. Newspaper haben den geringsten Einfluss.\n\nLineares Regressionsmodell\n\nTrainieren Sie drei lineare Regressinsmodelle, jeweils mit einem anderen Merkmal\nals Input, d.h. mit jeweils youtube, facebook, newspaper. Adaptieren Sie\ndazu passend die Daten. Lassen Sie jeweils den R²-Score berechnen und ausgeben.\n\nLösung\n\nWir trainieren zunächst drei einzelne lineare\nRegressionsmodelle und bestimmen den jeweiligen R²-Score.from sklearn.linear_model import LinearRegression\n\ny = daten['sales']\nfor merkmal in ['youtube', 'facebook', 'newspaper']:\n    X = daten[[merkmal]]\n    modell = LinearRegression()\n    modell.fit(X,y)\n    r2_score = modell.score(X,y)\n    print(f'Input: {merkmal}, R2-Score = {r2_score:.2f}')\n\nWie erwartet sind die R²-Scores bei YouTube besser als bei Facebook und den\nNewspapern, da dort ein lineares Modell besser passt.\n\nFinales Modell\n\nTrainieren Sie nun als finales Modell ein multiples Regressionsmodell und\nstellen Sie mit den Koeffizienten (Gewichten) und dem Achsenabschnitt (Bias) die\ndazugehörige Modellfunktion auf.\n\nLösungX = daten[['youtube', 'facebook', 'newspaper']]\ny = daten['sales']\n\nmodell_multiple_regression = LinearRegression()\nmodell_multiple_regression.fit(X,y)\nr2_score = modell_multiple_regression.score(X,y)\n\nprint(f'R2-Score multiples Regressionsmodell: {r2_score:.2f}')\n\nprint(f'Koeffizienten: {modell_multiple_regression.coef_}')\nprint(f'Achsenabschnitt: {modell_multiple_regression.intercept_:.4f}')\n\nModellfunktion:y = 0.0452\\cdot x_{\\text{youtube}} + 0.1884\\cdot x_{\\text{facebook}}\n+ 0.0043\\cdot x_{\\text{newspaper}} + 3.5059.","type":"content","url":"/chapter07-sec04#aufgabe-7-2-marketing-budget-f-r-soziale-medien-und-zeitungen","position":5},{"hierarchy":{"lvl1":"8.1 Fehlende Daten"},"type":"lvl1","url":"/chapter08-sec01","position":0},{"hierarchy":{"lvl1":"8.1 Fehlende Daten"},"content":"Realistische Datensätze sind oft unvollständig. In einer Umfrage hat eine Person\nmit einer Frage nichts anfangen können und daher nichts angekreuzt. Oder ein\nMesssensor an der Produktionsanlage ist abends ausgefallen, was erst am nächsten\nMorgen bemerkt wird. Es gibt viele Gründe, warum Datensätze unvollständig sind.\nIn diesem Abschnitt beschäftigen wir uns damit, fehlende Daten aufzuspüren und\nlernen einfache Methoden kennen, damit umzugehen.","type":"content","url":"/chapter08-sec01","position":1},{"hierarchy":{"lvl1":"8.1 Fehlende Daten","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter08-sec01#lernziele","position":2},{"hierarchy":{"lvl1":"8.1 Fehlende Daten","lvl2":"Lernziele"},"content":"Lernziele\n\nSie können in einem Datensatz mit isnull() fehlende Daten aufspüren und\nanalysieren.\n\nSie kennen die beiden grundlegenden Strategien, mit fehlenden Daten umzugehen:\n\nElimination (Löschen) und\n\nImputation (Vervollständigen).\n\nSie können Daten gezielt mit drop() löschen.\n\nSie können fehlende Daten mit fillna() vervollständigen.","type":"content","url":"/chapter08-sec01#lernziele","position":3},{"hierarchy":{"lvl1":"8.1 Fehlende Daten","lvl2":"Fehlende Daten aufspüren mit isnull()"},"type":"lvl2","url":"/chapter08-sec01#fehlende-daten-aufsp-ren-mit-isnull","position":4},{"hierarchy":{"lvl1":"8.1 Fehlende Daten","lvl2":"Fehlende Daten aufspüren mit isnull()"},"content":"Wir arbeiten im Folgenden mit einem echten Datensatz der Verkaufsplattform\n\n\nAutoscout24.de, der Verkaufsdaten zu 1000 Autos\nenthält.\n\nimport pandas as pd\n\nurl = 'https://raw.githubusercontent.com/gramschs/assets/refs/heads/main/ml4ing/data/autoscout24_fehlende_daten.csv'\ndaten = pd.read_csv(url)\n\ndaten.info()\n\n\n\nDer Datensatz enthält 1000 Autos, also 1000 Zeilen bzw. 1000 Datenpunkte, und 14\nMerkmale. Wir hatten bereits festgestellt, dass die Anzahl der\nnon-null-Einträge für die verschiedenen Merkmale unterschiedlich ist.\nOffensichtlich ist nur bei 963 Autos eine »Farbe« eingetragen und die »Leistung\n(PS)« ist nur bei 987 Autos gültig. Am wenigsten gültige Einträge hat das\nMerkmal »Verbrauch (l/100 km)«, wohingegen bei der Eigenschaft »Kilometerstand\n(km)« nur ein ungültiger Eintrag auftaucht. Welche Einträge ungültig sind,\nkönnen wir mit der Methode isnull() bestimmen. Die Methode liefert ein\nPandas-DataFrame zurück, das True/False-Werte enthält. True steht dabei dafür,\ndass ein Wert fehlt bzw. mit dem Eintrag NaN gekennzeichnet ist (= not a\nnumber). Weitere Details finden Sie in der \n\nPandas-Dokumentation →\nisnull().\n\ndaten.isnull()\n\n\n\nBereits in der zweiten Zeile befindet sich ein Auto, bei dem das Merkmal\n»Verbrauch (l/100 km)« nicht gültig ist (ggf. müssen Sie weiter nach rechts\nscrollen), denn dort steht True. Wir betrachten uns diesen Eintrag:\n\ndaten.loc[1]\n\n\n\nBei dem Auto handelt es sich um ein Hybrid-Fahrzeug, vielleicht wurde deshalb\nder »Verbrauch (l/100 km)« nicht angegeben? Ist das vielleicht auch bei den\nanderen Autos der Grund? Wir speichern zunächst die isnull()-Datenstruktur in\neiner eigenen Variable ab und ermitteln, wie viele Autos keinen gültigen Eintrag\nbei diesem Merkmal haben. Dazu nutzen wir aus, dass der boolesche Wert False\nbei Rechnungen als 0 interpretiert wird und der boolesche Wert True als 1. Die\nMethode .sum() summiert pro Spalte alle Werte, so dass sie direkt die Anzahl\nder ungültigen Werte pro Spalte liefert.\n\nfehlende_daten = daten.isnull()\n\nfehlende_daten.sum()\n\n\n\nJetzt lassen wir uns diese 109 Autos anzeigen, bei denen ungültige Werte beim\n»Verbrauch (l/100 km)« angegeben wurden. Dazu nutzen wir die True-Werte in der\nSpalte Verbrauch (l/100 km) als Filter für den ursprünglichen Datensatz.\nZumindest die ersten 20 Autos lassen wir uns dann mit der .head(20)-Methode\nanzeigen.\n\nautos_mit_fehlendem_verbrauch_pro_100km = daten[ fehlende_daten['Verbrauch (l/100 km)'] == True ]\nautos_mit_fehlendem_verbrauch_pro_100km.head(20)\n\n\n\nBemerkung: Der Vergleich == True ist redundant und kann auch weggelassen werden.\n\nBeim Kraftstoff werden alle möglichen Angaben gemacht: Hybrid, Benzin, Diesel\nund Elektro. Wir müssten jetzt systematisch den fehlenden Angaben nachgehen. Für\nElektrofahrzeuge und ggf. Hybridautos ist die Angabe »Verbrauch (l/100 km)«\nunsinnig. Aber das zweite Auto mit der Nr. 5 wird mit Benzin betrieben, da\nscheint Nachlässigkeit beim Ausfüllen der Merkmale vorzuliegen. Beim fünften\nAuto mit der Nr. 77 ist zwar der »Verbrauch (l/100 km)« nicht angegeben, aber\ndafür der »Verbrauch (g/km)«. Daraus könnten wir den »Verbrauch (l/100 km)«\nabschätzen und den fehlenden Wert ergänzen. Es gibt verschiedene Strategien, mit\nfehlenden Daten umzugehen. Die beiden wichtigsten Verfahren zum Umgang mit\nfehlenden Daten sind\n\nLöschen (Elimination) und\n\nVervollständigung (Imputation).\n\nBei Elimination werden Datenpunkte (Autos) und/oder Merkmale gelöscht. Bei\nImputation (Vervollständigung) werden die fehlenden Werte ergänzt. Beide\nVerfahren werden wir nun etwas detaillierter betrachten.","type":"content","url":"/chapter08-sec01#fehlende-daten-aufsp-ren-mit-isnull","position":5},{"hierarchy":{"lvl1":"8.1 Fehlende Daten","lvl2":"Löschen (Elimination) mit drop()"},"type":"lvl2","url":"/chapter08-sec01#l-schen-elimination-mit-drop","position":6},{"hierarchy":{"lvl1":"8.1 Fehlende Daten","lvl2":"Löschen (Elimination) mit drop()"},"content":"Bei der Elimination (Löschen) können wir filigran vorgehen oder die\nHolzhammer-Methode verwenden. Beispielsweise können wir entscheiden, das Merkmal\n»Verbrauch (l/100 km)« komplett zu löschen und einfach nur den »Verbrauch\n(g/km)« zu berücksichtigen. Aber ein kurzer Blick auf die Daten hatte ja bereits\ngezeigt, dass diese Werte auch nur unzuverlässig gefüllt waren, auch wenn sie\ntechnisch gültig sind. Wir löschen beide Merkmale. Dazu benutzen wir die Methode\ndrop() mit dem zusätzlichen Argument columns=['Verbrauch (l/ 100 km)', 'Verbrauch (g/km)']. Da wir gleich zwei Spalten auf einmal eliminieren möchten,\nmüssen wir die Spalten (Columns) als Liste übergeben. Danach überprüfen wir mit\nder Methode .info(), ob das Löschen geklappt hat.\n\ndaten.drop(columns=['Verbrauch (l/100 km)', 'Verbrauch (g/km)'])\ndaten.info()\n\n\n\nLeider hat der Befehl drop() nicht funktioniert! Was ist da los? Python\nverfolgt das Programmierparadigma »Explizit ist besser als implizit!« Daher\nwerden zwar durch den drop()-Befehl die beiden Spalten gelöscht, aber der\nDatensatz daten selbst bleibt aus Sicherheitsgründen unverändert. Möchten wir\nden Datensatz mit den gelöschten Merkmalen weiter verwenden, müssen wir ihn in\neiner neuen Variable speichern oder die alte Variable daten damit\nüberschreiben. Wir nehmen eine neue Variable namens daten_ohne_verbrauch.\n\ndaten_ohne_verbrauch = daten.drop(columns=['Verbrauch (l/100 km)', 'Verbrauch (g/km)'])\ndaten_ohne_verbrauch.info()\n\n\n\nEin weiterer Datenpunkt weist einen ungültigen Eintrag für den »Kilometerstand\n(km)« auf. Schauen wir zunächst nach, um welches Auto es sich handelt.\n\ndaten_ohne_verbrauch[ daten_ohne_verbrauch['Kilometerstand (km)'].isnull() ]\n\n\n\nBei den Einträgen des Autos sind noch mehr Probleme ersichtlich. Die\nErstzulassung war sicherlich nicht bei 37.500 km und das Jahr ist nicht 12/2020.\nWir können jetzt diesen Datenpunkt löschen oder den Datenpunkt reparieren.\nZunächst einmal der Code zum Löschen des Datenpunktes. Standardmäßig löscht die\ndrop()-Methode ohnehin Zeilen, also Datenpunkte, so dass wir ohne weitere\nOptionen den Index der zu löschenden Datenpunkte angeben. Diesmal verwenden wir\ndie alte Variable um den reduzierten Datensatz zu speichern.\n\ndaten_ohne_verbrauch = daten_ohne_verbrauch.drop(708)\ndaten_ohne_verbrauch.info()\n\n\n\nWie wir in der \n\nDokumentation Pandas →\ndrop()\nnachlesen können, gibt es zum expliziten Überschreiben der alten Variable auch\ndie Alternative, die Option inplace=True zu setzen. Welche Option genutzt\nwird, ist Geschmackssache.\n\nOb alle Angaben plausibel sind, ist nicht gesagt. Bei dem Peugeot mit dem Index\n708 hatten wir ja gesehen, dass bei der Erstzulassung eine Kilometerangabe\nstand. Tatsächlich gab es bereits erste Hinweise darauf, dass manche Werte\ntechnisch gültig, aber nicht plausibel sind. Die Spalte mit dem Jahr\nbeispielsweise wurde beim Import als Datentyp Object klassifiziert. Zu erwarten\nwäre jedoch der Datentyp Integer gewesen. Schauen wir noch einmal in den\nursprünglichen Datensatz hinein.\n\ndaten['Jahr'].unique()\n\n\n\nDa bei dem Peugeot mit dem Index 708 das Jahr fälschlicherweise mit 12/2020\nangegeben wurde, hat dieser eine Text-Eintrag dazu geführt, dass die komplette\nSpalte als Object klassifiziert wurde und nicht als Integer. Daher müssen stets\nweitere Plausibilitätsprüfungen durchgeführt werden, bevor die Daten genutzt\nwerden, um statistische Aussagen zu treffen oder ein ML-Modell zu trainieren.","type":"content","url":"/chapter08-sec01#l-schen-elimination-mit-drop","position":7},{"hierarchy":{"lvl1":"8.1 Fehlende Daten","lvl2":"Vervollständigung (Imputation) mit fillna()"},"type":"lvl2","url":"/chapter08-sec01#vervollst-ndigung-imputation-mit-fillna","position":8},{"hierarchy":{"lvl1":"8.1 Fehlende Daten","lvl2":"Vervollständigung (Imputation) mit fillna()"},"content":"Auch bei den Angaben zur Farbe fehlen Einträge. Zum Beispiel die Zeile mit\ndem Index 2 ist unvollständig.\n\ndaten_ohne_verbrauch.loc[2]\n\n\n\nDiesmal entscheiden wir uns dazu, diese Eigenschaft nicht wegzulassen.\nML-Verfahren brauchen immer einen gültigen Wert und nicht NaN. Wir müssen\ndaher den fehlenden Wert ersetzen. Eine Möglichkeit ist, eine Farbe zu erfinden,\nz.B. ‘bunt’, oder die fehlenden Werte explizit durch einen Eintrag ‘keine\nAngabe’ zu vervollständigen. Dazu benutzen wir die Methode fillna() (siehe\n\n\nPandas-Dokumentation →\nfillna()).\nDie Vervollständigung soll nur die NaN-Werte der Spalte »Farbe« füllen. Daher\nfiltern wir zuerst diese Spalte und wenden darauf die fillna()-Methode an. Das\nerste Argument der fillna()-Methode ist der Wert, durch den die NaN-Werte\nersetzt werden sollen (hier 'keine Angabe'). Damit die Vervollständigung\nexplizit gespeichert wird, überschreiben wir die Spalte.\n\ndaten_ohne_verbrauch['Farbe'] = daten_ohne_verbrauch['Farbe'].fillna('keine Angabe')\n\n# Kontrolle der Vervollständigung\ndaten_ohne_verbrauch.isnull().sum()\n\n\n\nWenn wir uns jetzt noch einmal die dritte Zeile ansehen, sehen wir, dass\nfillna() funktioniert hat.\n\ndaten_ohne_verbrauch.loc[2,:]\n\n\n\nBei den PS-Zahlen haben wir ebenfalls keine vollständigen Daten. Diesmal haben\nwir nicht kategoriale Daten wie die Farben, sondern numerische Werte. Daher\nbietet es sich hier eine zweite Methode der Ersetzung (Imputation) an. Wenn wir\nüberall da, wo keine PS-Zahlen vorliegen, den Mittelwert der vorhandenen\nPS-Zahlen einsetzen, verändern wir zumindest den Mittelwert des gesamten\nDatensatzes nur wenig. Wir berechnen daher zuerst den Mittelwert mit der Methode\n.mean() und nutzen dann die fillna()-Methode.\n\nmittelwert = daten_ohne_verbrauch['Leistung (PS)'].mean()\nprint(f'Der Mittelwert der vorhandenen Einträge »Leistung (PS)« ist: {mittelwert:.2f}')\n\ndaten_ohne_verbrauch['Leistung (PS)'] = daten_ohne_verbrauch['Leistung (PS)'].fillna(mittelwert)\n\n\n\nNoch einmal die Kontrolle, ob jetzt alle NaN-Werte eliminiert oder\nvervollständigt wurden:\n\ndaten_ohne_verbrauch.isnull().sum()\n\n\n\nDer Mittelwert der »Leistung (PS)« ist sehr hoch. Vielleicht haben wir doch den\nDatensatz eher verschlechtert, indem wir fehlende Werte durch den Mittelwert\nersetzt haben. Mit großer Wahrscheinlichkeit haben wir die Varianz im Datensatz\nreduziert und es könnte auch sein, dass wir Korrelationen verfälscht haben.\nVielleicht wäre der Median eine bessere Alternative. Auch könnten wir zunächst\ndie Autos mit fehlenden PS-Zahlen weglassen, für die übrigen Autos ein lineares\nRegressionsmodell oder einen Entscheidungsbaum trainieren und damit die\nfehlenden PS-Zahlen abschätzen. Bei diesem Beispiel wäre die beste Lösung zur\nImputation der ungültigen Werte »Leistung (PS)« die Umrechung der vorhandenen,\ngültigen Werte der Spalte »Leistung (kW)«. Tatsächlich sind die beiden Merkmale\nredundant, da es sich um dasselbe Merkmal in zwei verschiedenen Einheiten\nhandelt, so dass wir die Spalte »Leistung (PS)« auch entfernen könnten.","type":"content","url":"/chapter08-sec01#vervollst-ndigung-imputation-mit-fillna","position":9},{"hierarchy":{"lvl1":"8.1 Fehlende Daten","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter08-sec01#zusammenfassung-und-ausblick","position":10},{"hierarchy":{"lvl1":"8.1 Fehlende Daten","lvl2":"Zusammenfassung und Ausblick"},"content":"Ein wichtiger Teil eines ML-Projektes beschäftigt sich mit der Aufbereitung der\nDaten für die ML-Algorithmen. Dabei ist es nicht nur wichtig, in großen\nDatensammlungen fehlende Einträge aufspüren zu können, sondern ein Gespür dafür\nzu entwickeln, wie mit den fehlenden Daten umgegangen werden soll. Die\nStrategien hängen dabei von der Anzahl der fehlenden Daten und ihrer Bedeutung\nab. Häufig werden unvollständige Daten aus der Datensammlung gelöscht\n(Elimination) oder numerische Einträge durch den Mittelwert der vorhandenen\nDaten ersetzt (Imputation). Wie kategoriale Daten für ML-Algorithmen aufbereitet\nwerden müssen, wird im nächsten Kapitel erklärt.","type":"content","url":"/chapter08-sec01#zusammenfassung-und-ausblick","position":11},{"hierarchy":{"lvl1":"8.2 Trainings- und Testdaten"},"type":"lvl1","url":"/chapter08-sec02","position":0},{"hierarchy":{"lvl1":"8.2 Trainings- und Testdaten"},"content":"Bei den Entscheidungsbäumen und der linearen Regression haben wir mit der\nscore()-Methode bewertet, wie viele der Daten durch das Modell korrekt\nprognostiziert wurden. Je näher der Score an 1 liegt, desto besser. Doch selbst\nein perfekter Score bedeutet nicht zwangsläufig, dass das Modell optimal ist. Es\nkönnte überangepasst (overfitted) sein und daher bei neuen, unbekannten Daten\nschlechte Prognosen liefern. Im Folgenden beschäftigen wir uns mit der\nAufteilung von Daten in Trainings- und Testdaten.","type":"content","url":"/chapter08-sec02","position":1},{"hierarchy":{"lvl1":"8.2 Trainings- und Testdaten","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter08-sec02#lernziele","position":2},{"hierarchy":{"lvl1":"8.2 Trainings- und Testdaten","lvl2":"Lernziele"},"content":"Lernziele\n\nSie verstehen, warum Daten in Trainingsdaten und Testdaten aufgeteilt\nwerden.\n\nSie können mit der Funktion train_test_split() Pandas-DataFrames in\nTrainings- und Testdaten aufteilen.\n\nSie kennen das Konzept der Kreuzvalidierung.","type":"content","url":"/chapter08-sec02#lernziele","position":3},{"hierarchy":{"lvl1":"8.2 Trainings- und Testdaten","lvl2":"Auswendiglernen nützt nichts"},"type":"lvl2","url":"/chapter08-sec02#auswendiglernen-n-tzt-nichts","position":4},{"hierarchy":{"lvl1":"8.2 Trainings- und Testdaten","lvl2":"Auswendiglernen nützt nichts"},"content":"Um die Herausforderungen bei der Modellauswahl zu verdeutlichen, betrachten wir\neinen künstlich generierten Datensatz. Angenommen, wir hätten die folgenden 20\nMesswerte erfasst und möchten ein Regressionsproblem lösen.\n\nimport pandas as pd \nimport plotly.express as px\n\n# Generierung Daten\ndaten = pd.DataFrame()\ndaten['Ursache'] = [1.8681193560547067, 0.18892899670288932, 1.8907374398595373, 0.8592639746974586, 0.7909152983890833, -1.1356420176784945, 1.905097819104967, -1.9750789791816405, -0.9880705504662242, -0.26083387038221684, 1.1175316871750098, -1.2092597015989877, 1.451972942396889, 1.933602708701251, -1.3446310343812051, 0.38933577573143685, -1.96405560932978, -0.45371486942548245, -1.8233597682740017, 1.8266118708569437]\ndaten['Wirkung'] = [18.06801933135814, 0.09048390063552635, 18.29951272892001, 4.02392603643671, 1.97091878521032, 6.799411114666941, 17.540101218695103, 21.051664199041685, 5.604758672240995, 0.38630710692300024, 5.261393705782588, 7.365977868421521, 10.701020062336028, 17.48514901635516, 11.263523310016517, 1.1522069460363902, 20.979929897937023, -0.08352624016486021, 18.258951764602635, 15.321589041941028]\n\n# Visualisierung\nfig = px.scatter(daten, x = 'Ursache', y = 'Wirkung', title= 'Künstlich generierte Messdaten')\nfig.show()\n\n\n\nNun würden wir das folgende Modell implementieren. Der Name des Modells sagt\nbereits alles!\n\nfrom sklearn.metrics import r2_score\n\nclass AuswendigLerner:\n    def __init__(self) -> None:\n        self.X = None\n        self.y = None\n\n    def fit(self, X,y):\n        self.X = X\n        self.y = y\n\n    def predict(self, X):\n        return self.y\n\n\n\nWir trainieren unser Modell und lassen es dann bewerten. Um nicht selbst den\nR²-Score implementieren zu müssen, verwenden wir die allgemeine Funktion aus\nScikit-Learn (siehe \n\nDokumentation Scikit-Learn →\nr2_score).\n\n# Adaption der Daten\nX = daten[['Ursache']]\ny = daten['Wirkung']\n\n# Auswahl Modell und Training\nmein_super_modell = AuswendigLerner()\nmein_super_modell.fit(X, y)\n\n# prediction\ny_prognose = mein_super_modell.predict(X)\n\n# check quality\nscore = r2_score(y,y_prognose)\nprint(f'Der R2-Score ist: {score:.2f}')\n\n\n\nEin R²-Score von 1, unser Modell scheint perfekt zu funktionieren! Doch wie\nprognostiziert es neue Daten? Das Modell funktioniert zwar hervorragend für die\ngegebenen Trainingsdaten, ist jedoch nicht verallgemeinerbar.\n\nmein_super_modell.predict([[1.3]])\n\n\n\nAnstatt für den x-Wert 1.3 (Ursache) eine Prognose zu treffen, gibt das Modell\neinfach die auswendig gelernten y-Werte (Wirkungen) aus.","type":"content","url":"/chapter08-sec02#auswendiglernen-n-tzt-nichts","position":5},{"hierarchy":{"lvl1":"8.2 Trainings- und Testdaten","lvl2":"Daten für später aufheben"},"type":"lvl2","url":"/chapter08-sec02#daten-f-r-sp-ter-aufheben","position":6},{"hierarchy":{"lvl1":"8.2 Trainings- und Testdaten","lvl2":"Daten für später aufheben"},"content":"Bei der Modellauswahl und dem Training des Modells müssen wir zusätzlich\nsicherstellen, dass das Modell verallgemeinerbar ist, das heißt, dass es auch\nfür neue, zukünftige Daten verlässliche Prognosen liefern kann. Da wir jedoch\nsofort abschätzen wollen, wie gut das Modell auf neue Daten reagiert, und nicht\nwarten möchten, bis die nächsten Messungen vorliegen, legen wir jetzt schon\neinen Teil der vorhandenen Daten zur Seite. Diese Daten nennen wir\nTestdaten. Die verbleibenden Daten verwenden wir für das Training des\nModells, sie heißen Trainingsdaten. Später nutzen wir die Testdaten, um zu\nüberprüfen, wie gut das Modell bei Daten funktioniert, die nicht zum Training\nverwendet wurden.\n\nFür die Aufteilung in Trainings- und Testdaten verwenden wir eine dafür\nvorgesehene Funktion von Scikit-Learn namens train_test_split() (siehe\n\n\nDokumentation Scikit-Learn →\ntrain_test_split()).\nDiese Funktion müssen wir aus dem Modul sklearn.model_selection importieren.\nDann übergeben wir train_test_split() die Daten, die aufgeteilt werden sollen,\nund erhalten als Rückgabe zwei DataFrames: Der erste enthält die Trainingsdaten,\nder zweite die Testdaten.\n\nfrom sklearn.model_selection import train_test_split\n\ndaten_train, daten_test = train_test_split(daten)\n\n\n\nNun wollen wir sehen, welche Datenpunkte zu den Trainingsdaten und welche zu den\nTestdaten gehören. Dazu fügen wir dem Datensatz ein neues Merkmal hinzu und\nfüllen es mit den Strings 'Trainingsdaten' bzw. 'Testdaten'. Anschließend\nvisualisieren wir die Datenpunkte wie oben, wobei die Punkte entsprechend ihrer\nZugehörigkeit (Trainings- oder Testdaten) eingefärbt werden.\n\n# Anreicherung der Daten mit dem Splitstatus\ndaten.loc[daten_train.index,'Splitstatus'] = 'Trainingsdaten'\ndaten.loc[daten_test.index, 'Splitstatus'] = 'Testdaten'\n\n# Visualisierung\nfig = px.scatter(daten, x = 'Ursache', y = 'Wirkung', color='Splitstatus', \ntitle='Künstlich generierte Messdaten')\nfig.show()\n\n\n\nStandardmäßig hält die Funktion train_test_split() 25 % der Daten als\nTestdaten zurück. Ein schnelles Zählen der fünf Testdatenpunkte bestätigt dies.\nDie Auswahl der Testdaten erfolgt zufällig, sodass jeder Durchlauf des Codes\neine andere Aufteilung der Daten erzeugt.\n\nDie Funktion bietet aber auch Optionen, um die Aufteilung nach eigenen Wünschen\nanzupassen:\n\ntest_size: Mit der Option test_size kann ein anderer Anteil als 25 % für\ndie Testdaten festgelegt werden. Möchte man zum Beispiel nur 10 % der Daten\nals Testdaten zurückhalten, kann man test_size=0.1 einstellen. Der Anteil\nwird als Float zwischen 0.0 und 1.0 angegeben. Verwendet man stattdessen einen\nInteger, interpretiert Scikit-Learn diesen als Anzahl der Testdatenpunkte.\ntest_size=7 bedeutet also, dass sieben Datenpunkte als Testdaten verwendet\nwerden.\n\nrandom_state: Die zufällige Auswahl der Testdaten erfolgt durch einen\nZufallszahlengenerator, der bei jedem Durchlauf neu gestartet wird. Wenn wir\nzwar eine zufällige Auswahl wollen, aber den Neustart des\nZufallszahlengenerators verhindern möchten, können wir den Ausgangszustand des\nGenerators mit einem festen Wert (Integer) festlegen. Das ist vor allem für\nPräsentationen oder Lehrmaterialien nützlich.\n\nshuffle: Die Option shuffle bestimmt, ob die Daten vor der Aufteilung\ndurchmischt werden. Der Standard ist True, d.h. die Datenpunkte werden\nzufällig durchmischt, bevor sie aufgeteilt werden. Wird diese Option auf\nFalse gesetzt, behalten die Daten ihre ursprüngliche Reihenfolge. Bei einem\nüblichen Split von 80/20 in Trainingsdaten und Testdaten werden die ersten 80\n% für die Trainingsdaten genommen und die letzten 20 % für die Testdaten. Sind\ndie Daten sortiert, kann es dadurch zu Verzerrungen kommen. Kommen\nbeispielsweise erst alle billigen Autos und dann die teuren, lernt das\nML-Modell mit den billigeren Autos und testet mit den teureren Autos.\n\nstratify: Diese Option ist vor allem wichtig, wenn die Verteilung zwischen\nverschiedenen Klassen erhalten bleiben soll. Sind im gesamten Datensatz 30 %\nder Autos Diesel-Fahrzeuge, sollen auch in den Trainingsdaten 30 % der Autos\nDiesel-Fahrzeuge sein. Diese Option erfordert, dass die Option shuffle auf\nTrue gesetzt ist. Mehr Informationen zum Gebrauch von stratify finden wir\nin der \n\nDokumentation Scikit-Learn →\ntrain_test_split.\n\nNun verwenden wir train_test_split für unsere Daten.\n\ndaten_train, daten_test = train_test_split(daten, test_size=7, random_state=0)\n\n# Aktualisierung des Splitstatus\ndaten.loc[daten_train.index,'Splitstatus'] = 'Trainingsdaten'\ndaten.loc[daten_test.index, 'Splitstatus'] = 'Testdaten'\n\n# Visualisierung\nfig = px.scatter(daten, x = 'Ursache', y = 'Wirkung', color='Splitstatus', \ntitle='Künstlich generierte Messdaten')\nfig.show()\n\n\n\n","type":"content","url":"/chapter08-sec02#daten-f-r-sp-ter-aufheben","position":7},{"hierarchy":{"lvl1":"8.2 Trainings- und Testdaten","lvl2":"Idee der Kreuzvalidierung"},"type":"lvl2","url":"/chapter08-sec02#idee-der-kreuzvalidierung","position":8},{"hierarchy":{"lvl1":"8.2 Trainings- und Testdaten","lvl2":"Idee der Kreuzvalidierung"},"content":"Das Zurückhalten eines Teils der Daten als Testdaten hat den Nachteil, dass\nweniger Daten für das Training zur Verfügung stehen. Besonders bei kleinen\nDatensätzen kann dies dazu führen, dass das Modell ungenau oder schlecht\ntrainiert wird. Hier kommt die Kreuzvalidierung ins Spiel.\n\nDie Idee der Kreuzvalidierung ist, die Daten in mehrere Teilmengen zu\nunterteilen und das Modell mehrmals zu trainieren und zu testen, um die Leistung\nbesser beurteilen zu können. Schauen wir uns zunächst die zweifache\nKreuzvalidierung an:\n\nBei der zweifachen Kreuzvalidierung teilen wir die Daten in zwei Teilmengen, A\nund B. Das Modell wird dann zweimal trainiert und getestet: einmal mit A als\nTrainingsdaten und B als Testdaten, und einmal umgekehrt. Die endgültige\nModellbewertung ergibt sich aus dem Durchschnitt der beiden Testergebnisse.\n\nDie dreifache Kreuzvalidierung funktioniert ähnlich, mit dem Unterschied, dass\ndie Daten in drei Teilmengen A, B und C aufgeteilt werden. In drei Durchläufen\nwird jeweils mit zwei der Teilmengen trainiert und mit der dritten getestet:\n\nIm ersten Durchlauf wird mit A und B trainiert und mit C getestet.\n\nIm zweiten Durchlauf wird mit B und C trainiert und mit A getestet.\n\nIm dritten Durchlauf wird mit A und C trainiert und mit B getestet. Am Ende\nwird der Durchschnitt der drei Testergebnisse als Maß für die Modellleistung\nverwendet.\n\nDieses Verfahren lässt sich auf beliebig viele Teilmengen erweitern.\nScikit-Learn bietet dafür auch spezielle Funktionen zur effizienten Umsetzung\nder Kreuzvalidierung. Eine detailliertere Betrachtung dieser Techniken erfolgt\njedoch in einem späteren Kapitel. An dieser Stelle soll lediglich das Konzept\nder Kreuzvalidierung eingeführt werden.","type":"content","url":"/chapter08-sec02#idee-der-kreuzvalidierung","position":9},{"hierarchy":{"lvl1":"8.2 Trainings- und Testdaten","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter08-sec02#zusammenfassung-und-ausblick","position":10},{"hierarchy":{"lvl1":"8.2 Trainings- und Testdaten","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Abschnitt haben wir die Aufteilung von Daten in Trainings- und\nTestdaten kennengelernt und die Funktion train_test_split() verwendet. Diese\nFunktion wird uns in zukünftigen Kapiteln und Projekten begleiten. Zudem haben\nwir eine erste Einführung in die Kreuzvalidierung erhalten, die wir später\nausführlicher behandeln werden.","type":"content","url":"/chapter08-sec02#zusammenfassung-und-ausblick","position":11},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung"},"type":"lvl1","url":"/chapter08-sec03","position":0},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung"},"content":"ML-Algorithmen können nur Zahlen verarbeiten. In diesem Kapitel werden wir uns\nzunächst damit beschäftigen, wie auch kategoriale Daten wie beispielsweise die\nFarbe eines Autos verarbeitet werden können. Da viele ML-Modelle empfindlich\ndarauf reagieren, wenn die numerischen Werte in sehr unterschiedlichen\nGrößenordnungen liegen, beschäftigen wir uns auch mit der Skalierung von\nnumerischen Daten.","type":"content","url":"/chapter08-sec03","position":1},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter08-sec03#lernziele","position":2},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung","lvl2":"Lernziele"},"content":"Lernziele\n\nSie können geordnete kategoriale (= ordinale) Daten mit Hilfe eines\nDictionaries und der replace()-Methode als Zahlen kodieren.\n\nSie können ungeordnete kategoriale (= nominale) Daten mit Hilfe der\nget_dummies()-Methode als Zahlen kodieren. Diese Methode nennt man\nOne-Hot-Kodierung.\n\nSie können numerische Daten skalieren, indem Sie\n\nmit dem MinMaxScaler die Daten normieren oder\n\nmit dem StandardScaler die Daten standardisieren.","type":"content","url":"/chapter08-sec03#lernziele","position":3},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung","lvl2":"Kodierung von kategorialen Daten"},"type":"lvl2","url":"/chapter08-sec03#kodierung-von-kategorialen-daten","position":4},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung","lvl2":"Kodierung von kategorialen Daten"},"content":"Bei den Beispielen zur linearen Regression haben wir zur Prognose des\nVerkaufspreises nur numerische Daten genutzt, wie beispielsweise den\nKilometerstand. Es gibt jedoch weitere Merkmale, die die Kaufentscheidung\nbeeinflussen, wie der Kraftstofftyp (Diesel oder Benzin) und die Marke des\nAutos. Diese würden wir ebenfalls gerne in die Prognose des Preises einfließen\nlassen. Dazu müssen die kategorialen Daten, die in der Regel durch den Datentyp\nString gekennzeichnet sind, vorab in Integer oder Floats umgewandelt werden. Je\nnachdem, ob die kategorialen Daten geordnet oder ungeordnet sind, gibt es\nverschiedene Vorgehensweisen, wie wir uns im Folgenden anhand eines Beispiels\nerarbeiten.\n\nWir laden einen Datensatz mit Verkaufsdaten der Plattform\n\n\nAutoscout24.de. Sie können die csv-Datei hier\nherunterladen {download}Download autoscout24_kodierung.csv <https://gramschs.github.io/book_ml4ing/data/autoscout24_kodierung.csv> und in\ndas Jupyter Notebook importieren. Alternativ können Sie die csv-Datei auch über\ndie URL importieren, wie es in der folgenden Code-Zelle gemacht wird. Mit der\nMethode .info() lassen wir uns anzeigen, welchen Datentyp die Merkmale haben.\n\nimport pandas as pd \n\nurl = 'https://raw.githubusercontent.com/gramschs/assets/refs/heads/main/ml4ing/data/autoscout24_kodierung.csv'\ndaten = pd.read_csv(url)\n\ndaten.info()\n\n\n\nWir sehen\n\n8 Merkmale mit Datentyp object: Marke, Modell, Farbe, Erstzulassung,\nGetriebe, Kraftstoff, Bemerkungen, Zustand,\n\n4 Merkmale mit Datentyp int64: Jahr, Preis (Euro), Leistung (PS), Leistung\n(kW)\n\n2 Merkmale mit Datentyp float64: Verbrauch (l/100 km) und Kilometerstand\n(km).\n\nAls erstes betrachten wir geordnete Daten.","type":"content","url":"/chapter08-sec03#kodierung-von-kategorialen-daten","position":5},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung","lvl3":"Geordnete kategoriale Daten mit zwei Kategorien (binär ordinale Daten)","lvl2":"Kodierung von kategorialen Daten"},"type":"lvl3","url":"/chapter08-sec03#geordnete-kategoriale-daten-mit-zwei-kategorien-bin-r-ordinale-daten","position":6},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung","lvl3":"Geordnete kategoriale Daten mit zwei Kategorien (binär ordinale Daten)","lvl2":"Kodierung von kategorialen Daten"},"content":"Als erstes betrachten wir das Merkmal »Getriebe«. Mit der Methode .unique()\nermitteln wir, wie viele verschiedene Kategorien es für dieses Merkmal gibt.\n\ndaten['Getriebe'].unique()\n\n\n\nEs gibt nur zwei Kategorien: Automatik und Schaltgetriebe. Diese beiden Werte\nwollen wir durch Integer ersetzen:\n\nAutomatik --> 0 und\n\nSchaltgetriebe --> 1.\n\n0 ist dabei nicht besser als 1, wir wollen nur zwei verschiedene Integer nehmen,\num die beiden Werte ‘Automatik’ und ‘Schaltgetriebe’ als Zahlen darzustellen.\nPandas bietet dazu die Methode replace() an. Bei der Verwendung dieser Methode\ndarf sich der Datentyp nicht ändern (in Pandas Version 2 noch erlaubt, ab\nVersion 3 verboten). Daher kodieren wir zunächst die Strings 'Automatik' und\n'Schaltgetriebe' als die Strings '0' und '1'mit Hilfe eines Dictionaries:\n\ngetriebe_kodierung = {\n  'Automatik': '0',\n  'Schaltgetriebe': '1',\n}\n\n\n\nDann verwenden wir replace(), um die Ersetzung vorzunehmen. Zuletzt wandeln\nwir die Strings '0' und '1' noch mit der Methode astype() in Integer um:\n\ndaten['Getriebe'] = daten['Getriebe'].replace(getriebe_kodierung)\ndaten['Getriebe'] = daten['Getriebe'].astype('int')\n\n# Kontrolle\ndaten['Getriebe'].unique()\n\n\n\n","type":"content","url":"/chapter08-sec03#geordnete-kategoriale-daten-mit-zwei-kategorien-bin-r-ordinale-daten","position":7},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung","lvl3":"Geordnete kategoriale Daten (ordinale Daten)","lvl2":"Kodierung von kategorialen Daten"},"type":"lvl3","url":"/chapter08-sec03#geordnete-kategoriale-daten-ordinale-daten","position":8},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung","lvl3":"Geordnete kategoriale Daten (ordinale Daten)","lvl2":"Kodierung von kategorialen Daten"},"content":"Für das Merkmal »Zustand« gibt es vier Kategorien.\n\ndaten['Zustand'].unique()\n\n\n\nDie vier Zustände haben eine Ordnung, denn ein Neuwagen ist wertvoller als ein\nJahreswagen. Der Jahreswagen wiederum ist im Allgemeinen wertvoller als der junge\nGebrauchtwagen. Am wenigsten wertvoll ist der Gebrauchtwagen. Durch diese\nOrdnung ist es sinnvoll, beim Kodieren der Zustände durch Integer die Ordnung\nbeizubehalten.\n\nzustand_kodierung = {\n  'Gebrauchtwagen': '0',\n  'junger Gebrauchtwagen': '1', \n  'Jahreswagen': '2',\n  'Neuwagen': '3'\n}\n\ndaten['Zustand'] = daten['Zustand'].replace(zustand_kodierung)\ndaten['Zustand'] = daten['Zustand'].astype('int')\n\n# Kontrolle\ndaten['Zustand'].unique()\n\n\n\n","type":"content","url":"/chapter08-sec03#geordnete-kategoriale-daten-ordinale-daten","position":9},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung","lvl3":"Ungeordnete kategoriale Daten (nominale Daten): One-Hot-Kodierung","lvl2":"Kodierung von kategorialen Daten"},"type":"lvl3","url":"/chapter08-sec03#ungeordnete-kategoriale-daten-nominale-daten-one-hot-kodierung","position":10},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung","lvl3":"Ungeordnete kategoriale Daten (nominale Daten): One-Hot-Kodierung","lvl2":"Kodierung von kategorialen Daten"},"content":"Anders verhält es sich bei den ungeordneten kategorialen Daten wie\nbeispielsweise den Farben der Autos.\n\ndaten['Farbe'].unique()\n\n\n\n14 verschiedene Farben haben die Autos in dem Datensatz. Es wäre jedoch falsch,\nnun Integer von 0 bis 13 zu vergeben, denn das würde eine Ordnung der Farben\nvoraussetzen, die es nicht gibt. Wir verwenden daher das Verfahren der\nOne-Hot-Kodierung. Anstatt einer Spalte mit den Farben führen wir 14 neue\nSpalten mit den Farben ‘grau’, ‘grün’, ‘schwarz’, ‘blau’, usw. ein. Wenn ein\nAuto die Farbe ‘grau’ hat, notieren wir in der Spalte ‘grau’ in dieser Zeile\neine 1 und in den übrigen 13 Spalten mit den anderen Farben eine 0. So können\nwir die Farben numerisch kodieren, ohne eine Ordnung der Farben einzuführen, die\nes nicht gibt. Pandas bietet dafür die Methode get_dummies()an. Schauen wir\nuns zunächst an, was diese Methode bewirkt.\n\npd.get_dummies(daten['Farbe'])\n\n\n\nDamit haben wir die Spalte »Farbe« nun durch 14 Spalten kodiert. Wir könnten nun\nim ursprünglichen Datensatz die Spalte »Farbe« löschen und die neuen 14 Spalten\nhinzufügen. Tatsächlich erledigt das Pandas bereits für uns, wenn wir die\nMethode etwas modifiziert aufrufen. Mit dem Argument data= übergeben wir nun\nden kompletten Datensatz und mit dem Argument columns= spezifizieren wir die\nListe der ungeordneten kategorialen Daten, die One-Hot-kodiert werden sollen.\n\ndaten = pd.get_dummies(data=daten, columns=['Farbe'])\ndaten.head()\n\n\n\nDie neuen Spaltennamen sind eine Kombination aus dem alten Spaltennamen »Farbe«\nund den Kategorien.","type":"content","url":"/chapter08-sec03#ungeordnete-kategoriale-daten-nominale-daten-one-hot-kodierung","position":11},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung","lvl2":"Skalierung von numerischen Daten"},"type":"lvl2","url":"/chapter08-sec03#skalierung-von-numerischen-daten","position":12},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung","lvl2":"Skalierung von numerischen Daten"},"content":"Nachdem wir uns intensiv mit den kategorialen Daten beschäftigt haben,\nbetrachten wir nun die numerischen Daten. Wir laden den Original-Datensatz und\nentfernen die kategorialen Daten.\n\nurl = 'https://raw.githubusercontent.com/gramschs/assets/refs/heads/main/ml4ing/data/autoscout24_kodierung.csv'\ndaten = pd.read_csv(url)\n\ndaten = daten.drop(columns=['Marke', 'Modell', 'Farbe', 'Erstzulassung', \n                            'Getriebe', 'Kraftstoff','Bemerkungen', 'Zustand'])\ndaten.info()\n\n\n\nEin erster Blick auf die Daten zeigt bereits, dass die Eigenschaftswerte in\nunterschiedlichen Bereichen liegen.\n\ndaten.head()\n\n\n\nDer Verbrauch gemessen in Litern pro 100 Kilometer liegt zwischen 5 und 10,\nwohingegen der Kilometerstand die 100000 km übersteigt. Das zeigt auch die\nÜbersicht der statistischen Kennzahlen:\n\ndaten.describe()\n\n\n\nDamit ist auch der Boxplot nur noch schwer lesbar:\n\nimport plotly.express as px \n\nfig = px.box(daten)\nfig.show()\n\n\n\nDas hat auch Auswirkungen auf das Training der ML-Modelle. Daher beschäftigen\nwir uns nun mit der Skalierung von Daten.\n\nSind die Bereiche der Daten von ihren Zahlenwerten sehr verschieden, sollten\nalle numerischen Werte in dieselbe Größenordnung gebracht werden. Dieser Vorgang\nheißt Skalieren der Daten. Gebräuchlich sind dabei zwei verschiedene\nMethoden:\n\nNormierung und\n\nStandardisierung.","type":"content","url":"/chapter08-sec03#skalierung-von-numerischen-daten","position":13},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung","lvl3":"Normierung","lvl2":"Skalierung von numerischen Daten"},"type":"lvl3","url":"/chapter08-sec03#normierung","position":14},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung","lvl3":"Normierung","lvl2":"Skalierung von numerischen Daten"},"content":"Bei der Normierung wird festgelegt, dass alle Zahlenwerte in einem festen\nIntervall liegen. Besonders häufig wird das Intervall [0,1] genommen. Die\nVerbrauch (l/100 km), der zwischen 3.5 und 14.9 liegt, würde so transformiert\nwerden, dass das Minimum 3.5 der 0 entspricht und das Maximum 14.9 der 1.\nGenauso würde mit den anderen Eigenschaften verfahren werden. Wir nutzen zur\npraktischen Umsetzung Scikit-Learn.\n\nDamit keine Informationen über die Testdaten in das Training des ML-Modells\nsickern (Data Leakage), wird die Normierung an das Minimum und das Maximum der\nTrainingsdaten angepasst und ggf. für die Testdaten angewendet. Damit können\nTestdaten auch außerhalb des Intervalls [0,1] liegen. Wir splitten daher\nzunächst unsere Daten in Trainings- und Testdaten.\n\nfrom sklearn.model_selection import train_test_split\n\ndaten_train, daten_test = train_test_split(daten, random_state=0)\n\n\n\nDann importieren wir die Klasse MinMaxScaler aus dem Untermodul\nsklearn.preprocessing und erzeugen ein MinMaxScaler-Objekt:\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Auswahl Skalierungsmethode: Normierung\nnormierung = MinMaxScaler()\n\n\n\nJetzt wird das Minimum/Maximum jeder Spalte bestimmt, also der MinMaxScaler an\ndie Trainingsdaten angepasst. Daher ist es nicht verwunderlich, dass die Methode\nfit() genannt wurde. Dem MinMaxScaler werden also die Trainingsdaten\nübergeben:\n\nnormierung.fit(daten_train)\n\n\n\nZuletzt erfolgt die Transformation der Daten mit der transform()-Methode. Dazu\nwerden einmal die Trainingsdaten und einmal die Testdaten dem angepassten\nMinMaxScaler übergeben und die transformierten Daten in neuen Variablen\ngespeichert.\n\n# Transformation der Trainings- und Testdaten\nX_train_normiert = normierung.transform(daten_train)\nX_test_normiert = normierung.transform(daten_test)\n\n\n\nWir schauen in ‘X_train_normiert’ hinein:\n\nprint(X_train_normiert)\n\n\n\nDie Normierung der Daten scheint funktioniert zu haben. Alle Werte liegen\nzwischen 0 und 1. Gleichzeitig haben wir aber die Pandas-DataFrame-Datenstruktur\nverloren. Die Normierung ist nicht für uns Menschen gedacht, sondern für den\nML-Algorithmus. Daher nutzt Scikit-Learn die Transformation der Daten\ngleichzeitig für die Umwandlung in das speichereffizientere NumPy-Array, das für\nden ML-Algorithmus gebraucht wird.","type":"content","url":"/chapter08-sec03#normierung","position":15},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung","lvl3":"Standardisierung","lvl2":"Skalierung von numerischen Daten"},"type":"lvl3","url":"/chapter08-sec03#standardisierung","position":16},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung","lvl3":"Standardisierung","lvl2":"Skalierung von numerischen Daten"},"content":"Oft sind Daten normalverteilt. Die Standardisierung berücksichtigt das und\ntransformiert nicht auf ein festes Intervall, sondern verschiebt den Mittelwert\nauf 0 und die Varianz auf 1. Die normalverteilten Daten werden also\nstandardnormalverteilt. Auch das lassen wir Scikit-Learn erledigen:\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Auswahl Skalierungsmethode: Standardisierung\nstandardisierung = StandardScaler()\n\n# Analyse: jede Spalte wird auf ihr Minimum und ihre Maximum hin untersucht\n# es werden immer die Trainingsdaten verwendet\nstandardisierung.fit(daten_train)\n\n# Transformation der Trainungs- und Testdaten\nX_train_standardisiert = standardisierung.transform(daten_train)\nX_test_standardisiert = standardisierung.transform(daten_test)\n\nprint(X_train_standardisiert)\n\n\n\nAuch hier geht die Pandas-DataFrame-Struktur verloren.","type":"content","url":"/chapter08-sec03#standardisierung","position":17},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter08-sec03#zusammenfassung-und-ausblick","position":18},{"hierarchy":{"lvl1":"8.3 Kodierung und Skalierung","lvl2":"Zusammenfassung und Ausblick"},"content":"Kategoriale Daten müssen kodiert werden, damit sie in einem ML-Algorithmus\nverarbeitet werden können. Geordnete kategoriale (ordinale) Daten können dabei\nüber ein Dictionary und die replace()-Methode kodiert werden. Für ungeordnete\nkategoriale (nominale) Daten muss die One-Hot-Kodierung verwendet werden.\n\nAuch numerische Daten müssen häufig für ML-Algorithmen aufbereitet werden, vor\nallem, wenn die Daten in sehr unterschiedlichen Zahlenbereichen liegen. Bei den\nbisher eingeführten ML-Modellen lineare Regression und Entscheidungsbäumen ist\ndie Skalierung der numerischen Daten nicht notwendig. Erst die nachfolgenden\nML-Modelle werden davon Gebrauch machen.","type":"content","url":"/chapter08-sec03#zusammenfassung-und-ausblick","position":19},{"hierarchy":{"lvl1":"Übungen"},"type":"lvl1","url":"/chapter08-sec04","position":0},{"hierarchy":{"lvl1":"Übungen"},"content":"Aufgabe 1\n\nEine Abalone oder ein Seeohr ist eine Schnecke mit Schale, die einer Ohrmuschel\nähnelt (siehe \n\nSeeohren). Laden Sie den Datensatz\n‘abalone_DE.csv’. Ziel dieser Aufgabe ist ein Modell zu trainieren, das aus den\nAngaben zu Geschlecht, Größe und Gewicht die Anzahl der Ringe prognostiziert.\nDie Anzahl der Ringe +1.5 gibt das Alter der Abalone an.\n\nFühren Sie eine Datenexploration durch. Dazu gehören insbesondere\n\nÜbersicht\n\nstatistische Kennzahlen der Eigenschaften\n\nVisualisierungen der Eigenschaften\n\nAnalyse bzgl. Ausreißer\n\nBereinigen Sie den Datensatz. Dazu gehört insbesondere die Entfernung von\nAusreißern.\n\nWählen Sie ein Modell.\n\nBereiten Sie die Daten für das Modell auf. Dazu gehört insbesondere auch der\nSplit in Trainings- und Testdaten.\n\nValidieren Sie das Modell. Erhöhen Sie die Modellkomplexität und beurteilen\nSie, ob Over- oder Underfitting vorliegt.\n\nKodieren Sie das Geschlecht mit One-Hot-Kodierung und trainieren Sie das\nModell erneut. Verbessert sich die Prognose?\n\nLösungimport pandas as pd\n\ndata = pd.read_csv('abalone_DE.csv', skiprows=3)data.head(10)\n\nAbalone oder Seeohr: \n\nSeeohren\n\nDer Abalone-Datensatz enthält folgende Variablen:\n\nSex → Geschlecht\nLength → Länge (in Millimetern)\nDiameter → Durchmesser (senkrecht zur Länge, in Millimetern)\nHeight → Höhe (mit Fleisch in der Schale, in Millimetern)\nWhole weight → Gesamtgewicht (in Gramm)\nShucked weight → Gewicht geschält (Gewicht Fleisch, in Gramm)\nViscera weight → Gewicht der Eingeweide (in Gramm)\nShell weight → Gewicht der Schale (nach dem Trocknen, in Gramm)\nRings → Ringe (+ 1.5 ergibt das Alter der Abalone)data.info()\n\nDer Abalone-Datensatz enthält 4177 gültige Einträge mit 9 Eigenschaften. Der\nDatentyp des Geschlechts ist Object. Die physikalischen Eigenschaften der\nAbalonen Größe (Länge, Durchmesser und Höhe) und Gewicht (Gesamtgewicht, Gewicht\ngeschält, Gewicht der Eingeweide, Gewicht der Schale) sind als Floats gegeben.\nDie Anzahl der Ringe wird durch einen Integer repräsentiert. Alle Einträge sind\ngültig.\n\nWir untersuchen nacheinander Geschlecht, Größe, Gewicht und Ringe.data['Geschlecht'].describe()data['Geschlecht'].value_counts()\n\nDas Geschlecht enthält drei einzigartige Werte: männlich, Jungtier und weiblich.\nEs sind etwas mehr männliche Abalonen im Datensatz enthalten, Jungtiere und\nweibliche Abalonen gibt es ungefähr gleich viele.import plotly.express as px\n\nfig = px.bar(data['Geschlecht'],\n             title='Anzahl der Abalonen nach Geschlecht',\n             labels={'value': '', 'count': 'Anzahl', 'variable': 'Legende'})\nfig.show()\n\nAls nächstes betrachten wir die Größe der Abalonen.data[['Länge [mm]', 'Durchmesser [mm]', 'Höhe [mm]']].describe()fig = px.box(data[['Länge [mm]', 'Durchmesser [mm]', 'Höhe [mm]']],\n             title='Größe der Abalonen',\n             labels={'variable': '', 'value': 'Größenangabe in mm'})\nfig.show()\n\nBei allen drei Größenangaben gibt es Ausreißer nach unten. Auffällig sind jedoch\ndie beiden Ausreißer bei der Höhe. Die mittlere Höhe sind 0.139516 mm, der\nMedian liegt bei 0.14 mm. Die beiden Ausreißer haben mit 0.515 mm und 1.13 mm\nmehr als die zehnfache Höhe. Um welche Abalonen handelt es sich?data[ data['Höhe [mm]'] > 0.5 ].head(2)\n\nAbalone 1417 scheint einfach nur groß zu sein, bei Abalone 2051 passt die Höhe\naber nicht zu den ansonsten eher durchschnittlichen Größenangaben. Im Folgenden\neliminieren wir die beiden Ausreißer aus dem Datensatz.data = data[ data['Höhe [mm]'] <= 0.5 ]\ndata.info()data[['Länge [mm]', 'Durchmesser [mm]', 'Höhe [mm]']].describe()fig = px.box(data[['Länge [mm]', 'Durchmesser [mm]', 'Höhe [mm]']],\n             title='Größe der Abalonen',\n             labels={'variable': '', 'value': 'Größenangabe in mm'})\nfig.show()\n\nAls nächstes betrachten wir die Gewichtsangaben.data[['Gesamtgewicht [g]', 'Gewicht geschält [g]', 'Gewicht der Eingeweide [g]', 'Gewicht der Schale [g]']].describe()fig = px.box(data[['Gesamtgewicht [g]', 'Gewicht geschält [g]', 'Gewicht der Eingeweide [g]', 'Gewicht der Schale [g]']],\n             title='Gewicht der Abalonen',\n             labels={'variable': '', 'value': 'Gewicht in g'})\nfig.show()\n\nBei den Gewichtsangaben gibt es einige Ausreißer nach oben, aber keine\noffensichtlich herausragenden Ausreißer.\n\nZuletzt verschaffen wir uns noch einen Überblick über die statistischen\nKennzahlen der Ringe.data['Ringe'].describe()fig = px.box(data['Ringe'],\n             title='Ringe der Abalonen',\n             labels={'variable': '', 'value': 'Anzahl'})\nfig.show()fig = px.bar(data['Ringe'].value_counts())\nfig.show()\n\nMittelwert 9.9 und Median 9 stimmen praktisch überein. Das Histogramm der Ringe\nzeigt aber auch eine ganz leichte Rechtsschiefe.\n\nAls nächstes untersuchen wir, welche Wirkung Größe und Gewicht auf die Ringe haben.fig = px.scatter_matrix(data[['Länge [mm]', 'Durchmesser [mm]', 'Höhe [mm]', 'Ringe']])\nfig.show()\n\nJe größer eine Abalone ist, desto mehr Ringe scheint sie zu haben.fig = px.scatter_matrix(data[['Gesamtgewicht [g]', 'Gewicht geschält [g]', 'Gewicht der Eingeweide [g]', 'Gewicht der Schale [g]', 'Ringe']])\nfig.show()\n\nAuch bei steigenden Gewicht scheint die Anzahl der Ringe zuzunehmen.\n\nZuletzt trainieren wir ein multiples lineares Regressionsmodell.from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Split der Daten in Trainingsdaten und Testdaten\ndata_train, data_test = train_test_split(data, random_state=42)\n\n# Wahl des linearen Regressionsmodells\nmodel = LinearRegression()\n\n# Adaption der Daten für das lineare Regressionsmodell\n# Geschlecht wird weggelassen, da kategorial\nX_train = data_train.loc[:, 'Länge [mm]' : 'Gewicht der Schale [g]']\ny_train = data_train['Ringe']\n\n# Training\nmodel.fit(X_train, y_train)\n\n# Validierung\nr2_score_train = model.score(X_train, y_train)\n\nX_test = data_test.loc[:, 'Länge [mm]' : 'Gewicht der Schale [g]']\ny_test = data_test['Ringe']\n\nr2_score_test = model.score(X_test, y_test)\n\nprint(f'R2-Score Trainingsdaten: {r2_score_train:.2f}')\nprint(f'R2-Score Testdaten: {r2_score_test:.2f}')\n\nBei den Trainingsdaten und bei den Testdaten erzielen wir R²-Scores von ca. 0.55\n(leicht variierend je nach zufälliger Aufteilung in Test- und Trainingsdaten).\nDamit scheint die gewählte Modellkomplexität für die Daten zu gering zu sein.\nWir erhöhen die Modellkomplexität, gehen also zu einer polynomialen Regression\nüber. Zu jedem Polynomgrad betrachten wir den jeweiligen R²-Score für\nTrainingsdaten und Testdaten.from sklearn.preprocessing import PolynomialFeatures\n\nfor d in [2, 3, 4, 5]:\n    polynom_transformator = PolynomialFeatures(degree = d)\n\n    # Adaption der Daten für das lineare Regressionsmodell\n    X_train = polynom_transformator.fit_transform(data_train.loc[:, 'Länge [mm]' : 'Gewicht der Schale [g]'])\n    y_train = data_train['Ringe']\n\n    # Training\n    model.fit(X_train, y_train)\n\n    # Validierung\n    r2_score_train = model.score(X_train, y_train)\n\n    X_test = polynom_transformator.transform(data_test.loc[:, 'Länge [mm]' : 'Gewicht der Schale [g]'])\n    y_test = data_test['Ringe']\n\n    r2_score_test = model.score(X_test, y_test)\n\n    print(f'Grad: {d} ==> R2-Score Trainingsdaten: {r2_score_train:.2f} | R2-Score Testdaten: {r2_score_test:.2f}')\n\nDer Wechsel von einem linearen auf ein quadratisches Regressionspolynom scheint\neine leichte Verbesserung des R2-Wertes zu bringen. Aber bereits bei Grad 3\nfällt der R²-Score bei den Testdaten auf 0.25 ab, während er für die\nTrainingsdaten auf 0.61 steigt. Damit befinden wir uns bereits im Overfitting. Bei\nGrad 4 und 5 sind die R²-Scores sogar negativ. Daher darf allerhöchstens Grad 2\nfür das Regressionspolynom gewählt werden. Da aber die Verbesserung von Grad 1\nauf 2 nur marginal ist, ist weiterhin das lineare multiple Regressionsmodell\nempfehlenswert.\n\nNun beziehen wir noch das Geschlecht ein. Dazu kodieren wir es mit\nOne-Hot-Kodierung und trainieren dann das lineare multiple Regressionsmodell\nerneut.# One-Hot-Kodierung des Geschlechts\ndata_kodiert = pd.get_dummies(data, columns=['Geschlecht'])\n\n # Erneuter Split (mit gleichem random_state für Vergleichbarkeit)\ndata_train, data_test = train_test_split(data_kodiert, random_state=42)\n\n# Training mit Geschlecht\nX_train = data_train.drop(columns=['Ringe'])\ny_train = data_train['Ringe']\nmodel.fit(X_train, y_train)\n\n# Validierung\nX_test = data_test.drop(columns=['Ringe'])\ny_test = data_test['Ringe']\nr2_score_test = model.score(X_test, y_test)\nprint(f'R2-Score mit Geschlecht: {r2_score_test:.2f}')\n\nDer R²-Score ist bei den Testdaten minimal von 0.56 auf 0.57 gestiegen. Eine\nsignifikante Verbesserung des Modells ist es nicht, das Geschlecht mit\nhinzuzunehmen.\n\nAufgabe 2\n\nDer Datensatz\n‘statistic_id226994_annual-average-unemployment-figures-for-germany-2005-2022.csv’\nstammt von Statista. Die Daten beschreiben die Entwicklung der\nArbeitslosenzahlen (in Mio.) seit 1991. Im Original-Excel sind einige\nUngereimtheiten, die sich auch so im csv-File befinden.\n\nKorrigieren Sie den Datensatz zuerst mit einem Texteditor.\n\nFühren Sie dann eine explorative Datenanalyse durch (Übersicht, statistische\nKennzahlen, Boxplot und Visualisierung der Arbeitslosenzahlen abhängig vom\nJahr.)\n\nWählen Sie mehrere ML-Modelle aus. Adaptieren Sie die Daten für das Training\nund lassen Sie die gewählten ML-Modelle trainieren.\n\nValidieren Sie Ihr Modell: ist es geeignet? Bewerten Sie die Modelle bzgl.\nOver- und Underfitting.\n\nVisualisieren Sie eine Prognose von 1990 bis 2030.\n\nLösungimport pandas as pd\n\ndata = pd.read_csv('statistic_id226994_korrigiert.csv')\ndata.info()\n\nDer Datensatz enthält 32 Zeilen und zwei Spalten. Das Jahr wird durch Integer repräsentiert, die Arbeitslosenzahl durch Floats.einzigartige_jahre = len(data['Jahr'].unique())\nprint(f'Es sind {einzigartige_jahre} verschiedene Jahreszahlen gelistet.')\n\nDie Arbeitslosenzahlen sind Floats und alle Einträge sind gültig.data.describe()import plotly.express as px\n\nfig = px.box(data['Arbeitslosenzahl'],\n             title='Arbeitslosenzahlen in Deutschland von 1991 bis 2022',\n             labels={'variable':'', 'value': 'Mio. Personen'})\nfig.show()\n\nDer Median liegt mittig zwischen Q1 und Q3, aber insgesamt gibt es mehrere\nJahre, bei denen mehr als 4 Mio. Arbeitslose (Q3) gibt. Es gibt keine Ausreißer.\n\nAls nächstes visualisieren wir die Arbeitslosenzahlen abhängig vom Jahr.fig = px.scatter(data, x = 'Jahr', y = 'Arbeitslosenzahl',\n             title='Arbeitslosenzahlen in Deutschland von 1991 bis 2022')\nfig.show()\n\nAufgrund der Visualisierung ist abzusehen, dass das lineare Regressionsmodell\nkein gutes Modell sein wird. Wir probieren polynomiale Regression für Grad 1 bis\n10 und notieren den jeweiligen R²-Score.from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\n# Auswahl des Modells\nmodel = LinearRegression()\n\n\n# Adaption der Daten\ndata_train, data_test = train_test_split(data, random_state=42)\n\nX_train = data_train[['Jahr']]\ny_train = data_train['Arbeitslosenzahl']\n\nX_test = data_test[['Jahr']]\ny_test = data_test['Arbeitslosenzahl']\n\n# Training für Polynomgrad 1 bis 5\nfor grad in [1, 2, 3, 4, 5]:\n    polynom_transformator = PolynomialFeatures(degree = grad)\n\n    X_train_transformiert =  polynom_transformator.fit_transform(X_train)\n    model.fit(X_train_transformiert, y_train)\n\n    # Validierung mit Testdaten\n    r2_score_train = model.score(X_train_transformiert, y_train)\n    X_test_transformiert = polynom_transformator.transform(X_test)\n    r2_score_test  = model.score(X_test_transformiert, y_test)\n\n    # Vergleich der Modelle\n    print(f'Grad {grad}: R2-Score Trainingsdaten: {r2_score_train:.2f} \\t R2-Score Testdaten: {r2_score_test}')\n\nAb Grad 3 gibt es keine Veränderung mehr. Der R²-Score für die Testdaten ist\ndeutlich kleiner als für die Trainingsdaten.  Dies deutet darauf hin, dass das Modell bereits anfängt zu overfitten. Dennoch wählen wir Grad 3, da höhere Grade zu noch schlechteren Testscores führen.\n# Wähle Grad 3\npolynom_transformator = PolynomialFeatures(degree = 3)\n\nX_train_transformiert =  polynom_transformator.fit_transform(X_train)\nmodel.fit(X_train_transformiert, y_train)\n\n# Validierung mit Testdaten\nr2_score_train = model.score(X_train_transformiert, y_train)\nX_test_transformiert = polynom_transformator.transform(X_test)\nr2_score_test  = model.score(X_test_transformiert, y_test)\n\n# Vergleich der Modelle\nprint(f'Grad 3: R2-Score Trainingsdaten: {r2_score_train:.2f} \\t R2-Score Testdaten: {r2_score_test}')import numpy as np\nimport plotly.graph_objects as go\n\n# Prognose für die Jahre 1990 bis 2030\nprognose = pd.DataFrame()\nprognose['Jahr'] = np.arange(1990, 2031)\n\nX_prognose = polynom_transformator.transform(prognose[['Jahr']])\nprognose['Arbeitslosenzahl'] = model.predict(X_prognose)\n\nfig1 = px.scatter(data, x = 'Jahr', y = 'Arbeitslosenzahl')\nfig2 = px.line(prognose, x = 'Jahr', y = 'Arbeitslosenzahl')\n\nfig = go.Figure(fig1.data + fig2.data)\nfig.show()\n\nDa das von uns gewählte Modell ein Polynom dritten Grades ist, schwingt es für\nJahre nach 2020 wieder nach oben. Dieses Verhalten wird nicht von den\nTrainingsdaten gestützt. Daher sollte das Modell nicht für eine Prognose in die\nZukunft genutzt werden.","type":"content","url":"/chapter08-sec04","position":1},{"hierarchy":{"lvl1":"9.1 Grundideen der Ensemble-Methoden"},"type":"lvl1","url":"/chapter09-sec01","position":0},{"hierarchy":{"lvl1":"9.1 Grundideen der Ensemble-Methoden"},"content":"Eins, zwei, viele ... im Bereich des maschinellen Lernens sind Ensemble-Methoden\nleistungsstarke Techniken zur Verbesserung der Modellgenauigkeit und Robustheit.\nDiese Methoden kombinieren mehrere Modelle, um die Gesamtleistung zu steigern,\nindem sie die individuellen Stärken der Modelle nutzen und deren Schwächen\nausgleichen. In diesem Kapitel werden wir die grundlegenden Konzepte und\nUnterschiede zwischen diesen Methoden erläutern, um ein besseres\nVerständnis ihrer Funktionsweise und Anwendungen zu vermitteln.","type":"content","url":"/chapter09-sec01","position":1},{"hierarchy":{"lvl1":"9.1 Grundideen der Ensemble-Methoden","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter09-sec01#lernziele","position":2},{"hierarchy":{"lvl1":"9.1 Grundideen der Ensemble-Methoden","lvl2":"Lernziele"},"content":"Lernziele\n\nSie können in eigenen Worten erklären, was Ensemble-Methoden sind.\n\nSie kennen die Grundideen der Ensemble-Methoden\n\nVoting,\n\nAveraging,\n\nStacking,\n\nBagging und\n\nBoosting.","type":"content","url":"/chapter09-sec01#lernziele","position":3},{"hierarchy":{"lvl1":"9.1 Grundideen der Ensemble-Methoden","lvl2":"Ensemble-Methoden"},"type":"lvl2","url":"/chapter09-sec01#ensemble-methoden","position":4},{"hierarchy":{"lvl1":"9.1 Grundideen der Ensemble-Methoden","lvl2":"Ensemble-Methoden"},"content":"Der Begriff »Ensemble« wird im Allgemeinen eher mit Musik und Kunst in\nVerbindung gebracht als mit Informatik. In der Musik bezeichnet ein Ensemble\neine kleine Gruppe von Musikern, die entweder das gleiche Instrument spielen\noder verschiedene Instrumente kombinieren. Im Theater bezeichnet man eine Gruppe\nvon Schauspielern ebenfalls als Ensemble, und in der Architektur beschreibt der\nBegriff eine Gruppe von Gebäuden, die in einem besonderen Zusammenhang\nzueinander stehen.\n\nAuch im Bereich des maschinellen Lernens hat sich der Begriff Ensemble\netabliert. Mit Ensemble-Methoden (Ensemble Learning) wird eine Gruppe von\nmaschinellen Modellen bezeichnet, die zusammen eine Prognose treffen sollen.\nÄhnlich wie bei Musik-Ensembles können bei den Ensemble-Methoden entweder\nidentische Modelle oder verschiedene Modelle kombiniert werden. Diese Modelle\nkönnen entweder gleichzeitig eine Prognose treffen, die dann kombiniert wird,\noder nacheinander verwendet werden, wobei ein nachfolgendes Modell die Fehler\ndes vorherigen korrigiert. Je nach Vorgehensweise unterscheidet man im\nmaschinellen Lernen zwischen Voting, Averaging, Stacking,\nBagging und Boosting.\n\nIn dieser Vorlesung konzentrieren wir uns auf Bagging und Boosting mit ihren\nbekanntesten Vertretern, den Random Forests und XGBoost. Die Konzepte Voting und\nAveraging sowie Stacking werden hier nur kurz ohne weitere Details vorgestellt.\nEine allgemeine Einführung in Ensemble-Methoden mit Scikit-Learn findet sich in\nder \n\nDokumentation Scikit-Learn →\nEnsemble.","type":"content","url":"/chapter09-sec01#ensemble-methoden","position":5},{"hierarchy":{"lvl1":"9.1 Grundideen der Ensemble-Methoden","lvl2":"Voting, Averaging und Stacking"},"type":"lvl2","url":"/chapter09-sec01#voting-averaging-und-stacking","position":6},{"hierarchy":{"lvl1":"9.1 Grundideen der Ensemble-Methoden","lvl2":"Voting, Averaging und Stacking"},"content":"\n\nDie Prognosen von mehreren unterschiedlichen ML-Modellen werden zu einer\nfinalen Prognose kombiniert. Die Kombination kann beispielsweise durch\nMehrheitsentscheidung (Voting), aber auch Mittelwertbildung (Averaging)\nerfolgen.\n\nIn einem ersten Schritt werden mehrere ML-Modelle unabhängig voneinander auf den\nTrainingsdaten trainiert. Jedes dieser Modelle liefert eine Prognose, die dann\nauf verschiedene Arten miteinander kombiniert werden können. Bei\nKlassifikationsaufgaben ist Voting, also die Wahl durch\nMehrheitsentscheidung, eine beliebte Methode, um die Einzelprognosen zu\nkombinieren. Wurden beispielsweise drei ML-Modelle gewählt, die jeweils ja oder\nnein prognostizieren, dann wird für die finale Prognose das Ergebnis genommen,\ndas die Mehrheit der einzelnen Modelle vorausgesagt hat. Scikit-Learn bietet\ndafür einen Voting Classifier an, siehe \n\nDokumentation Scikit-Learn → Voting\nClassifier.\n\nBei Regressionsaufgaben werden die einzelnen Prognosen häufig gemittelt. Beim\nAveraging kann entweder der übliche arithmetische Mittelwert verwendet\nwerden oder ein gewichteter Mittelwert, was als Weighted Averaging bezeichnet\nwird. Dennoch wird die Mittelwertbildung bei Regressionsaufgaben von\nScikit-Learn ebenfalls als Voting bezeichnet, siehe \n\nDokumentation Scikit-Learn\n→ Voting\nRegressor.\n\nEine alternative Kombinationsmethode ist die Verwendung eines weiteren\nML-Modells. In diesem Fall werden die Modelle, die die einzelnen Prognosen\nliefern, als Basismodelle bezeichnet. Die Basismodelle liefern Features für ein\nweiteres ML-Modell, das als Meta-Modell bezeichnet wird. Diese Ensemble-Methode\nwird Stacking genannt. Weitere Informationen liefert die \n\nScikit-Learn\nDokumentation → Stacked\nGeneralization.\n\nStacking bietet viele Vorteile. Der wichtigste Vorteil ist, dass die\nPrognosefähigkeit des Gesamtmodells in der Regel deutlich besser ist als die der\neinzelnen Basismodelle. Die Stärken der Basismodelle werden kombiniert und die\nSchwächen ausgeglichen. Allerdings erfordert Stacking sehr viel Feinarbeit. Auch\nsteigt die Trainingszeit für das Gesamtmodell, selbst wenn die Basismodelle bei\ngenügend Rechenleistung parallel trainiert werden können. Aus diesem Grund\nwerden wir in dieser Vorlesung kein Stacking verwenden.","type":"content","url":"/chapter09-sec01#voting-averaging-und-stacking","position":7},{"hierarchy":{"lvl1":"9.1 Grundideen der Ensemble-Methoden","lvl2":"Bagging"},"type":"lvl2","url":"/chapter09-sec01#bagging","position":8},{"hierarchy":{"lvl1":"9.1 Grundideen der Ensemble-Methoden","lvl2":"Bagging"},"content":"\n\nBeim Bagging wird das gleiche ML-Modell auf unterschiedlichen Stichproben der\nTrainingsdaten trainiert (Bootstrapping). Die Einzelprognosen der Modelle werden\ndann zu einer finalen Prognose kombiniert (Aggregating).\n\nBagging ist eine Ensemble-Methode, bei der stets dasselbe Modell für die\nEinzelprognosen verwendet wird. Die Unterschiede in den Einzelprognosen\nentstehen dadurch, dass für das Training der einzelnen Modelle unterschiedliche\nDaten verwendet werden.\n\nIm ersten Schritt werden zufällige Datenpunkte aus den Trainingsdaten ausgewählt\nund in einen neuen Datensatz, „Stichprobe 1“, aufgenommen. Nachdem ein\nDatenpunkt ausgewählt wurde, kehrt er in die ursprüngliche Menge der\nTrainingsdaten zurück und kann erneut ausgewählt werden. Dieser Prozess wird in\nder Mathematik als Ziehen mit Zurücklegen bezeichnet, auf Englisch\nBootstrapping. Durch Bootstrapping werden dann noch weitere Stichproben\ngebildet, wobei jede Stichprobe üblicherweise die gleiche Anzahl an Datenpunkten\nwie der ursprüngliche Datensatz enthält.\n\nIm zweiten Schritt wird ein ML-Modell gewählt und für jede Bootstrap-Stichprobe\ntrainiert. Da die Stichproben unterschiedliche Trainingsdaten enthalten,\nentstehen unterschiedlich trainierte Modelle, die für neue Daten verschiedene\nEinzelprognosen liefern. Diese Einzelprognosen werden kombiniert bzw. nach\nfestgelegten Regeln zu einer finalen Prognose zusammengefasst. In der Statistik\nwird die Zusammenfassung von Daten als Aggregation bezeichnet. Auf Englisch\nheißt der Vorgang des Zusammenfassens Aggregating.\n\nDie beiden wesentlichen Schritte der Bagging-Methode sind also Bootstrapping\nund Aggregating, woraus sich die Abkürzung »Bagging« ableitet.\nScikit-Learn bietet sowohl für Klassifikations- als auch für Regressionsaufgaben\neine allgemeine Implementierung der Bagging-Methode an (siehe \n\nDokumentation\nScikit-Learn →\nBagging).\nDie bekannteste Bagging-Methode ist Random Forests, bei dem\nEntscheidungsbäume (Decision Trees) auf unterschiedlichen Stichproben trainiert\nund aggregiert werden. Random Forests werden wir im nächsten Kapitel\ndetaillierter betrachten. Vorab beschäftigen wir uns noch mit dem Konzept der\nBoosting-Methoden.","type":"content","url":"/chapter09-sec01#bagging","position":9},{"hierarchy":{"lvl1":"9.1 Grundideen der Ensemble-Methoden","lvl2":"Boosting"},"type":"lvl2","url":"/chapter09-sec01#boosting","position":10},{"hierarchy":{"lvl1":"9.1 Grundideen der Ensemble-Methoden","lvl2":"Boosting"},"content":"\n\nDer Fehler in der Prognose wird benutzt, um das nächste Modell zu trainieren.\nBeim hier gezeigten Adaboost-Verfahren werden die Daten neu gewichtet, beim\n(Stochastic) Gradient Boosting werden Modelle zur Fehlerkorrektur trainiert.\n\nDas englische Verb „to boost sth.“ hat viele Bedeutungen. Insbesondere wird es\nim Deutschen mit „etwas verstärken“ übersetzt. Im Kontext des maschinellen\nLernens bezeichnet Boosting eine Ensemble-Methode, bei der mehrere ML-Modelle\nhintereinander geschaltet werden, um die Genauigkeit der Prognose zu verstärken.\nDie Idee des Boosting besteht darin, dass jedes Modell die Fehler des\nVorgängermodells reduziert. Es gibt mehrere Varianten zur Fehlerreduktion, aus\ndenen sich unterschiedliche Boosting-Methoden ableiten. Die wichtigsten\nVarianten sind:\n\nAdaboost,\n\nGradient Boosting und\n\nStochastic Gradient Boosting.\n\nBeim Adaboost-Verfahren wird im ersten Schritt ein Modell (z.B. ein\nEntscheidungsbaum) auf den Trainingsdaten trainiert. Anschließend werden die\nPrognosen dieses Modells mit den tatsächlichen Werten verglichen. Im zweiten\nSchritt wird ein neuer Datensatz erstellt, wobei die falsch prognostizierten\nDatenpunkte ein größeres Gewicht erhalten. Nun wird erneut ein Modell trainiert;\nund dessen Prognosen werden wieder mit den echten Werten verglichen. Dieser\nVorgang wird mehrfach wiederholt. Das Training der Modelle erfolgt sequentiell,\nda jedes Vorgängermodell die neue Gewichtung der Trainingsdaten liefert. Am Ende\nwerden alle Einzelprognosen gewichtet zu einer finalen Prognose kombiniert.\nWeitere Details finden sich in der \n\nDokumentation Scikit-Learn →\nAdaboost.\n\nBeim Gradient Boosting wird ebenfalls ein sequentieller Ansatz verfolgt,\naber der Fokus liegt auf der Minimierung der Fehler. Im ersten Schritt wird ein\nML-Modell (häufig ein Entscheidungsbaum) trainiert. Danach wird für jeden\nDatenpunkt der Fehler des Modells, das sogenannte Residuum, berechnet, indem\ndie Differenzen zwischen dem tatsächlichen Wert und den Prognosen bestimmt wird.\nIm nächsten Schritt wird ein neues Modell trainiert, das darauf abzielt, diese\nResiduen vorherzusagen. Dieses neue Modell wird dann zu dem vorherigen Modell\nhinzugefügt, um die Gesamtprognose zu verbessern. Dieser Prozess wird\nwiederholt, wobei in jeder Iteration ein neues Modell trainiert wird, das die\nFehler der bisherigen Modelle reduziert (mit Hilfe einer Verlustfunktion und\neines Gradientenverfahrens). Am Ende ergibt sich eine starke Vorhersage, indem\nalle Modelle kombiniert werden. Da häufig Entscheidungsbäume als Modell gewählt\nwerden, bietet Scikit-Learn eine Implementierung der sogenannten\nGradient-Boosted Decision Trees an, siehe \n\nDokumentation Scikit-Learn →\nGradient-boosted\ntrees.\n\nStochastic Gradient Boosting ist eine Erweiterung des Gradient Boosting, bei\nder zusätzlich Stochastik eingeführt wird. Hierbei wird in jedem Schritt nur\neine zufällige Stichprobe der Trainingsdaten verwendet, um ein Modell zu\ntrainieren. Der Trainingsprozess ähnelt dem von Gradient Boosting, wobei in\njeder Runde ein neues Modell trainiert wird, das die Fehler der vorherigen\nModelle korrigiert. Durch die zufällige Auswahl der Trainingsdaten in jeder\nIteration wird eine höhere Robustheit gegenüber Overfitting (Überanpassung)\nerreicht. Stochastic Gradient Boosting wird von Scikit-Learn nicht direkt\nunterstützt. Eine sehr bekannte Implementierung davon ist XGBoost (siehe\n\n\nhttps://​xgboost​.readthedocs​.io/),\ndie wir in einem der nächsten Kapitel noch näher betrachten werden.","type":"content","url":"/chapter09-sec01#boosting","position":11},{"hierarchy":{"lvl1":"9.1 Grundideen der Ensemble-Methoden","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter09-sec01#zusammenfassung-und-ausblick","position":12},{"hierarchy":{"lvl1":"9.1 Grundideen der Ensemble-Methoden","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Kapitel haben Sie die Konzepte Voting, Averaging, Stacking, Bagging\nund Boosting kennengelernt. Alle Methoden sind Ensemble-Methoden, bei denen\nmehrere ML-Modelle parallel oder sequentiell kombiniert werden. Obwohl diese\nEnsemble-Methoden allgemein für verschiedene ML-Modelle eingesetzt werden\nkönnen, haben sich vor allem Random Forests (Bagging für Entscheidungsbäume) und\nStochastic Gradient Boosting als besonders effektiv erwiesen. Letztere sind\nnicht in Scikit-Learn implementiert, sondern werden durch eine eigene Bibliothek\nnamens XGBoost bereitgestellt. In den nächsten beiden Kapiteln werden wir beide\nauch mit praktischen Beispielen vertiefen.","type":"content","url":"/chapter09-sec01#zusammenfassung-und-ausblick","position":13},{"hierarchy":{"lvl1":"9.2 Random Forests"},"type":"lvl1","url":"/chapter09-sec02","position":0},{"hierarchy":{"lvl1":"9.2 Random Forests"},"content":"Im letzten Kapitel haben wir verschiedene Ensemble-Methoden in der Theorie\nkennengelernt: Stacking, Bagging und Boosting. Für die beiden letzteren\nEnsemble-Methoden werden besonders häufig Entscheidungsbäume (Decision Trees)\neingesetzt. Daher betrachten wir in diesem Kapitel die praktische Umsetzung von\nBagging mit Entscheidungsbäumen, die sogenannten Random Forests.","type":"content","url":"/chapter09-sec02","position":1},{"hierarchy":{"lvl1":"9.2 Random Forests","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter09-sec02#lernziele","position":2},{"hierarchy":{"lvl1":"9.2 Random Forests","lvl2":"Lernziele"},"content":"Lernziele\n\nSie können das ML-Modell Random Forest in der Praxis anwenden.\n\nSie können mit Hilfe der Feature Importance bewerten, wie groß der\nEinfluss eines Merkmals auf die Prognosegenauigkeit des Random Forests ist.","type":"content","url":"/chapter09-sec02#lernziele","position":3},{"hierarchy":{"lvl1":"9.2 Random Forests","lvl2":"Random Forests mit Scikit-Learn"},"type":"lvl2","url":"/chapter09-sec02#random-forests-mit-scikit-learn","position":4},{"hierarchy":{"lvl1":"9.2 Random Forests","lvl2":"Random Forests mit Scikit-Learn"},"content":"Entscheidungsbäume (Decision Trees) haben wir bereits betrachtet. Sie sind\naufgrund ihrer Einfachheit und vor allem aufgrund ihrer Interpretierbarkeit sehr\nbeliebt. Allerdings ist ihre Tendenz zum Overfitting problematisch. Daher\nkombinieren wir die Ensemble-Methode Bagging mit Entscheidungsbäumen (Decision\nTrees). Indem aus den Trainingsdaten zufällig Bootstrap-Stichproben ausgewählt\nwerden, erhalten wir unterschiedliche Entscheidungsbäume (Decision Trees).\nZusätzlich wird beim Training der Entscheidungsbäume nicht mit allen Merkmalen\n(Features) trainiert. Bei jedem Split eines Baumes wird nur eine zufällige\nTeilmenge der Merkmale betrachtet, um die beste Trennung zu finden. Durch diese\nzwei Maßnahmen wird die Anpassung der Entscheidungsbäume an die Trainingsdaten\n(Overfitting) reduziert.\n\nUm den Random Forest von Scikit-Learn praktisch auszuprobieren, erzeugen wir\nkünstliche Daten. Dazu verwenden wir die Funktion make_moons von Scikit-Learn,\ndie Zufallszahlen generiert und interpretieren die Zufallszahlen als\nKilometerstände und Preise von Autos bei einer fiktiven Verkaufsaktion.\nZusätzlich lassen wir zufällig Nullen und Einsen erzeugen, die wir als\n»verkauft« oder »nicht verkauft« interpretieren.\n\nimport pandas as pd \nfrom sklearn.datasets import make_moons\n\n# Erzeugung künstlicher Daten\nX_array, y_array = make_moons(n_samples=120, random_state=0, noise=0.3)\n\ndaten = pd.DataFrame({\n    'Kilometerstand [km]': 10000 * (X_array[:,0] + 2),\n    'Preis [EUR]': 5000 * (X_array[:,1] + 2),\n    'verkauft': y_array,\n    })\n\n# Adaption der Daten\nX = daten[['Kilometerstand [km]', 'Preis [EUR]']].values\ny = daten['verkauft'].values\n\n\n\nDiesmal werden in dem Autohaus 120 Autos zum Verkauf angeboten (siehe Option\nn_samples=120). Nach Aktionsende werden die Merkmale Kilometerstand und Preis\ntabellarisch erfasst und notiert, ob das Auto verkauft wurde (True bzw. 1) oder\nnicht verkauft wurde (False bzw. 0). Wir visualisieren die Daten.\n\nimport plotly.express as px\n# plot artificial data\nfig = px.scatter(daten, x = 'Kilometerstand [km]', y = 'Preis [EUR]', color=daten['verkauft'].astype(bool),\n        title='Künstliche Daten: Verkaufsaktion Autohaus',\n        labels = {'x': 'Kilometerstand [km]', 'y': 'Preis [EUR]', 'color': 'verkauft'})\nfig.show()\n\n\n\nNachdem wir die Vorbereitungen für die Daten abgeschlossen haben, können wir\nScikit-Learn einen Random Forest trainieren lassen. Dazu importieren wir den\nAlgorithmus aus dem Modul ensemble. Da der Random Forest ein Ensemble von\nEntscheidungsbäumen (Decision Trees) ist, haben wir nun die Möglichkeit, die\nAnzahl der Entscheidungsbäume festzulegen. Voreingestellt sind 100\nEntscheidungsbäume. Aus didaktischen Gründen reduzieren wir diese Anzahl auf\nvier und setzen das Argument n_estimators= auf 4. Ebenfalls aus didaktischen\nGründen fixieren wir die Zufallszahlen, mit Hilfe derer das Bootstrapping und\ndie Auswahl der Merkmale (Features) umgesetzt wird, mit random_state=0. In\neinem echten Projekt würden wir das unterlassen. Zuletzt führen wir das Training\nmit der .fit()-Methode durch. Weitere Details finden Sie unter \n\nScikit-Learn\nDokumentation →\nRandomForestClassifier.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_random_forest = RandomForestClassifier(n_estimators=4, random_state=0)\nmodel_random_forest.fit(X,y)\n\n\n\nAls nächstes lassen wir den Random Forest für jeden Punkt des Gebiets\nprognostizieren, ob ein Auto mit diesem Kilometerstand und diese Preis\nverkaufbar wäre oder nicht.\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nmy_colormap = ListedColormap(['#EF553B33', '#636EFA33'])\nfig = DecisionBoundaryDisplay.from_estimator(model_random_forest, X,  cmap=my_colormap)\nfig.ax_.scatter(X[:,0], X[:,1], c=y, cmap=my_colormap)\nfig.ax_.set_xlabel('Kilometerstand [km]');\nfig.ax_.set_ylabel('Preis [EUR]');\nfig.ax_.set_title('Random Forest: Entscheidungsgrenzen');\n\n\n\nMöchte man die vier Entscheidungsbäume (Decision Trees) analysieren, aus denen\nder Random Forest kombiniert wurde, kann man mit dem Attribut estimators_\ndarauf zugreifen. Wir lassen uns jetzt die Entscheidungsgrenzen einzeln für\njeden Entscheidungsbaum anzeigen.\n\nmy_colormap = ListedColormap(['#EF553B33', '#636EFA33'])\nfor (nummer, baum) in zip(range(4), model_random_forest.estimators_):\n    fig = DecisionBoundaryDisplay.from_estimator(baum, X,  cmap=my_colormap)\n    fig.ax_.scatter(X[:,0], X[:,1], c=y, cmap=my_colormap)\n    fig.ax_.set_xlabel('Kilometerstand [km]');\n    fig.ax_.set_ylabel('Preis [EUR]');\n    fig.ax_.set_title(f'Entscheidungsbaum {nummer+1}');\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/chapter09-sec02#random-forests-mit-scikit-learn","position":5},{"hierarchy":{"lvl1":"9.2 Random Forests","lvl2":"Feature Importance"},"type":"lvl2","url":"/chapter09-sec02#feature-importance","position":6},{"hierarchy":{"lvl1":"9.2 Random Forests","lvl2":"Feature Importance"},"content":"Der Random Forest reduziert das Overfitting und ist damit für zukünftige\nPrognosen besser gerüstet, verliert aber seine leichte Interpretierbarkeit. Ein\ngroßer Vorteil des Entscheidungsbaumes (Decision Trees) ist ja, dass wir die\nEntscheidungen als eine Abfolge von Entscheidungsfragen gut nachvollziehen\nkönnen. Jeder der einzelnen Entscheidungsbäume kommt jedoch zu einer anderen\nReihenfolge der Entscheidungsfragen und zu anderen Grenzen. Dafür bietet der\nRandom-Forest-Algorithmus eine alternative Bewertung, wie wichtig einzelne\nMerkmale (Features) sind, die sogenannte Feature Importance.\n\nFeature Importance bewertet, wie wichtig der Einfluss eines Merkmals (Features)\nauf die Prognoseleistung ist. Ist die Feature Importance eines Merkmals\n(Features) höher, so trägt dieses Merkmal (Feature) auch mehr zu der Genauigkeit\nder Prognose bei. Bei Entscheidungsbäumen wird für jedes Merkmal (Feature)\nberechnet, wie groß die Reduktion der Gini-Impurity ist. Gibt es ein Merkmal,\ndas eindeutig die Gini-Impurity reduziert, dann hat dieses Merkmal auch einen\ngroßen Einfluss auf die Prognosefähigkeit des Modells. Wir könnten nach dem\nTraining des Entscheidungsbaumes zusammenfassen, wie oft und wieviel ein\nbestimmtes Merkmal zur Reduktion beiträgt. In der Praxis kommt es aber oft vor,\ndass bei einem Split mehrere Merkmale gleichermaßen die Gini-Impurity\nreduzieren. Dann wird eines der Merkmale zufällig ausgewählt. Daher kann es\nschwierig sein, bei einem Entscheidungsbaum die Feature Importance zu bewerten.\nBei einem Random Forest hingegen werden viele Entscheidungsbäume trainiert. Wenn\nwir jetzt bei allen Entscheidungsbäumen die Feature Importance berechnen und den\nMittelwert bilden, erhalten wir ein aussagekräftiges Bewertungskriterium, wie\nstark einzelne Merkmale die Prognosefähigkeit beeinflussen.\n\nWir trainieren nun einen Random Forest mit der Standardeinstellung von 100\nEntscheidungsbäumen und lassen uns dann die Feature Importance ausgeben.\n\nmodel = RandomForestClassifier(random_state=0)\nmodel.fit(X,y)\n\nprint(model.feature_importances_)\n\n\n\nDer erste Wert gibt die Feature Importance für das erste Merkmal an und der zweite Wert entsprechend für das zweite Merkmal. Es ist üblich, die Feature Importance als Balkendiagramm zu visualisieren.\n\nfeature_importances = pd.Series(model.feature_importances_, index=['Kilometerstand [km]', 'Preis [EUR]'])\n\nfig = px.bar(feature_importances, orientation='h',\n  title='Verkaufsaktion im Autohaus', \n  labels={'value':'Feature Importance', 'index': 'Merkmal'})\nfig.update_traces(showlegend=False) \nfig.show()\n\n\n\nDer Preis ist demnach wichtiger als der Kilometerstand (wobei es hier ja ein\nkünstliches Beispiel ist).","type":"content","url":"/chapter09-sec02#feature-importance","position":7},{"hierarchy":{"lvl1":"9.2 Random Forests","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter09-sec02#zusammenfassung-und-ausblick","position":8},{"hierarchy":{"lvl1":"9.2 Random Forests","lvl2":"Zusammenfassung und Ausblick"},"content":"Random Forests sind einfachen Entscheidungsbäumen vorzuziehen, da sie das\nOverfitting reduzieren. Die Erzeugung der einzelnen Entscheidungsbäume kann\nparallelisiert werden, so dass das Training eines Random Forests sehr schnell\ndurchgeführt werden kann. Auch für große Datenmengen mit sehr unterschiedlichen\nEigenschaften arbeitet der Random Forest sehr effizient. Er ermöglicht auch eine\nInterpretation, welche Eigenschaften ggf. einen größeren Einfluss haben als\nandere Eigenschaften.","type":"content","url":"/chapter09-sec02#zusammenfassung-und-ausblick","position":9},{"hierarchy":{"lvl1":"9.3 XGBoost"},"type":"lvl1","url":"/chapter09-sec03","position":0},{"hierarchy":{"lvl1":"9.3 XGBoost"},"content":"In der bisherigen Vorlesung haben wir vor allem Pandas und Scikit-Learn benutzt.\nZwar bietet Scikit-Learn Boosting-Verfahren an, in vielen Wettbewerben hat sich\njedoch eine andere Bibliothek durchgesetzt, die eine optimierte Variante des\nStochastic Gradient Boosting anbietet: XGBoost.\n\nWarnung\n\nFalls bei Ihnen XGBoost nicht installiert sein sollte, folgen Sie bitte den\nAnweisungen auf der Internetseite\n\n\nhttps://​xgboost​.readthedocs​.io\nund installieren Sie XGBoost jetzt nach.","type":"content","url":"/chapter09-sec03","position":1},{"hierarchy":{"lvl1":"9.3 XGBoost","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter09-sec03#lernziele","position":2},{"hierarchy":{"lvl1":"9.3 XGBoost","lvl2":"Lernziele"},"content":"Lernziele\n\nSie können XGBoost für Regressions- und Klassifikationsaufgaben einsetzen.\n\nSie wissen, wie Sie mit Analysen der Maßzahlen Fehler und Log Loss für\nTrainings- und Testdaten beurteilen können, ob Überanpassung (Overfitting)\nvorliegt.\n\nSie kennen die Methode Frühes Stoppen zur Reduzierung der Überanpassung\n(Overfitting).\n\nSie wissen, dass XGBoost nicht manuell feinjustiert werden sollte, sondern mit\nGittersuche oder weiteren Bibliotheken (z.B. Optuna).","type":"content","url":"/chapter09-sec03#lernziele","position":3},{"hierarchy":{"lvl1":"9.3 XGBoost","lvl2":"XGBoost benutzt Scikit-Learn API"},"type":"lvl2","url":"/chapter09-sec03#xgboost-benutzt-scikit-learn-api","position":4},{"hierarchy":{"lvl1":"9.3 XGBoost","lvl2":"XGBoost benutzt Scikit-Learn API"},"content":"In einem früheren Kapitel haben wir Stochastic Gradient Boosting theoretisch\nkennengelernt: Dabei werden sequentiell Modelle trainiert, die jeweils die\nFehler des Vorgängermodells korrigieren. XGBoost ist eine hochoptimierte und\nerweiterte Implementierung dieses Verfahrens.\n\nXGBoost steht für eXtreme Gradient Boosting und ist aus\nPerformancegründen in der Programmiersprache C++ implementiert. Für\nPython-Programmierer wurde ein Python-Modul mit dem Ziel geschaffen, die\ngleichen Schnittstellen wie Scikit-Learn anzubieten, so dass kaum\nEinarbeitungszeit in eine neue Bibliothek erforderlich ist. Vor allem benötigen\nData Scientists auch keine C++-Programmierkenntnisse, sondern können weiterhin\nmit Python arbeiten.\n\nWir bleiben bei unserem Beispiel mit der Verkaufsaktion im Autohaus aus dem\nvorherigen Kapitel.\n\nimport pandas as pd \nfrom sklearn.datasets import make_moons\n\n# Erzeugung künstlicher Daten\nX_array, y_array = make_moons(n_samples=120, random_state=0, noise=0.3)\n\ndaten = pd.DataFrame({\n    'Kilometerstand [km]': 10000 * (X_array[:,0] + 2),\n    'Preis [EUR]': 5000 * (X_array[:,1] + 2),\n    'verkauft': y_array,\n    })\n\n\n\nXGBoost kann Pandas DataFrames nicht verarbeiten, sondern benötigt die reinen\nZahlenwerte in Form von Matrizen. Das ist in der Tat kein Problem, denn die\nDatenstruktur DataFrame stellt die reinen Matrizen über die Methode .values\ndirekt zur Verfügung.\n\n# Adaption der Daten\nX = daten[['Kilometerstand [km]', 'Preis [EUR]']].values\ny = daten['verkauft'].values\n\n\n\nAls nächstes importieren wir XGBoost. Es ist üblich, das ganze Modul zu\nimportieren und mit xgb abzukürzen. Danach initialisieren wir das\nKlassifikationsmodell XGBClassifier und trainieren es auf den Daten.\n\nimport xgboost as xgb \n\nmodell = xgb.XGBClassifier()\nmodell.fit(X,y)\n\n\n\nAls nächstes visualisieren wir die Prognose des trainierten\nXGBoost-Klassifikators.\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nmy_colormap = ListedColormap(['#EF553B33', '#636EFA33'])\nfig = DecisionBoundaryDisplay.from_estimator(modell, X,  cmap=my_colormap)\nfig.ax_.scatter(X[:,0], X[:,1], c=y, cmap=my_colormap)\nfig.ax_.set_xlabel('Kilometerstand [km]');\nfig.ax_.set_ylabel('Preis [EUR]');\nfig.ax_.set_title('XGBoost: Entscheidungsgrenzen');\n\n\n\nDie Entscheidungsgrenzen sehen sehr plausibel aus.\n\nGenau wie beim Random Forest können wir uns die Feature Importance ausgeben\nlassen.\n\n# Feature Importance wie bei Random Forest\nimport plotly.express as px\n\nfeature_importance = pd.Series(\n    modell.feature_importances_, \n    index=['Kilometerstand', 'Preis']\n)\n\nfig = px.bar(feature_importance, orientation='h',\n    title='Feature Importance bei XGBoost',\n    labels={'value': 'Wichtigkeit', 'index': 'Merkmal'})\nfig.update_traces(showlegend=False)\nfig.show()\n\n\n\nDer Preis (0.57) hat eine um den Faktor 1.3 höhere Feature Importance als der\nKilometerstand (0.43). Allerdings liegen hier aus didaktischen Gründen nur\nkünstliche Daten vor, so dass diese Erkenntnis nicht auf die reale Welt\nverallgemeinerbar ist.","type":"content","url":"/chapter09-sec03#xgboost-benutzt-scikit-learn-api","position":5},{"hierarchy":{"lvl1":"9.3 XGBoost","lvl2":"XGBoost neigt stark zur Überanpassung (Overfitting)"},"type":"lvl2","url":"/chapter09-sec03#xgboost-neigt-stark-zur-beranpassung-overfitting","position":6},{"hierarchy":{"lvl1":"9.3 XGBoost","lvl2":"XGBoost neigt stark zur Überanpassung (Overfitting)"},"content":"XGBoost neigt zu Überanpassung (Overfitting), weil es sehr komplexe Modelle\ndurch Hinzufügen vieler Bäume erstellt, die sich immer stärker an die\nTrainingsdaten anpassen. Um das an unserem Beispiel mit der Verkaufsaktion im\nAutohaus zu zeigen, fügen wir noch neue, unbekannte Testdaten hinzu. Dazu\nverdoppeln wir die Anzahl der Autos (n_samples=2000).\n\n# Erzeugung künstlicher Daten\nX_array, y_array = make_moons(n_samples=2000, random_state=0, noise=0.3)\n\ndaten = pd.DataFrame({\n    'Kilometerstand [km]': 10000 * (X_array[:,0] + 2),\n    'Preis [EUR]': 5000 * (X_array[:,1] + 2),\n    'verkauft': y_array,\n    })\n\nX = daten[['Kilometerstand [km]', 'Preis [EUR]']].values\ny = daten['verkauft'].values\n\n\n\nAnschließend teilen wir die 2000 Autos in zwei Gruppen: Trainings- und\nTestdaten.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.5, random_state=0)\n\n\n\nDiesmal legen wir explizit fest, aus wie vielen Modellen das Boosting-Verfahren\nbestehen soll. Dazu setzen wir n_estimators=200. Oft wird auch von der Anzahl\nder »Boosting-Runden« gesprochen. Das Training auf den Trainingsdaten liefert\nein sehr gutes Ergebnis:\n\nimport xgboost as xgb\n\nmodell = xgb.XGBClassifier(n_estimators=200)\n\nmodell.fit(X_train, y_train)\n\nscore_train = modell.score(X_train, y_train)\nprint(f'Score bezogen auf Trainingsdaten: {score_train:.2f}')\nscore_test = modell.score(X_test, y_test)\nprint(f'Score bezogen auf Testdaten: {score_test:.2f}')\n\n\n\nDie Trainingsdaten werden perfekt prognostiziert. Auch bei den Testdaten\nerhalten wir ein gutes Ergebnis, das aber im Vergleich zu dem sehr guten Score\nbei den Trainingsdaten abfällt. Es fällt schwer, zu entscheiden, ob eine\nÜberanpassung (Overfitting) vorliegt. XGBoost ist ein iteratives Verfahren.\nZunächst wird Modell Nr. 1 trainiert, darauf aufbauend Modell Nr. 2 usw. Wir\nwiederholen jetzt das Training des XGBoost-Klassifikators, aber lassen durch ein\nweiteres Argument mitprotokollieren, was in den einzelnen Boosting-Runden (=\nIterationen) passiert.\n\nZuerst legen wir fest, welche internen Bewertungskennzahlen (= Metrik, Maßzahl)\nmitprotokolliert werden sollen. Wir wählen als erste Maßzahl den Fehler, also\ndie relative Anzahl der falsch klassifizierten Autos. Die zweite Maßzahl ist die\nLog Loss, die nicht nur bewertet, ob die Klassifikation richtig ist, sondern\nauch wie sicher das Modell bei seiner Vorhersage ist.\n\nTechnisch setzen wir dies um, indem wir bei der Initialisierung des\nXGBoost-Modells das optionale Argument eval_metric=['error', 'logloss']\nsetzen.\n\nmodell = xgb.XGBClassifier(n_estimators=200, eval_metric=['error', 'logloss'])\n\n\n\nAllerdings ist damit noch nicht festgelegt, auf welchen Daten die Fehler-Maßzahl\nund die Log-Loss-Maßzahl berechnet werden. Zunächst sollen beide Maßzahlen für\ndie Trainingsdaten berechnet werden, dann für die Testdaten. Das erreichen wir\nmit dem optionalen Argument eval_set=, dem wir folgendermaßen die Trainings-\nund Testdaten mitgeben.\n\nmodell.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=False)\n\n\n\nWir setzen noch verbose=False, damit nicht für jedes Modell bzw. jede\nBoosting-Runde die vier Maßzahlen auf dem Bildschirm ausgegeben werden. Nach dem\nTraining können wir die vier Maßzahlen mit der Methode .evals_result() aus dem\ntrainierten Modell extrahieren. Um die Maßzahlen zu visualisieren, packen wir\nsie in einen Pandas-DataFrame.\n\nmasszahlen = modell.evals_result()\nmetriken = pd.DataFrame({\n    'Fehler Train': masszahlen['validation_0']['error'],\n    'Fehler Test': masszahlen['validation_1']['error'],\n    'Log Loss Train': masszahlen['validation_0']['logloss'],\n    'Log Loss Test': masszahlen['validation_1']['logloss']\n    })\n\n\n\nWir visualisieren Fehler und Log Loss getrennt voneinander.\n\nimport plotly.express as px\n\n# Fehler plotten\nfig = px.line(metriken[['Fehler Train', 'Fehler Test']],\n    title='Fehler in jeder Boosting-Runde',\n    labels={'value': 'Fehler', 'index': 'Boosting-Runde', 'variable': 'Legende'})\nfig.show()\n\n# Log Loss plotten\nfig = px.line(metriken[['Log Loss Train', 'Log Loss Test']],\n    title='Log Loss in jeder Boosting-Runde',\n    labels={'value': 'Log Loss', 'index': 'Boosting-Runde', 'variable': 'Legende'})\nfig.show()\n\n\n\n\n\nDer Fehler bei den Trainingsdaten wird von Boosting-Runde zu Boosting-Runde\nkleiner, aber der Fehler der Testdaten wächst. Zunächst wird der Fehler der\nTestdaten kleiner, erreicht in Minimum in der 6. Boosting-Runde, um dann wieder zu\nsteigen. Dieses Verhalten ist typisch für Überanpassung (Overfitting). Etwas\ndeutlicher wird dieses Phänomen, wenn wir uns die (transformierte) Differenz\nder Wahrscheinlichkeiten ansehen, die Log-Loss-Maßzahl.\n\nAm kleinsten ist die Log-Loss-Maßzahl für die Boosting-Runde 9, danach steigt\ndie Log-Loss-Maßzahl wieder an. Am besten wäre es nach dieser Analyse gewesen,\nnach der 6. oder 9. Boosting-Runde aufzuhören, da dann die Überanpassung\n(Overfitting) an die Trainingsdaten einsetzt.","type":"content","url":"/chapter09-sec03#xgboost-neigt-stark-zur-beranpassung-overfitting","position":7},{"hierarchy":{"lvl1":"9.3 XGBoost","lvl2":"Bekämpfen von Überanpassung (Overfitting)"},"type":"lvl2","url":"/chapter09-sec03#bek-mpfen-von-beranpassung-overfitting","position":8},{"hierarchy":{"lvl1":"9.3 XGBoost","lvl2":"Bekämpfen von Überanpassung (Overfitting)"},"content":"Es gibt einige Hyperparameter von XGBoost, die helfen, Überanpassung\n(Overfitting) zu reduzieren. Eine Möglichkeit ist es, früher zu stoppen und\nnicht die voreingestellte Anzahl an Modellen bzw. Boosting-Runden (Iterationen)\nzu durchlaufen. Das wird durch das optionale Argument early_stopping_rounds=\nim Konstruktor ermöglicht. Die Zahl, die diesem Parameter übergeben wird, gibt\ndie Anzahl der Boosting-Runden vor, nach denen gestoppt wird, falls sich kaum\netwas an der Maßzahl geändert hat.\n\nWichtig: In der Praxis sollte early stopping nicht auf den Testdaten erfolgen,\nsondern auf einem separaten Validierungsset, um Data Leakage zu vermeiden. Für\ndieses didaktische Beispiel verwenden wir vereinfacht die Testdaten.\n\nmodell = xgb.XGBClassifier(n_estimators=200, early_stopping_rounds=10, eval_metric=['error', 'logloss'])\nmodell.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=False)\n\n\n\nVisualisiert sieht die Log-Loss-Statistik für das obige Beispiel so aus:\n\nmasszahlen = modell.evals_result()\nmetriken = pd.DataFrame({\n    'Fehler Train': masszahlen['validation_0']['error'],\n    'Fehler Test': masszahlen['validation_1']['error'],\n    'Log Loss Train': masszahlen['validation_0']['logloss'],\n    'Log Loss Test': masszahlen['validation_1']['logloss']\n    })\n\nfig = px.line(metriken[['Fehler Train', 'Fehler Test']],\n    title='Frühes Stoppen: Fehler',\n    labels={'value': 'Fehler', 'index': 'Boosting-Runde', 'variable': 'Legende'})\nfig.show()\n\nfig = px.line(metriken[['Log Loss Train', 'Log Loss Test']],\n    title='Frühes Stoppen: Log Loss',\n    labels={'value': 'Log Loss', 'index': 'Boosting-Runde', 'variable': 'Legende'})\nfig.show()\n\nprint(f'Training gestoppt nach {modell.best_iteration + 1} Boosting-Runde(n)')\nprint(f'Bester Score auf Testdaten: {modell.best_score}')\n\n\n\n\n\n\n\nEine weitere Möglichkeit, Überanpassung (Overfitting) zu reduzieren, besteht\ndarin, die Tiefe der Entscheidungsbäume zu begrenzen. Wir benutzen\nEntscheidungsbaum-Stümpfe, die nur einen Split haben. Das erreichen wir mit dem\noptionalen Argument max_depth=1.\n\nmodell = xgb.XGBClassifier(max_depth=1, n_estimators=200, eval_metric=['error', 'logloss'])\nmodell.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=False)\n\nmasszahlen = modell.evals_result()\nmetriken = pd.DataFrame({\n    'Fehler Train': masszahlen['validation_0']['error'],\n    'Fehler Test': masszahlen['validation_1']['error'],\n    'Log Loss Train': masszahlen['validation_0']['logloss'],\n    'Log Loss Test': masszahlen['validation_1']['logloss']\n    })\n\nfig = px.line(metriken[['Fehler Train', 'Fehler Test']],\n    title='Begrenzte Entscheidungsbäume: Fehler',\n    labels={'value': 'Fehler', 'index': 'Boosting-Runde', 'variable': 'Legende'})\nfig.show()\n\nfig = px.line(metriken[['Log Loss Train', 'Log Loss Test']],\n    title='Begrenzte Entscheidungsbäume: Log Loss',\n    labels={'value': 'Log Loss', 'index': 'Boosting-Runde', 'variable': 'Legende'})\nfig.show()\n\n\n\n\n\nEs gibt noch einige weitere Hyperparameter, die für “das” beste Modell\nfeinjustiert werden können. Händisch gelingt es kaum, alle Hyperparameter\noptimal einzustellen, so dass hier eine Gittersuche oder gar eine Bibliothek wie\n\n\nOptuna eingesetzt werden sollte. Das\nübersteigt jedoch den zeitlichen Rahmen dieser Vorlesung und wird daher hier\nnicht behandelt.","type":"content","url":"/chapter09-sec03#bek-mpfen-von-beranpassung-overfitting","position":9},{"hierarchy":{"lvl1":"9.3 XGBoost","lvl3":"Vergleich: Random Forest vs. XGBoost","lvl2":"Bekämpfen von Überanpassung (Overfitting)"},"type":"lvl3","url":"/chapter09-sec03#vergleich-random-forest-vs-xgboost","position":10},{"hierarchy":{"lvl1":"9.3 XGBoost","lvl3":"Vergleich: Random Forest vs. XGBoost","lvl2":"Bekämpfen von Überanpassung (Overfitting)"},"content":"Zum Abschluss dieses Kapitels beschäftigen wir uns noch mit einem Vergleich der beiden Verfahren Random Forest und XGBoost.\n\nAspekt\n\nRandom Forest\n\nXGBoost\n\nTraining\n\nparallel\n\nsequentiell\n\nOverfitting\n\nweniger anfällig\n\nstark anfällig\n\nHyperparameter-Tuning\n\nwenig nötig\n\nintensiv nötig\n\nGeschwindigkeit\n\nschnell\n\nlangsamer\n\nTypische Genauigkeit\n\ngut\n\nsehr gut\n\nRandom Forests eignen sich gut für einen schnellen ersten Ansatz, während\nXGBoost durch sorgfältiges Tuning oft bessere Ergebnisse liefert, aber mehr\nAufwand erfordert.","type":"content","url":"/chapter09-sec03#vergleich-random-forest-vs-xgboost","position":11},{"hierarchy":{"lvl1":"9.3 XGBoost","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter09-sec03#zusammenfassung-und-ausblick","position":12},{"hierarchy":{"lvl1":"9.3 XGBoost","lvl2":"Zusammenfassung und Ausblick"},"content":"Mit XGBoost haben Sie ein ML-Modell für das überwachte Lernen kennengelernt, das\nin den vergangenen Jahren sehr viele Wettbewerbe beispielsweise auf der\nPlattform Kaggle gewonnen hat. Die Mächtigkeit der Algorithmen führt aber häufig\nzur Überanpassung (Overfitting), so dass die sorgsame Feinjustierung der\nHyperparameter besonders wichtig ist.","type":"content","url":"/chapter09-sec03#zusammenfassung-und-ausblick","position":13},{"hierarchy":{"lvl1":"Übungen"},"type":"lvl1","url":"/chapter09-sec04","position":0},{"hierarchy":{"lvl1":"Übungen"},"content":"Der California-Housing-Datensatz ist ein klassischer Datensatz für\nRegressionsaufgaben im maschinellen Lernen. Er wurde 1990 aus dem US-Census\nextrahiert und enthält Daten zu Wohnblöcken in Kalifornien. Ein Wohnblock\nist eine statistische Einheit mit typischerweise 600-3000 Einwohnern. Die\nMerkmale sind Durchschnittswerte für alle Haushalte in diesem Block. Das Ziel\nist, den Median-Hauswert (in US-Dollar) vorherzusagen.\n\nÜberblick über die Daten\n\nLaden Sie den Datensatz california_housing_DE.csv.\n\nWelche Daten enthält der Datensatz?\n\nWie viele Datenpunkte und Merkmale gibt es?\n\nSind die Daten vollständig?\n\nWelche Datentypen haben die Merkmale?\n\nGibt es Merkmale mit unerwarteten Datentypen?\n\nLösungimport pandas as pd\n\n# Laden der Daten und erste Informationen\ndaten = pd.read_csv('california_housing_DE.csv')\ndaten.info()\n\n# Check der Vollständigkeit\nprint(daten.isnull().sum())\n\n# Inhalt der ersten 10 Zeilen\ndaten.head(10)\n\nDer Datensatz enthält 20.640 Datenpunkte (Wohnblöcke) mit 10 Merkmalen. Die Daten\nsind nicht vollständig, denn beim Merkmal Schlafzimmer fehlen 207 Einträge. Die\nübrigen Merkmale sind vollständig. Die Datentypen der Merkmale sind wie folgt:\n\nLaenge --> float64,\n\nBreite --> float64,\n\nAlter --> float64,\n\nZimmer --> float64,\n\nSchlafzimmer --> float64,\n\nBevoelkerung --> float64,\n\nHaushalte --> float64,\n\nEinkommen --> float64,\n\nHauswert --> float64 und\n\nKuestennaehe --> object.\n\nDie Datentypen sind für die Merkmale angemessen.\n\nDatenvorverarbeitung\n\nDer Datensatz enthält fehlende Werte in der Spalte Schlafzimmer. Entscheiden\nSie, wie Sie diese behandeln möchten. Begründen Sie Ihre Wahl. Führen Sie die\ngewählte Methode durch.\n\nLösung\n\nEs fehlen 207 Werte (ca. 1 % des Datensatzes). Da es sich um eine numerische\nSpalte handelt und der Anteil der fehlenden Werte gering ist, ist eine\nImputation mit dem Median sinnvoll. Der Median ist robust gegenüber Ausreißern\nund verändert die Verteilung der Daten kaum.\n\nDie Imputation führen wir wie folgt durch:# Median berechnen\nmedian_schlafzimmer = daten['Schlafzimmer'].median()\nprint(f'Median Schlafzimmer: {median_schlafzimmer:.1f} ')\n\n# fehlende Werte ersetzen\ndaten['Schlafzimmer'] = daten['Schlafzimmer'].fillna(median_schlafzimmer)\n\n# Überprüfen, ob keine fehlenden Werte mehr vorhanden sind\nprint(f\"Fehlende Werte nach Imputation: {daten['Schlafzimmer'].isna().sum()}\")\n\nNach der Imputation sind keine fehlenden Werte mehr in der Spalte Schlafzimmer\nvorhanden.\n\nAlternativen wären: Zeilen löschen (bei nur 1 % vertretbar), Mittelwert (aber\nweniger robust) oder komplexere Imputation mit ML-Modellen (hier übertrieben\naufwendig).\n\nStatistik der numerischen Daten\n\nErstellen Sie eine Übersicht der statistischen Kennzahlen für die numerischen\nDaten. Interpretieren Sie die statistischen Kennzahlen. Gibt es Auffälligkeiten?\nSind die Werte plausibel?\n\nLösungimport plotly.express as px\n\n# Laenge, Breite wird weggelassen, da die Positionen weniger gut statistisch\n# interpretiert werden kann; Hauswert wird gesondert untersucht, da es die \n# Zielgröße ist \nfig = px.box(daten.loc[:, 'Alter':'Einkommen'],\n             labels={'variable': 'Merkmal', 'value': 'Wert'})\n\nfig.show()\n\n# Statistik für alle Merkmale und Zielgröße\ndaten.describe()\n\nBeobachtungen:\n\nAlter: reicht von 1 bis 52 Jahren, durchschnittlich 28.6 Jahre\n\nZimmer: sehr große Spannweite (2 bis 39320), Median (2127) deutlich unter\nMittelwert (2635), d.h. rechtsschiefe Verteilung, Ausreißer vorhanden\n\nSchlafzimmer: ähnliches Bild wie bei Zimmer\n\nBevoelkerung: Maximum von 35682 deutet auf sehr große Bezirke hin\n\nEinkommen: reicht von 0.5 bis 15 (Einheit: 10.000 USD)\n\nHauswert: Maximum von 500001 ist auffällig, vermutlich werden Werte über\n500.000 USD gekappt\n\nDie Werte erscheinen grundsätzlich plausibel, zeigen aber deutliche Ausreißer.\n\nStatistik der kategorialen Merkmale\n\nWie viele Beobachtungen gibt es pro Kategorie in Küstennähe? Visualisieren\nSie die Verteilung mit einem Balkendiagramm. Ist der Datensatz ausgewogen?\n\nLösunghaeufigkeiten = daten['Kuestennaehe'].value_counts()\nprint(haeufigkeiten)\n\nfig = px.bar(x=haeufigkeiten.index, y=haeufigkeiten.values,\n             labels={'x': 'Küstennähe', 'y': 'Anzahl'},\n             title='Verteilung der Kuestennähe-Kategorien')\nfig.show()\n\nDer Datensatz ist nicht ausgewogen. Die Kategorie ‘weniger_als_1h_zum_Ozean’\ndominiert mit über 9000 Einträgen, gefolgt von ‘Binnenland’ mit über 6000\nEinträgen. ‘nahe_Bucht’ und ‘nahe_Ozean’ haben deutlich weniger Einträge, und\n‘Insel’ ist mit nur 5 Beobachtungen extrem unterrepräsentiert.\n\nAnalyse der Zielgröße\n\nVisualisieren Sie die Verteilung der Zielgröße Hauswert mit einem\nHistogramm. Ist die Verteilung normalverteilt? Gibt es Auffälligkeiten?\n\nLösungfig = px.histogram(daten, x='Hauswert', \n                   nbins=50,\n                   title='Verteilung der Hauswerte')\nfig.update_layout(\n    yaxis_title='Anzahl',\n    xaxis_title='Hauswert (USD)'\n)\nfig.show()\n\nDie Verteilung ist rechtsschief und nicht normalverteilt. Auffällig ist der\ngroße Balken bei 500.001 USD. Hier wurden offensichtlich alle Werte über 500.000\nUSD auf diesen Maximalwert gesetzt. Dies ist eine Besonderheit des Datensatzes,\ndie man bei der Interpretation der Ergebnisse berücksichtigen sollte.\n\nKorrelationen\n\nErstellen Sie eine Korrelationsmatrix für alle numerischen Variablen und\nvisualisieren Sie diese als Heatmap. Welche Merkmale korrelieren am stärksten\nmit dem Hauswert? Gibt es starke Korrelationen zwischen den Merkmalen?\n\nLösungnumerische_daten = daten.loc[:, 'Laenge' : 'Hauswert']\nkorrelation = numerische_daten.corr()\n\nfig = px.imshow(korrelation, \n                text_auto='.2f',\n                aspect='auto',\n                labels={'color': 'Korrelation'},\n                title='Korrelationsmatrix')\nfig.show()\n\nEine häufig verwendete Faustregel für die Interpretation des\nKorrelationskoeffizienten r ist\n\n|r| < 0.3: schwacher Zusammenhang\n\n0.3 ≤ |r| < 0.7: mittelstarker Zusammenhang\n\n|r| ≥ 0.7: starker Zusammenhang\n\nDas Einkommen korreliert am stärksten mit dem Hauswert (r = 0.69). Alle anderen\nMerkmale zeigen keine bzw. nur einen sehr schwachen Zusammenhang zum Hauswert\n(Werte zwischen -0.14 bis 0.13).\n\nBei den Merkmalen untereinander gibt es starke Korrelationen zwischen den\nZimmern, Schlafzimmern, Bevölkerung und Haushalten (0.86 bis 0.93).\n\nAnalyse der Positionen\n\nErstellen Sie einen Scatterplot mit Längengrad (x-Achse) und Breitengrad\n(y-Achse). Färben Sie die Punkte nach Hauswert ein. Erkennen Sie geografische\nMuster?\n\nTipp: Benutzen Sie das optionale Argument opacity=0.5, damit knapp\nübereinanderliegende Punkte dennoch sichtbar werden.\n\nLösungfig = px.scatter(daten, \n                 x='Laenge', \n                 y='Breite',\n                 color='Hauswert',\n                 labels={'Laenge': 'Längengrad',\n                        'Breite': 'Breitengrad',\n                        'Hauswert': 'Hauswert (USD)'},\n                 title='Geografische Verteilung der Hauswerte',\n                 opacity=0.5\n                )\nfig.show()\n\nDie Form von Kalifornien ist deutlich erkennbar. Die höchsten Hauswerte sind in\nden Küstenregionen (Vergleich mit Atlas: San Francisco Bay Area und Los\nAngeles/San Diego). Im Binnenland sind die Hauswerte niedriger.\n\nDie geografische Lage ist ein wichtiger Einflussfaktor auf den Hauswert. Der\nEinfluss ist allerdings nichtlinear, denn sonst hätten wir in der\nKorrelationsmatrix nicht r = -0.05 für die Laenge und r = -0.14 für die Breite\nerhalten.\n\nNeue Merkmale\n\nWir haben festgestellt, dass die Merkmale Zimmer, Schlafzimmer, Bevölkerung und\nHaushalte stark linear korreliert sind.\n\nErstellen Sie folgende neue Merkmale:\n\nZimmer pro Haushalt\n\nSchlafzimmer pro gesamte Anzahl Zimmer\n\nPersonen pro Haushalt\n\nDürfen die drei Divisionen durchgeführt werden? Berechnen Sie die Korrelation\ndieser neuen Merkmale mit der Zielgröße.\n\nLösung\n\nDie statische Analyse ergab, dass das Minimum der Haushalte 1 ist und die\nminimale Anzahl der Zimmer 2. Null kommt nicht vor, so dass alle drei Divisionen\ndurchgeführt werden können.# Erstellung neue Merkmale als zusätzliche Spalten\ndaten['Zimmer_pro_Haushalt'] = daten['Zimmer'] / daten['Haushalte']\ndaten['Schlafzimmer_Anteil'] = daten['Schlafzimmer'] / daten['Zimmer']\ndaten['Personen_pro_Haushalt'] = daten['Bevoelkerung'] / daten['Haushalte']\n\n# Auswahl der Merkmale für Korrelationsmatrix\nneue_daten = daten.loc[:, ['Zimmer_pro_Haushalt', 'Schlafzimmer_Anteil', 'Personen_pro_Haushalt', 'Hauswert' ]]\n\n# Berechnung und Anzeige Korrelationsmatrix\nneue_korrelationen = neue_daten.corr()\nprint(neue_korrelationen)\n\n# Visualisierung Korrelationsmatrix als Heatmap\nfig = px.imshow(neue_korrelationen, \n                text_auto='.2f',\n                aspect='auto',\n                labels={'color': 'Korrelation'},\n                title='Neue Korrelationsmatrix')\nfig.show()\n\nDie neuen Merkmale zeigen stärkere Korrelationen als die ursprünglichen\nMerkmale. Besonders der Schlafzimmer_Anteil könnte ein nützliches Merkmal zur\nPrognose des Hauswertes sein.\n\nVorbereitung der Daten für das Training\n\nBereiten Sie die Daten für das maschinelle Lernen vor:\n\nKodieren Sie die kategoriale Variable Küstennähe mit One-Hot-Encoding.\n\nTrennen Sie die Merkmale (Input X) und die Zielgröße (Output y).\n\nTeilen Sie die Daten in Trainings- und Testdaten im Verhältnis 80:20 auf.\n\nLösungfrom sklearn.model_selection import train_test_split\n\n# One-Hot-Encoding für Kuestennaehe\ndaten = pd.get_dummies(data=daten, columns=['Kuestennaehe'])\ndaten.head()\n\n# Input und Output trennen\nX = daten.drop('Hauswert', axis=1)\ny = daten['Hauswert']\n\n# Train-Test-Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f'Trainingsdaten: {X_train.shape[0]} Datenpunkte')\nprint(f'Testdaten: {X_test.shape[0]} Datenpunkte')\nprint(f'Anzahl Merkmale: {X_train.shape[1]}')\n\nML-Modell: Entscheidungsbaum\n\nTrainieren Sie einen Entscheidungsbaum (DecisionTreeRegressor) auf den\nTrainingsdaten. Berechnen Sie den R²-Score auf den Trainings- und Testdaten. Was\nfällt Ihnen auf?\n\nLösungfrom sklearn.tree import DecisionTreeRegressor\n\nmodell_baum = DecisionTreeRegressor(random_state=42)\nmodell_baum.fit(X_train, y_train)\n\nr2_train_baum = modell_baum.score(X_train, y_train)\nr2_test_baum = modell_baum.score(X_test, y_test)\n\nprint(f'R²-Score Entscheidungsbaum (Training): {r2_train_baum:.2f}')\nprint(f'R²-Score Entscheidungsbaum (Test): {r2_test_baum:.2f}')\n\nDer R²-Score auf den Trainingsdaten ist nahezu perfekt (1.0), während der Score\nauf den Testdaten deutlich niedriger ist (0.6). Dies ist ein klares Zeichen für\nOverfitting. Das Modell hat die Trainingsdaten auswendig gelernt, kann aber\nschlecht auf neue Daten verallgemeinern.\n\nML-Modell: Random Forest\n\nTrainieren Sie nun einen Random Forest (RandomForestRegressor) mit 100 Bäumen\nauf denselben Daten. Berechnen Sie wieder die R²-Scores für Training und Test.\nWie unterscheiden sich die Ergebnisse vom einzelnen Entscheidungsbaum?\n\nLösungfrom sklearn.ensemble import RandomForestRegressor\n\nmodell_rf = RandomForestRegressor(n_estimators=100, random_state=42)\nmodell_rf.fit(X_train, y_train)\n\nr2_train_rf = modell_rf.score(X_train, y_train)\nr2_test_rf = modell_rf.score(X_test, y_test)\n\nprint(f'R²-Score Random Forest (Training): {r2_train_rf:.2f}')\nprint(f'R²-Score Random Forest (Test): {r2_test_rf:.2f}')\n\nDer Random Forest zeigt weniger Overfitting, er lässt sich besser\nverallgemeinern. Der R²-Score für die Trainingsdaten ist etwas geringer (0.97),\naber dafür liegt der R²-Score der Testdaten bei 0.81. Die Differenz zwischen\ndem Trainings- und dem Testscore ist kleiner.\n\nFeature Importance\n\nLassen Sie sich die Feature Importance des Random-Forest-Modells ausgeben und\nvisualisieren Sie die Top 10 wichtigsten Merkmale als Balkendiagramm. Welche\nMerkmale sind am wichtigsten für die Vorhersage?\n\nLösung# Feature Importance extrahieren\nimportance = pd.DataFrame({\n    'Merkmal': X_train.columns,\n    'Feature Importance': modell_rf.feature_importances_\n}).sort_values('Feature Importance', ascending=False)\n\n# Top 10 visualisieren\ntop10 = importance.head(10)\nfig = px.bar(top10, x='Feature Importance', y='Merkmal', orientation='h',\n             title='Top 10 wichtigste Merkmale im Random Forest')\nfig.update_yaxes(autorange=\"reversed\")\nfig.show()\n\nprint(top10)\n\nDas wichtigste Merkmal aus den vorhandenen Merkmalen ist das Einkommen. Mit\netwas Abstand kommen dann die Lage (Küstennähe Binnenland) und die Anzahl der\nPersonen pro Haushalt.\n\nML-Modell: lineare Regression\n\nTrainieren Sie nun ein lineares Regressionsmodell auf denselben Daten. Berechnen\nSie wieder die R²-Scores für Training und Test. Wie unterscheiden sich die\nErgebnisse vom den beiden vorherigen Modellen?\n\nWählen Sie dann ein finales Modell.\n\nLösungfrom sklearn.linear_model import LinearRegression\n\nmodell_lr = LinearRegression()\nmodell_lr.fit(X_train, y_train)\n\nr2_train_lr = modell_lr.score(X_train, y_train)\nr2_test_lr = modell_lr.score(X_test, y_test)\n\nprint(f'R²-Score lineare Regression (Training): {r2_train_lr:.2f}')\nprint(f'R²-Score lineare Regression (Test): {r2_test_lr:.2f}')\n\nDer R²-Score der Trainingsdaten ist 0.66, für die Testdaten liegt er bei 0.58.\nAuch wenn die Differenz zwischen Trainingsscore und Testscore klein ist und wohl\nkein Overfitting vorliegt, sind die beiden Scores erheblich schlechter als beim\nRandom Forest (0.97 und 0.81). Daher verwenden wir den Random Forest als finales\nModell.","type":"content","url":"/chapter09-sec04","position":1},{"hierarchy":{"lvl1":"10.1 Maximiere den Rand, aber soft"},"type":"lvl1","url":"/chapter10-sec01","position":0},{"hierarchy":{"lvl1":"10.1 Maximiere den Rand, aber soft"},"content":"Wir bleiben weiter bei den klassischen ML-Methoden und beschäftigen uns in\ndiesem Kapitel mit den Support Vector Machines. Zunächst jedoch ergründen wir\ndas Konzept, das hinter den Support Vector Machines steht.","type":"content","url":"/chapter10-sec01","position":1},{"hierarchy":{"lvl1":"10.1 Maximiere den Rand, aber soft","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter10-sec01#lernziele","position":2},{"hierarchy":{"lvl1":"10.1 Maximiere den Rand, aber soft","lvl2":"Lernziele"},"content":"Lernziele\n\nSie kennen die Abkürzung SVM für Support Vector Machines.\n\nSie kennen die Idee, bei Support Vector Machines den Margin (=\nRandabstand) zu maximieren.\n\nSie wissen, was Stützvektoren bzw. Support Vectors sind.\n\nSie wissen, dass ein harter Randabstand nur bei linear trennbaren Datensätzen\nmöglich ist.\n\nSie wissen, dass eigentlich nicht trennbare Datensätze mit der Technik Soft\nMargin (= weicher Randabstand) dennoch klassifiziert werden können.","type":"content","url":"/chapter10-sec01#lernziele","position":3},{"hierarchy":{"lvl1":"10.1 Maximiere den Rand, aber soft","lvl2":"Welche Trenngerade soll es sein?"},"type":"lvl2","url":"/chapter10-sec01#welche-trenngerade-soll-es-sein","position":4},{"hierarchy":{"lvl1":"10.1 Maximiere den Rand, aber soft","lvl2":"Welche Trenngerade soll es sein?"},"content":"Support Vector Machines (SVM) können sowohl für Klassifikations- als auch\nRegressionsprobleme genutzt werden. Insbesondere wenn viele Merkmale (Features)\nvorliegen, sind SVMs gut geeignet. Auch neigen SVMs weniger zu Overfitting.\nDaher lohnt es sich, Support Vector Machines anzusehen.\n\nWarum SVMs weniger zu Overfitting neigen und mit Ausreißern besser umgehen\nkönnen, sehen wir bereits an der zugrundeliegenden Idee, die hinter dem\nVerfahren steckt. Um das Basis-Konzept der SVMs zu erläutern, erzeugen wir\nkünstliche Messdaten. Dazu verwenden wir die Funktion make_blobs aus dem\nScikit-Learn-Modul Datasets. Weitere Details zum Aufruf der Funktion finden\nwir in der\n\n\nScikit​-Learn​-Dokumentation​/make​_blobs.\n\nfrom sklearn.datasets import make_blobs\n\n# generate artificial data\nX, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.50)\n\nprint(X)\nprint(y)\n\n\n\nDie Funktion make_blobs erzeugt standardmäßig zwei Input-Features, da die\nOption n_features auf den Wert 2 voreingestellt ist, und einen Output, bei dem\ndie Labels entweder durch 0 oder 1 gekennzeichnet sind. Durch die Option\nrandom_state=0 wird der Zufall ausgeschaltet.\n\nWenn wir die Daten in ein Pandas-DataFrame packen und anschließend\nvisualisieren, erhalten wir folgenden Plot.\n\nimport pandas as pd \nimport plotly.express as px\n\ndaten = pd.DataFrame({\n    'Feature 1': X[:,0],\n    'Feature 2': X[:,1],\n    'Status': y.astype(bool),\n    })\n\nfig = px.scatter(daten, x = 'Feature 1', y = 'Feature 2',  color='Status',\n                 title='Künstliche Daten', color_discrete_sequence=['#b40426','#3b4cc0'])\nfig.show()\n\n\n\nWir können uns jetzt verschiedene Geraden vorstellen, die die blauen Punkte von\nden roten Punkten trennen. In der folgenden Grafik sind drei eingezeichnet.\nWelche würden Sie nehmen und warum?\n\n\n\nFigure 1:Drei Geraden trennen die roten von den blauen Punkten, aber welche ist die bessere Wahl?\n(Quelle: eigene Darstellung; Lizenz CC-BY-NC-SA 4.0)\n\nAlle drei Geraden trennen die blauen von den roten Punkten. Jedoch könnte Gerade\n3 problematisch werden, wenn beispielsweise ein neuer blauer Datenpunkt an der\nPosition (2.3, 3.3) dazukäme. Dann würde Gerade 3 diesen Punkt als rot\nklassifizieren. Ähnlich verhält es sich mit Gerade 1. Ein neuer blauer\nDatenpunkt an der Position (0.5, 3) würde fälschlicherweise als rot\nklassifiziert werden. Gerade 2 bietet den sichersten Abstand zu den bereits\nvorhandenen Datenpunkten. Wir können diesen “Sicherheitsstreifen” folgendermaßen\nvisualisieren.\n\n\n\nFigure 2:Ein Sicherheitsstreifen bzw. breiter Rand, im Englischen Margin genannt,\ntrennt die beiden Klassen.\n(Quelle: eigene Darstellung; Lizenz CC-BY-NC-SA 4.0)\n\nDer Support-Vector-Algorithmus sucht nun die Gerade, die die Datenpunkte mit dem\ngrößten Randabstand (= Margin) voneinander trennt. Im Englischen sprechen wir\ndaher auch von Large Margin Classification. Die Suche nach dieser Geraden\nist dabei etwas zeitaufwändiger als die Berechnung der Gewichte bei der\nlogistischen Regression. Wenn aber einmal das Modell trainiert ist, ist die\nPrognose effizienter, da nur die sogenannten Stützvektoren, auf englisch\nSupport Vectors gespeichert und ausgewertet werden. Die Stützvektoren sind\ndie Vektoren, die vom Ursprung des Koordinatensystems zu den Punkten zeigen, die\nauf der Grenze des Sicherheitsbereichs liegen.\n\n\n\nFigure 3:Einige wenige Punkte (gelb markiert) bestimmen den Verlauf des Randabstandes.\nDie Vektoren, die vom Ursprung des Koordinatensystems zu diesen Punkten zeigen,\nwerden Stützvektoren (= Support Vectors) genannt.\n(Quelle: eigene Darstellung; Lizenz CC-BY-NC-SA 4.0)","type":"content","url":"/chapter10-sec01#welche-trenngerade-soll-es-sein","position":5},{"hierarchy":{"lvl1":"10.1 Maximiere den Rand, aber soft","lvl2":"Großer, aber weicher Randabstand"},"type":"lvl2","url":"/chapter10-sec01#gro-er-aber-weicher-randabstand","position":6},{"hierarchy":{"lvl1":"10.1 Maximiere den Rand, aber soft","lvl2":"Großer, aber weicher Randabstand"},"content":"Die bisherigen Beispiele zeigten perfekt trennbare Daten. In der Praxis sind\nDatensätze jedoch oft nicht linear trennbar. Für den Fall, dass einige wenige\nDatenpunkte “falsch” liegen, erlauben wir Ausnahmen. Wie viele Ausnahmen wir\nerlauben wollen, die im Sicherheitsstreifen liegen, steuern wir mit dem\nParameter C. Ein großes C bedeutet, dass wir eine große Mauer an den Grenzen\ndes Sicherheitsabstandes errichten. Es kommt kaum vor, dass Datenpunkte\ninnerhalb des Margins liegen. Je kleiner C wird, desto mehr Datenpunkte sind\ninnerhalb des Sicherheitsbereichs erlaubt.\n\nIm Folgenden betrachten wir einen neuen künstlichen Datensatz, bei dem die\nblauen von den roten Punkte nicht mehr ganz so stark getrennt sind. Schauen Sie\nsich die fünf verschiedenen Margins an, die entstehen, wenn der Parameter C\nvariiert wird.\n\nfrom IPython.display import HTML\nHTML('../assets/chapter10/fig04.html')\n\n\n\n","type":"content","url":"/chapter10-sec01#gro-er-aber-weicher-randabstand","position":7},{"hierarchy":{"lvl1":"10.1 Maximiere den Rand, aber soft","lvl2":"Zusammenfassung"},"type":"lvl2","url":"/chapter10-sec01#zusammenfassung","position":8},{"hierarchy":{"lvl1":"10.1 Maximiere den Rand, aber soft","lvl2":"Zusammenfassung"},"content":"In diesem Abschnitt haben wir die Ideen kennengelernt, die den Support Vector\nMachines zugrunde liegen. Im nächsten Abschnitt schauen wir uns an, wie ein\nSVM-Modell mit Scikit-Learn trainiert wird.","type":"content","url":"/chapter10-sec01#zusammenfassung","position":9},{"hierarchy":{"lvl1":"10.2 Training SVM mit Scikit-Learn"},"type":"lvl1","url":"/chapter10-sec02","position":0},{"hierarchy":{"lvl1":"10.2 Training SVM mit Scikit-Learn"},"content":"Wenn wir in der Dokumentation von Scikit-Learn\n\n\nScikit-Learn/SVM die Support\nVector Machines nachschlagen, so finden wir viele Einträge: SVC, NuSVC,\nLinearSVC, SVR, NuSVR und LinearSVR. Die Varianten mit “C” stehen für\nKlassifikation (englisch Classification) und die Varianten mit “R” für\nRegression. Wir benutzen in diesem Kapitel das Modell SVC.","type":"content","url":"/chapter10-sec02","position":1},{"hierarchy":{"lvl1":"10.2 Training SVM mit Scikit-Learn","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter10-sec02#lernziele","position":2},{"hierarchy":{"lvl1":"10.2 Training SVM mit Scikit-Learn","lvl2":"Lernziele"},"content":"Lernziele\n\nSie können ein SVM-Modell mit der Klasse SVC erzeugen.\n\nSie können den Parameter C zur Steuerung der Margins einsetzen.","type":"content","url":"/chapter10-sec02#lernziele","position":3},{"hierarchy":{"lvl1":"10.2 Training SVM mit Scikit-Learn","lvl2":"Wahl des linearen Kernels"},"type":"lvl2","url":"/chapter10-sec02#wahl-des-linearen-kernels","position":4},{"hierarchy":{"lvl1":"10.2 Training SVM mit Scikit-Learn","lvl2":"Wahl des linearen Kernels"},"content":"Zuerst importieren wir aus Scikit-Learn das entsprechende Modul ‘SVM’ und\ninstantiieren ein Modell. Da wir die etwas allgemeinere Klasse SVC anstatt\nLinearSVC verwenden, müssen wir bereits bei der Erzeugung die Option kernel=\nauf linear setzen, also kernel='linear'.\n\nfrom sklearn import svm\nsvm_modell = svm.SVC(kernel='linear')\n\n\n\nWir erzeugen uns erneut künstliche Messdaten.\n\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pylab as plt; plt.style.use('bmh')\n\n# Erzeugung künstlicher Daten\nX, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.50)\n\n# Visualisierung künstlicher Daten\nimport plotly.express as px\n\nfig = px.scatter(x = X[:,0], y = X[:,1],  color=y, color_continuous_scale=['#3b4cc0', '#b40426'],\n                 title='Künstliche Daten',\n                 labels={'x': 'Feature 1', 'y': 'Feature 2'})\nfig.show()\n\n\n\nWarnung: Datenskalierung für SVM notwendig\n\nSVMs sind empfindlich gegenüber unterschiedlichen Feature-Skalen. Ein Feature mit\nWerten von 0-1000 dominiert ein Feature mit Werten 0-1, auch wenn beide gleich\nwichtig sind. Daher sollten Features vor dem Training skaliert werden.\n\nEine Skalierung der Daten ist hier nicht erforderlich, da alle Features im\ngleichen Wertebereich liegen. Als nächstes teilen wir die Messdaten in\nTrainings- und Testdaten auf.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n\n\nNun können wir unser SVM-Modell trainieren:\n\nsvm_modell.fit(X_train, y_train);\n\n\n\nUnd als nächstes analysieren, wie viele der Testdaten mit dem trainierten Modell\nkorrekt klassifiziert werden.\n\nsvm_modell.score(X_test, y_test)\n\n\n\nEin super Ergebnis! Schön wäre es jetzt noch, die gefundene Trenngerade zu\nvisualisieren. Dazu modifizieren wir ein Code-Beispiel aus dem Buch: »Data\nScience mit Python« von Jake VanderPlas (mitp Verlag 2017), ISBN 978-3-95845-\n695-2, siehe\n\n\nhttps://​github​.com​/jakevdp​/PythonDataScienceHandbook.\n\n# Quelle: VanderPlas \"Data Science mit Python\", S. 482\n# modified by Simone Gramsch\nimport numpy as np\n\ndef plot_svc_grenze(model):\n    # aktuelles Grafik-Fenster auswerten\n    ax = plt.gca()\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n    # Raster für die Auswertung erstellen\n    x = np.linspace(xlim[0], xlim[1], 30)\n    y = np.linspace(ylim[0], ylim[1], 30)\n    Y, X = np.meshgrid(y, x)\n    xy = np.vstack([X.ravel(), Y.ravel()]).T\n\n    # Abstand zur Trennhyperebene berechnen mit eingebauter \n    # decision_function()\n    P = model.decision_function(xy).reshape(X.shape)\n\n    # Entscheidungsgrenzen und Margin darstellen\n    ax.contour(X, Y, P, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n\n    # Stützvektoren darstellen\n    ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, linewidth=1, facecolors='none', edgecolors='orange');\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)\n\n\n\nfig, ax = plt.subplots()\nax.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm')\nax.set_xlabel('Feature 1')\nax.set_ylabel('Feature 2')\nax.set_title('SVM mit Soft Margin');\n\nplot_svc_grenze(svm_modell)\n\n\n\n","type":"content","url":"/chapter10-sec02#wahl-des-linearen-kernels","position":5},{"hierarchy":{"lvl1":"10.2 Training SVM mit Scikit-Learn","lvl2":"Der Parameter C"},"type":"lvl2","url":"/chapter10-sec02#der-parameter-c","position":6},{"hierarchy":{"lvl1":"10.2 Training SVM mit Scikit-Learn","lvl2":"Der Parameter C"},"content":"Im letzten Abschnitt haben wir uns mit dem Parameter C beschäftigt, der\nAusnahmen innerhalb des Sicherheitsstreifens erlaubt. Wir können uns den\nParameter C als die “Höhe der Mauer” um den Margin vorstellen. Eine hohe Mauer\n(großes C) schützt den Sicherheitsbereich streng. Praktisch keine Datenpunkte\ndürfen hinein. Eine niedrige Mauer (kleines C) ist toleranter und lässt mehr\nAusnahmen zu. Als nächstes schauen wir uns an, wie der Parameter C gesetzt\nwird.\n\nDie Option zum Setzen des Parameters C lautet schlicht und einfach C=. Dabei\nmuss C immer positiv sein. Der voreingestellte Standardwert ist C=1.\n\nDamit aber besser sichtbar wird, wie sich C auswirkt, vermischen wir die\nkünstlichen Daten stärker. Für die Bewertung der Modelle trennen wir die Daten\nin Trainings- und Testdaten.\n\n# Erzeugung künstlicher Daten\nX, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.80)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n# Visualisierung künstlicher Daten\nimport plotly.express as px\n\nfig = px.scatter(x = X[:,0], y = X[:,1],  color=y, color_continuous_scale=['#3b4cc0', '#b40426'],\n                 title='Künstliche Daten',\n                 labels={'x': 'Feature 1', 'y': 'Feature 2'})\nfig.show()\n\n\n\nZuerst wählen wir ein sehr hohes C=1000000. Das ist ein Hard Margin, eine\npraktisch nicht durchdringbare Mauer, die keine Datenpunkte in den\nSicherheitsbereich lässt.\n\n# Wahl des Modells mit linearem Kern und großem C\nsvm_modell = svm.SVC(kernel='linear', C=1000000)\n\n# Training und Bewertung\nsvm_modell.fit(X_train, y_train);\nscore = svm_modell.score(X_test, y_test)\nprint(f'Score auf den Testdaten: {score:.2f}')\n\n# Visualisierung\nfig, ax = plt.subplots()\nax.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm')\nax.set_xlabel('Feature 1')\nax.set_ylabel('Feature 2')\nax.set_title('SVM mit Hard Margin');\nplot_svc_grenze(svm_modell)\n\n\n\n\n\nNun reduzieren wir den Wert des Parameters C deutlich auf C=1. Vereinzelt\nliegen Datenpunkte nun im Sicherheitsbereich, d.h. wir haben einen Soft Margin.\n\n# Wahl des Modells mit linearem Kern und kleinem C\nsvm_modell = svm.SVC(kernel='linear', C=1)\n\n# Training und Bewertung\nsvm_modell.fit(X_train, y_train);\nscore = svm_modell.score(X_test, y_test)\nprint(f'Score auf den Testdaten: {score:.2f}')\n\n# Visualisierung\nfig, ax = plt.subplots()\nax.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm')\nax.set_xlabel('Feature 1')\nax.set_ylabel('Feature 2')\nax.set_title('SVM mit Soft Margin');\nplot_svc_grenze(svm_modell)\n\n\n\n\n\nDie Visualisierung zeigt, dass bei kleinem C mehr Stützvektoren (orange\neingekreist) vorhanden sind, und einige davon innerhalb des Margins liegen oder\ndie Klassifikationsgrenze verletzen.\n\nWelches C sollen wir wählen? Beide Modelle liefern den gleichen Score von 1.0\nauf den Testdaten. Das kleinere C führt jedoch zu einem robusteren Modell mit\nbreiterem Margin. In der Praxis wird C oft durch Kreuzvalidierung\n(Crossvalidation) optimiert, was wir in einem späteren Kapitel aufgreifen\nwerden.","type":"content","url":"/chapter10-sec02#der-parameter-c","position":7},{"hierarchy":{"lvl1":"10.2 Training SVM mit Scikit-Learn","lvl2":"Zusammenfassung"},"type":"lvl2","url":"/chapter10-sec02#zusammenfassung","position":8},{"hierarchy":{"lvl1":"10.2 Training SVM mit Scikit-Learn","lvl2":"Zusammenfassung"},"content":"Verwenden wir den SVC-Klassifikator aus dem Modul SVM von Scikit-Learn, können\nwir mittels der Option kernel='linear' eine binäre Klassifikation durchführen,\nbei der die Trennungsgerade den größtmöglichen Abstand zwischen den Gruppen von\nPunkten erzeugt, also einen möglichst großen Margin. Sind die Daten nicht linear\ntrennbar, so können wir mit der Option C= steuern, wie viele Ausnahmen erlaubt\nwerden sollen. Mit Ausnahmen sind Punkte innerhalb des Margins gemeint. Im\nnächsten Abschnitt betrachten wir nichtlineare Trennungsgrenzen.","type":"content","url":"/chapter10-sec02#zusammenfassung","position":9},{"hierarchy":{"lvl1":"10.3 Nichtlineare SVM"},"type":"lvl1","url":"/chapter10-sec03","position":0},{"hierarchy":{"lvl1":"10.3 Nichtlineare SVM"},"content":"","type":"content","url":"/chapter10-sec03","position":1},{"hierarchy":{"lvl1":"10.3 Nichtlineare SVM","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter10-sec03#lernziele","position":2},{"hierarchy":{"lvl1":"10.3 Nichtlineare SVM","lvl2":"Lernziele"},"content":"Lernziele\n\nSie kennen den Kernel-Trick.\n\nSie können mit den radialen Basisfunktionen als neue Option für\nSVM-Verfahren nichtlinear trennbare Daten klassifizieren.","type":"content","url":"/chapter10-sec03#lernziele","position":3},{"hierarchy":{"lvl1":"10.3 Nichtlineare SVM","lvl2":"Nichtlineare trennbare Daten"},"type":"lvl2","url":"/chapter10-sec03#nichtlineare-trennbare-daten","position":4},{"hierarchy":{"lvl1":"10.3 Nichtlineare SVM","lvl2":"Nichtlineare trennbare Daten"},"content":"Für die Support Vector Machines sind wir bisher davon ausgegangen, dass die\nDaten -- ggf. bis auf wenige Ausnahmen -- linear getrennt werden können. Im\nFolgenden betrachten wir nun einen künstlichen Messdatensatz, bei dem das\noffensichtlich nicht geht. Dazu nutzen wir die in Scikit-Learn integrierte\nFunktion make_circles().\n\nfrom sklearn.datasets import make_circles\n\n# künstliche Messdaten generieren\nX,y = make_circles(100, random_state=0, factor=0.3, noise=0.1)\n\n# künstliche Messdaten visualisieren\nimport plotly.express as px\n\nfig = px.scatter(x = X[:,0], y = X[:,1],  color=y, color_continuous_scale=['#3b4cc0', '#b40426'],\n                 title='Künstliche Daten',\n                 labels={'x': 'Feature 1', 'y': 'Feature 2'})\nfig.show()\n\n\n\nDas menschliche Auge erkennt sofort das Muster in den Daten. Ganz offensichtlich\nsind die roten und blauen Punkte kreisförmig angeordnet und können\ndementsprechend auch durch einen Kreis getrennt werden. Allerdings wird ein\nSVM-Klassifikator, so wie wir das SVM-Verfahren bisher kennengelernt haben,\nversagen. Eine Gerade zur Klassifikation der roten und blauen Punkte passt\neinfach nicht.\n\n","type":"content","url":"/chapter10-sec03#nichtlineare-trennbare-daten","position":5},{"hierarchy":{"lvl1":"10.3 Nichtlineare SVM","lvl2":"Aus 2 mach 3"},"type":"lvl2","url":"/chapter10-sec03#aus-2-mach-3","position":6},{"hierarchy":{"lvl1":"10.3 Nichtlineare SVM","lvl2":"Aus 2 mach 3"},"content":"Die Idee zur Überwindung dieses Problems klingt zunächst einmal absurd. Wir\nmachen aus zwei Features drei Features. Die dahinterliegende Idee ist: Was in 2D\nnicht linear trennbar ist, kann in 3D linear trennbar werden. Ähnlich wie ein\nSchatten (2D) zweier sich berührender Kugeln (3D) sich zu überlappen scheint,\nobwohl die Kugeln getrennt sind.\n\nAls drittes Feature wählen wir den Abstand eines Punktes zum Ursprung.\n\nimport numpy as np\nimport plotly.express as px\n\n# Extraktion der Daten, damit leichter darauf zugegriffen werden kann\nX1 = X[:,0]\nX2 = X[:,1]\n\n# neues Feature: Abstand eines Punktes zum Ursprung\nX3 = np.sqrt(X1**2 + X2**2)\n\nfig = px.scatter_3d(x=X1, y=X2, z=X3, color=y, \n                    color_continuous_scale=['#3b4cc0', '#b40426'])\nfig.show()\n\n\n\nBitte drehen Sie die interaktive 3D-Ansicht mit der Maus (klicken und ziehen),\nbis die z-Achse nach oben zeigt. Die Punkte bilden eine Art Paraboloid (eine\nschüsselförmige 3D-Fläche). In dieser neuen Ansicht können wir eine Ebene\nfinden, die die roten von den blauen Punkten trennt.\n\nIn der folgenden Grafik ist eine Trennebene eingezeichnet. Wenn wir nun den\nSchnitt der Trennebene mit dem Paraboloid bilden, entsteht eine Kreislinie.\nDrehen wir wieder unsere Ansicht zurück, so dass wir von oben auf die\nX1-X2-Feature-Ebene blicken, so ist dieser Kreis genau das, was wir auch als\nMenschen genommen hätten, um die roten von den blauen Punkten zu trennen.\n\n\n\nFigure 1:Schematische Darstellung der Trennebene in 3D\n\n\n\nFigure 2:Schematische Darstellung der Rückprojektion: Trennebene geschnitten mit\nParaboloid ergibt eine Kreislinie","type":"content","url":"/chapter10-sec03#aus-2-mach-3","position":7},{"hierarchy":{"lvl1":"10.3 Nichtlineare SVM","lvl2":"Kernel-Trick"},"type":"lvl2","url":"/chapter10-sec03#kernel-trick","position":8},{"hierarchy":{"lvl1":"10.3 Nichtlineare SVM","lvl2":"Kernel-Trick"},"content":"Bei diesem künstlichen Datensatz haben die Abstände zum Ursprung als neues\nFeature sehr gut funktioniert. Das lag daran, dass die Punkte tatsächlich in\nKreisen um den Ursprung verteilt waren. Was ist, wenn das nicht der Fall ist?\nWenn der Schwerpunkt der Kreise verschoben wäre, müssten wir auch die\nTransformationsfunktion zum Erzeugen des dritten Features in diesen Schwerpunkt\nverschieben.\n\nGlücklicherweise übernimmt Scikit-Learn für uns die Suche nach einer passenden\nTransformationsfunktion automatisch. Das Verfahren, das dazu in die\nSVM-Algorithmen eingebaut ist, wird Kernel-Trick genannt. Es beruht darauf,\ndass manche Funktionen in ein Skalarprodukt umgewandelt werden können. Und dann\nwird nicht das dritte Feature mit der Transformationsfunktion aus den ersten\nbeiden Features berechnet, was sehr zeitaufwendig werden könnte, sondern die\nTransformationsfunktion wird direkt in das Lernverfahren eingebaut. Funktionen,\ndie dafür geeignet sind, werden als Kernel-Funktionen bezeichnet.\n\nAm häufigsten kommt dabei die sogenannte radiale Basisfunktion zum Einsatz.\nDie radialen Basisfunktionen werden mit RBF abgekürzt. Sie haben die\nwichtige Eigenschaft, dass sie nur vom Abstand eines Punktes zum Ursprung\nabhängen; so wie unser Beispiel oben.\n\nUm nichtlinear trennbare Daten zu klassifizieren, nutzen wir in Scikit-Learn das\nSVC-Lernverfahren. Doch diesmal wählen wir als Kern nicht die linearen\nFunktionen, sondern die sogenannten radialen Basisfunktionen RBF.\n\nfrom sklearn import svm\nsvm_modell = svm.SVC(kernel='rbf')\n\n\n\nErneut sind die beiden Merkmale im gleichen Bereich, so dass wir direkt die\nTrennung in Trainings- und Testdaten vollziehen können, ohne die Daten zu\nskalieren. Danach erfolgt das Training wie gewohnt mit der fit()-Methode, die\nBewertung mit der score()-Methode.\n\n# Trennung in Trainings- und Testdaten\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n# Training\nsvm_modell.fit(X_train, y_train);\n\n# Bewertung\nscore_train = svm_modell.score(X_train,y_train)\nscore_test = svm_modell.score(X_test,y_test)\n\nprint('Score Trainingsdaten: {:.2f}'.format(score_train))\nprint('Score Testdaten: {:.2f}'.format(score_test))\n\n\n\nSowohl für die Trainings- als auch Testdaten haben wir einen Score von 1.0. Die\nTrennung hat perfekt funktioniert. Zum Vergleich können wir ein lineares SVM\ntrainieren und bewerten lassen.\n\n# Zum Vergleich: lineares SVM\nsvm_linear = svm.SVC(kernel='linear')\nsvm_linear.fit(X_train, y_train)\nscore_train = svm_linear.score(X_train,y_train)\nscore_test = svm_linear.score(X_test,y_test)\n\nprint('Score Trainingsdaten lineares SVM-Modell: {:.2f}'.format(score_train))\nprint('Score Testdaten lineares SVM-Modell: {:.2f}'.format(score_test))\n\n\n\nDie Scores des linearen SVM-Modells (0.37 für die Trainingsdaten und 0.32 für\ndie Testdaten) zeigen, dass das lineare SVM-Modell nicht in der Lage ist, die\nblauen von den roten Punkten zu trennen bzw. die Kategorien 0 oder 1 korrekt\nzu klassifizieren.\n\nAber wie sieht nun die Trennung aus? Wir können erneut die Funktion\nplot_svc_grenze()aus dem vorherigen Abschnitt nutzen, um die Stützvektoren mit\neinem orangefarbenem Kreis zu markieren und die Entscheidungsgrenze zu\nvisualisieren.\n\n# Quelle: VanderPlas \"Data Science mit Python\", S. 482\n# modified by Simone Gramsch\nimport numpy as np\nimport matplotlib.pylab as plt; plt.style.use('bmh')\n\ndef plot_svc_grenze(model):\n    # aktuelles Grafik-Fenster auswerten\n    ax = plt.gca()\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n    # Raster für die Auswertung erstellen\n    x = np.linspace(xlim[0], xlim[1], 30)\n    y = np.linspace(ylim[0], ylim[1], 30)\n    Y, X = np.meshgrid(y, x)\n    xy = np.vstack([X.ravel(), Y.ravel()]).T\n    P = model.decision_function(xy).reshape(X.shape)\n\n    # Entscheidungsgrenzen und Margin darstellen\n    ax.contour(X, Y, P, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n\n    # Stützvektoren darstellen\n    ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, linewidth=1, facecolors='none', edgecolors='orange');\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)\n\nfig, ax = plt.subplots()\nax.scatter(X1, X2, c=y, cmap='coolwarm')\nax.set_xlabel('Feature 1')\nax.set_ylabel('Feature 2')\nax.set_title('Künstliche Messdaten');\nplot_svc_grenze(svm_modell)\n\n\n\nMit den radialen Basisfunktionen passt sich die Entscheidungsgrenze flexibel an\ndie Daten an. Statt eines perfekten Kreises erhalten wir ein deformiertes Ei,\ndas sehr gut zu den Daten passt.","type":"content","url":"/chapter10-sec03#kernel-trick","position":9},{"hierarchy":{"lvl1":"10.3 Nichtlineare SVM","lvl2":"Zusammenfassung"},"type":"lvl2","url":"/chapter10-sec03#zusammenfassung","position":10},{"hierarchy":{"lvl1":"10.3 Nichtlineare SVM","lvl2":"Zusammenfassung"},"content":"In diesem Abschnitt haben wir uns mit nichtlinearen Support Vector Machines\nbeschäftigt. Die Idee zur Klassifizierung nichtlinearer Daten ist, ein neues\nFeature hinzuzufügen. Mathematisch gesehen projizieren wir also die Daten mit\neiner nichtlinearen Transformationsfunktion in einen höherdimensionalen Raum und\ntrennen sie in dem höherdimensionalen Raum. Dann kehren wir durch den Schnitt der\nTrennebene mit der Transformationsfunktion wieder in den ursprünglichen Raum\nzurück. Wenn wir als Transformationsfunktion die sogenannten Kernel-Funktionen\nverwenden, können wir auf die Transformation der Daten verzichten und die\nTransformation direkt in die SVM einbauen. Das wird Kernel-Trick genannt und\nsorgt für die Effizienz und damit Beliebtheit von SVMs.","type":"content","url":"/chapter10-sec03#zusammenfassung","position":11},{"hierarchy":{"lvl1":"Übung"},"type":"lvl1","url":"/chapter10-sec04","position":0},{"hierarchy":{"lvl1":"Übung"},"content":"Auf der Internetseite\n[\n\nhttps://​archive​.ics​.uci​.edu​/dataset​/151​/connectionist+bench+sonar+mines+vs+rocks]\nfinden Sie einen Datensatz mit Sonarsignalen. Die Muster der Signale sind durch\n60 Zahlenwerte codiert (es handelt sich um die Energie zu bestimmten Frequenzen,\nnormalisiert auf [0,1]). Darüber hinaus wird angegeben, ob das Sonarsignal\nGestein (= Stein) oder Metall detektiert hat.\n\nLaden Sie nun die Datei ‘metall_oder_stein.csv’. Führen Sie eine explorative\nDatenanalyse durch. Lassen Sie dann alle Ihnen bekannten Klassifikationsmodelle\ntrainieren und validieren, um die Materialeigenschaft Stein/Metall auf Basis der\nnumerischen Werte zu prognostizieren.\n\nÜberblick über die Daten\n\nWelche Daten enthält der Datensatz?\n\nWie viele Datenpunkte und Merkmale gibt es?\n\nSind die Daten vollständig?\n\nWelche Datentypen haben die Merkmale?\n\nGibt es Merkmale mit unerwarteten Datentypen?\n\nLösungimport pandas as pd\n\n# Import der Daten (zwei Kommentarzeilen werden übersprungen)\ndaten = pd.read_csv('metall_oder_stein.csv', skiprows=2)\ndaten.info()\n\n# Check der Vollständigkeit:\nprint(daten.isnull().sum())\n\n# Blick in die Daten:\ndaten.head(10)\n\nDer Datensatz enthält 208 Einträge (Datenpunkte) und 61 Merkmale. Die ersten 60\nMerkmale Signal01 bis Signal60 werden durch Floats repräsentiert, das Merkmal\nMaterial wird durch Objekte repräsentiert.\n\nDie Daten sind vollständig.\n\nDie ersten 10 Zeilen zeigen in den Spalten Signal01 bis Signal60 numerische\nWerte. In der letzten Spalte Material wird der String ‘Stein’ aufgelistet. Ein\nkurzer Test mitdaten['Material'].unique()\n\nzeigt, dass in dieser Spalte lediglich die beiden Einträge ‘Stein’ und ‘Metall’\nauftreten. Insgesamt sind die Werte der Merkmale für die Datentypen plausibel.\n\nStatistik der numerischen Daten\n\nErstellen Sie eine Übersicht der statistischen Kennzahlen für die numerischen\nDaten. Interpretieren Sie die statistischen Kennzahlen. Gibt es Auffälligkeiten?\nSind die Werte plausibel?\n\nLösungdaten.describe()\n\nDie statistischen Kennzahlen lassen sich aufgrund der großen Anzahl an Merkmalen\nso kaum interpretieren. Daher hilft hier ein Boxplot der numerischen Daten\nweiter.import plotly.express as px\n\nfig = px.box(daten.drop('Material', axis=1), \n             title='Stein oder Metall',\n             labels={'variable': 'Merkmal', 'value':'Wert'})\nfig.show()\n\nDie Medianwerte scheinen einem Muster zu folgen. Beginnend bei Merkmal Signal01\nsteigen sie bis zu Signal26, wo der Median den Wert 0.7545 erreicht, um dann\nwieder abzufallen. Ab Signal50 liegt der Median unter 0.0179. Bei Eigenschaften\nmit einem größeren Median ist auch der Interquartilsabstand (IQR) größer, dafür\ngibt es keine Ausreißer. Das Maximum liegt bei 1, was konsistent mit der\nNormalisierung auf [0,1] ist.\n\nStatistik der kategorialen Merkmale\n\nWie viele Einträge gibt es für Metall oder Stein? Visualisieren Sie die\nVerteilung mit einem Balkendiagramm der Klassenverteilung. Ist der Datensatz\nausgewogen?\n\nLösung# Berechnung der Anzahl der Einträge\nprint(daten['Material'].value_counts())\n\n# Visualisierung als Balkendiagramm\nfig = px.bar(daten['Material'].value_counts(),\n             title='Stein oder Metall')\nfig.update_layout(\n    yaxis_title='Anzahl',\n    xaxis_title='Material',\n    showlegend=False\n)\nfig.show()\n\nIm Datensatz sind 111 Proben Metall und 97 Stein. Damit sind 53 % der Proben\nMetall und 47 % Stein, was ungefähr gleichverteilt ist.\n\nVorbereitung der Daten für das Training\n\nBereiten Sie die Daten für das maschinelle Lernen vor:\n\nKodieren Sie die kategoriale Variable Material mit replace().\n\nTrennen Sie die Merkmale (Input X) und die Zielgröße (Output y).\n\nTeilen Sie die Daten in Trainings- und Testdaten im Verhältnis 80:20 auf.\n\nSkalieren Sie - falls notwendig - die Trainingsdaten mit fit_transform()\nund die Testdaten mit transform().\n\nLösungfrom sklearn.model_selection import train_test_split\n\n# Kodierung mit Check (sollte [0 1] ausgeben)\ndaten['Material'] = daten['Material'].replace({'Stein': '0', 'Metall': '1'}).astype(int)\nprint(daten['Material'].unique())\n\n# Aufteilung in Input/Output\nX = daten.drop('Material', axis=1)\ny = daten['Material']\n\n# Split 80:20\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Prüfen der Wertebereiche\nprint(X.min().min(), X.max().max())\n\nDa die Signaldaten alle im Intervall [0,1] liegen, brauchen wir die Daten\nnicht skalieren.\n\nML-Modell Entscheidungsbaum\n\nTrainieren Sie einen Entscheidungsbaum und bewerten Sie das trainierte Modell.\n\nLösungfrom sklearn.tree import DecisionTreeClassifier\n\n# Instanziierung des Entscheidungsbaums mit Training\nmodell_baum = DecisionTreeClassifier()\nmodell_baum.fit(X_train,y_train)\n\n# Bewertung\nscore_train = modell_baum.score(X_train, y_train)\nscore_test = modell_baum.score(X_test, y_test)\nprint(f'Score Trainingsdaten Entscheidungsbaum: {score_train :.2f}')\nprint(f'Score Testdaten Entscheidungsbaum: {score_test :.2f}')\n\nDer Trainingsscore ist 1.00, der Testscore 0.79; ein Zeichen für Overfitting.\n\nML-Modell Random Forest\n\nTrainieren Sie nun einen Random Forest mit 1, 5, 10, 20, 50 und 100 Bäumen auf\ndenselben Daten. Berechnen Sie wieder die Scores für Training und Test. Wie\nunterscheiden sich die Ergebnisse vom einzelnen Entscheidungsbaum? Welches\nRandom-Forest-Modell würden Sie wählen?\n\nBewerten Sie darüber hinaus mit der Feature Importance, welche Signale am\nwichtigsten sind.\n\nLösungfrom sklearn.ensemble import RandomForestClassifier\n\nfor n in [1, 5, 10, 20, 50, 100]:\n    # Instanziierung des Random-Forest-Modells und Training\n    modell_rf = RandomForestClassifier(n_estimators=n, random_state=0)\n    modell_rf.fit(X_train, y_train)\n\n    # Bewertung\n    score_train = modell_rf.score(X_train, y_train)\n    score_test = modell_rf.score(X_test, y_test)\n    print(f'Anzahl Entscheidungsbäume: {n}')\n    print(f'Score Training: {score_train :.2f} | Score Test: {score_test :.2f}')\n    print('')\n\n# bestes RF-Modell wählen\nmodell_rf_final = RandomForestClassifier(n_estimators=50, random_state=0)\nmodell_rf_final.fit(X_train, y_train)\n\n# Feature Importance extrahieren\nimportance = pd.DataFrame({\n    'Merkmal': X_train.columns,\n    'Feature Importance': modell_rf_final.feature_importances_\n}).sort_values('Feature Importance', ascending=False)\n\n# die 5 wichtigsten Merkmale anzeigen\nimportance.head(5)\n\nDas Random-Forest-Modell ist besser verallgemeinerbar als der Entscheidungsbaum.\nFür einen Random Forest aus 50 Entscheidungsbäumen erhalten wir einen\nTrainingsscore von 1.00 und einen Testscore von 0.88. Noch mehr\nEntscheidungsbäume führen zu keinem besseren Ergebnis, so dass wir dieses\nModell wählen würden.\n\nMit dem besten Modell berechnen wir dann die Feature Importance. Am wichtigsten\nsind die Signale Signal09 bis Signal12 und Signal52.\n\nML-Modelle SVM und finales Modell\n\nTrainieren Sie nun sowohl eine lineare SVM als auch eine nichtlineare SVM.\nOrdnen Sie ein: welches ML-Modell würden Sie final wählen?\n\nLösungfrom sklearn.svm import SVC\n\n# Instanziierung einer linearen SVM und Training\nsvm_linear = SVC(kernel='linear', random_state=0)\nsvm_linear.fit(X_train, y_train)\n\n# Bewertung lineare SVM\nscore_train = svm_linear.score(X_train, y_train)\nscore_test = svm_linear.score(X_test, y_test)\nprint(f'Score Trainingsdaten lineare SVM: {score_train :.2f}')\nprint(f'Score Testdaten lineare SVM: {score_test :.2f}')\n\n# Instanziierung einer nichtlinearen SVM und Training\nsvm_rbf = SVC(kernel='rbf', random_state=0)\nsvm_rbf.fit(X_train, y_train)\n\n# Bewertung nichtlineare SVM\nscore_train = svm_rbf.score(X_train, y_train)\nscore_test = svm_rbf.score(X_test, y_test)\nprint(f'Score Trainingsdaten nichtlineare SVM: {score_train :.2f}')\nprint(f'Score Testdaten nichtlineare SVM: {score_test :.2f}')\n\nWir erhalten die folgenden Scores (zusammen mit den vorherigen):\n\nML-Modell\n\nScore Training\n\nScore Test\n\nEntscheidungsbaum\n\n1.00\n\n0.79\n\nRandom Forest\n\n1.00\n\n0.88\n\nlineare SVM\n\n0.86\n\n0.86\n\nnichtlineare SVM (RBF-Kernel)\n\n0.88\n\n0.79\n\nDie lineare SVM generalisiert am besten (gleicher Score auf Train/Test), während\ndie nichtlineare SVM (mit RBF-Kernel) leichtes Overfitting zeigt (0.88 vs. 0.79)\n\nObwohl der Random Forest das beste Ergebnis auf den Testdaten erzielt (Testscore\n0.88), entscheiden wir uns nicht für dieses Modell. Der Random Forest besteht\naus vielen Entscheidungsbäumen und ist dadurch zwar leistungsstark, aber nur\neingeschränkt interpretierbar.\n\nDie lineare SVM erreicht mit einem Trainings- und Testscore von jeweils 0.86\neine ähnlich gute Prognosegüte wie der Random Forest, zeigt aber keinerlei\nOverfitting. Außerdem ist sie deutlich einfacher interpretierbar, da die\nEntscheidungsregel auf einem linearen Modell basiert. Die gelernten Gewichte\ngeben direkt an, wie stark und in welche Richtung jedes Signal die\nKlassifikation beeinflusst. Das macht das lineare SVM-Modell transparent.\n\nDaher wählen wir als finales Modell die lineare SVM, da sie stabile Ergebnisse\nliefert, gut verallgemeinert und die zugrunde liegende Entscheidungslogik\nnachvollziehbar bleibt.","type":"content","url":"/chapter10-sec04","position":1},{"hierarchy":{"lvl1":"11.1 Kreuzvalidierung"},"type":"lvl1","url":"/chapter11-sec01","position":0},{"hierarchy":{"lvl1":"11.1 Kreuzvalidierung"},"content":"In der Praxis ist es entscheidend, dass ein ML-Modell nicht nur gute Prognosen\nfür die Daten liefert, sondern auch für neue, unbekannte Daten zuverlässig\nfunktioniert. Durch das Aufteilen der Daten in Trainings- und Testdaten können\nwir eine erste Einschätzung über die Verallgemeinerungsfähigkeit eines Modells\ntreffen. Dieser Ansatz weist jedoch einige Schwächen auf, die wir in diesem\nKapitel näher beleuchten. Im Anschluss lernen wir ein fortschrittlicheres\nVerfahren kennen: die Kreuzvalidierung, die über die einfache Aufteilung in\nTrainings- und Testdaten hinausgeht und eine robustere Bewertung der\nModellleistung ermöglicht.","type":"content","url":"/chapter11-sec01","position":1},{"hierarchy":{"lvl1":"11.1 Kreuzvalidierung","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter11-sec01#lernziele","position":2},{"hierarchy":{"lvl1":"11.1 Kreuzvalidierung","lvl2":"Lernziele"},"content":"Lernziele\n\nSie sind in der Lage, das Konzept der Kreuzvalidierung (Cross Validation)\nverständlich zu erklären.\n\nSie können die Vor- und Nachteile der Kreuzvalidierung aufzählen und bewerten.\n\nSie können mit KFold einen Datensatz in verschiedene Teilmengen\n(Folds) aufteilen.\n\nSie beherrschen die Durchführung einer Kreuzvalidierung mithilfe der Funktion\ncross_validate().","type":"content","url":"/chapter11-sec01#lernziele","position":3},{"hierarchy":{"lvl1":"11.1 Kreuzvalidierung","lvl2":"Idee der Kreuzvalidierung"},"type":"lvl2","url":"/chapter11-sec01#idee-der-kreuzvalidierung","position":4},{"hierarchy":{"lvl1":"11.1 Kreuzvalidierung","lvl2":"Idee der Kreuzvalidierung"},"content":"Ein zentraler Schritt im ML-Workflow ist die Aufteilung der Daten in einen\nTrainings- und einen Testdatensatz. Das Modell wird auf den Trainingsdaten\ntrainiert und anschließend auf den Testdaten bewertet. Diese Methode hat jedoch\nauch Nachteile. Besonders bei kleinen Datensätzen ist es problematisch,\nbeispielsweise 25 % der Daten für den Test zurückhalten zu müssen, da dies die\nDatenmenge für das Training reduziert. Zudem kann eine zufällige Aufteilung der\nDaten zu unbalancierten Splits führen, die die Trainings- und Testergebnisse\nverfälschen. Eine sinnvolle Alternative zu dieser simplen Aufteilung ist die\nKreuzvalidierung (engl. Cross Validation).\n\nBei der Kreuzvalidierung werden die Daten in mehrere Teilmengen, sogenannte\nFolds, aufgeteilt. Beispielsweise können die Daten in fünf Folds unterteilt\nwerden. Das Modell wird dann fünfmal trainiert und getestet, wobei in jedem\nDurchlauf eine andere Teilmenge als Testdaten verwendet wird. Im ersten\nDurchlauf wird etwa Fold A für den Test zurückgehalten, während die Folds B, C,\nD und E zum Training genutzt werden. Im zweiten Durchlauf wird Fold B als\nTestdatensatz verwendet und die restlichen Folds dienen wieder dem Training.\nDieser Prozess wird so lange wiederholt, bis jeder Fold einmal als Testdaten\nfungiert hat. Am Ende wird die Modellleistung (Score) als Durchschnitt der\nErgebnisse aus den fünf Durchläufen berechnet.\n\nEs müssen jedoch nicht zwingend fünf Folds verwendet werden. Oftmals werden die\nDaten in zehn Folds aufgeteilt, sodass 90 % der Daten zum Training und 10 % für\nden Test verwendet werden. Ein weiterer Vorteil ist, dass jeder Datenpunkt im\nLaufe der Kreuzvalidierung sowohl im Training als auch im Test berücksichtigt\nwird, jedoch nie gleichzeitig. Dies verringert die Gefahr, dass unausgewogene\nDaten zu verzerrten Testergebnissen führen, wie es bei einer zufälligen\nAufteilung passieren könnte.\n\nZusammengefasst bietet die Kreuzvalidierung mehrere Vorteile:\n\nEffizientere Datennutzung: Jeder Datenpunkt wird mindestens einmal als\nTestdatenpunkt verwendet, was besonders bei kleinen Datensätzen wichtig ist,\nda die Daten optimal ausgenutzt werden.\n\nStabilere Schätzung der Modellleistung: Durch das wiederholte Training und\nTesten auf verschiedenen Daten erhöht sich die Robustheit der geschätzten\nModellleistung (Score), da zufällige Verzerrungen durch unbalancierte Splits\nminimiert werden.\n\nEin Nachteil der Kreuzvalidierung ist der erhöhte Rechenaufwand, da das Modell\nmehrfach trainiert und getestet wird.\n\nKönnen wir also auf die Aufteilung in Trainings- und Testdaten verzichten? Nein,\ndenn für das Hyperparameter-Tuning ist der Split weiterhin notwendig. Mehr dazu\nim nächsten Kapitel. Zunächst widmen wir uns der praktischen Umsetzung der\nKreuzvalidierung in Scikit-Learn.","type":"content","url":"/chapter11-sec01#idee-der-kreuzvalidierung","position":5},{"hierarchy":{"lvl1":"11.1 Kreuzvalidierung","lvl2":"Kreuzvalidierung mit KFold"},"type":"lvl2","url":"/chapter11-sec01#kreuzvalidierung-mit-kfold","position":6},{"hierarchy":{"lvl1":"11.1 Kreuzvalidierung","lvl2":"Kreuzvalidierung mit KFold"},"content":"Um die Kreuzvalidierung in Scikit-Learn zu demonstrieren, generieren wir\nzunächst einen künstlichen Datensatz. Mithilfe der Funktion make_moons()\nerstellen wir 50 Datenpunkte und speichern sie in einem Pandas-DataFrame. Für\neine einfachere Visualisierung mit Plotly Express wandeln wir die Zielvariable\n'Wirkung' von den Werten 0/1 in boolesche Werte (False/True) um.\n\nimport pandas as pd\nimport plotly.express as px\nfrom sklearn.datasets import make_moons \n\nX_array, y_array = make_moons(noise = 0.5, n_samples=50, random_state=3)\ndaten = pd.DataFrame({\n    'Merkmal 1': X_array[:,0],\n    'Merkmal 2': X_array[:,1],\n    'Wirkung': y_array\n})\ndaten['Wirkung'] = daten['Wirkung'].astype('bool')\n\nfig = px.scatter(daten, x = 'Merkmal 1', y = 'Merkmal 2', color='Wirkung',\n    title='Künstliche Daten')\nfig.show()\n\n\n\nAls Nächstes laden wir die Klasse KFold aus dem Untermodul\nsklearn.model_selection. Wir instanziieren ein KFold-Objekt mit dem Argument\nn_splits=5, das die Daten in fünf Teilmengen (Folds) aufteilt. Tatsächlich ist\ndies die Standardeinstellung, wie uns die \n\nDokumentation Scikit-Learn →\nKFold\nzeigt. Das Argument könnte also weggelassen werden.\n\nfrom sklearn.model_selection import KFold\n\nkfold = KFold(n_splits = 5)\n\n\n\nIm Hintergrund wurde ein Generator erzeugt, mit Hilfe dessen wir Daten in fünf\nTeilmengen (Folds) aufteilen können. Dazu benutzen wir die Methode .split()\nund übergeben ihr die Daten, die gesplittet werden sollen.\n\nkfold.split(daten)\n\n\n\nZwar wurde hiermit die Aufteilung in fünf Teilmengen vollzogen, doch die\neigentlichen Trainings- und Testdaten wurden noch nicht gespeichert und\nweiterverarbeitet. Mithilfe einer for-Schleife greifen wir in jedem Durchgang\nauf die Trainings- und Testindizes zu, die die Methode split() als Tupel\nzurückgibt. Das erste Element enthält die Indizes der Trainingsdaten, das zweite\ndie der Testdaten.\n\nfor (train_index, test_index) in kfold.split(daten):\n  print(f'Index Trainingsdaten: {train_index}')\n  print(f'Index Testdaten: {test_index}')\n\n\n\nDie Aufteilung der Daten erfolgt hierbei sehr systematisch. Im ersten Durchgang\nwerden die Datenpunkte 0–9 als Testdaten verwendet, im zweiten Durchgang die\nPunkte 10–19 und so weiter. Bei sortierten Daten kann dies ungünstig sein. Um\neine zufällige Aufteilung zu gewährleisten, können wir das Argument\nshuffle=True verwenden, um die Daten vor dem Split zu mischen.\n\nkfold = KFold(n_splits = 5, shuffle=True)\n\nfor (train_index, test_index) in kfold.split(daten):\n  print(f'Index Trainingsdaten: {train_index}')\n  print(f'Index Testdaten: {test_index}')\n\n\n\nNun verwenden wir diese fünf Aufteilungen, um einen Entscheidungsbaum zu\ntrainieren. Dabei begrenzen wir die Baumtiefe auf 3 und bewerten in jedem\nDurchgang die Genauigkeit (Score) sowohl auf den Trainings- als auch auf den\nTestdaten.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodell = DecisionTreeClassifier(max_depth=3) \nkfold = KFold(n_splits = 5, shuffle=True, random_state=0)\n\nfor (train_index, test_index) in kfold.split(daten):\n  X_train = daten.loc[train_index, ['Merkmal 1', 'Merkmal 2']]\n  y_train = daten.loc[train_index, 'Wirkung']\n  X_test = daten.loc[test_index, ['Merkmal 1', 'Merkmal 2']]\n  y_test = daten.loc[test_index, 'Wirkung']\n  \n  modell.fit(X_train, y_train)\n  score_train = modell.score(X_train, y_train)\n  score_test = modell.score(X_test, y_test)\n\n  print(f'Score Training: {score_train:.2f}, Score Test: {score_test:.2f}')\n\n\n\nDie Scores auf den Trainingsdaten könnten den Eindruck erwecken, dass der\nEntscheidungsbaum sehr gut funktioniert. Doch die Testdaten zeigen Schwankungen\nzwischen 0.4 und 0.8. Hätten wir eine einfache Aufteilung in Trainings- und\nTestdaten vorgenommen und zufällig den dritten Split erwischt, hätten wir\nwahrscheinlich eine zu optimistische Einschätzung der Modellqualität getroffen.\nAus didaktischen Gründen verwenden wir das Argument random_state=0, um die\nErgebnisse mit dem Vorlesungsskript vergleichbar zu machen.","type":"content","url":"/chapter11-sec01#kreuzvalidierung-mit-kfold","position":7},{"hierarchy":{"lvl1":"11.1 Kreuzvalidierung","lvl2":"Automatische Kreuzvalidierung mit cross_validate"},"type":"lvl2","url":"/chapter11-sec01#automatische-kreuzvalidierung-mit-cross-validate","position":8},{"hierarchy":{"lvl1":"11.1 Kreuzvalidierung","lvl2":"Automatische Kreuzvalidierung mit cross_validate"},"content":"Wie so oft bietet Scikit-Learn eine elegantere und einfachere Möglichkeit, die\nKreuzvalidierung (Cross Validation) durchzuführen, ohne manuell eine\nfor-Schleife programmieren zu müssen. Die Funktion cross_validate() übernimmt\ndie Durchführung der Kreuzvalidierung automatisch. Wir importieren sie aus dem\nUntermodul sklearn.model_selection und teilen anschließend die Daten in\nEingabedaten X und Zielgröße y auf.\n\nDie Funktion cross_validate() wird mit dem ML-Modell (hier einem\nEntscheidungsbaum), den Eingabedaten X und der Zielgröße y aufgerufen.\nStandardmäßig wird eine 5-fache Kreuzvalidierung ohne Mischen durchgeführt. Mit\ndem optionalen Argument cv= kann jedoch auch ein benutzerdefinierter\nAufteilungsgenerator übergeben werden, wie zum Beispiel KFold. Das zusätzliche\nArgument return_train_score=True sorgt dafür, dass auch die Trainingsscores in\njedem Durchlauf gespeichert werden. Der entsprechende Code sieht folgendermaßen\naus:\n\nfrom sklearn.model_selection import cross_validate\n\nX = daten[['Merkmal 1', 'Merkmal 2']]\ny = daten['Wirkung']\n\ncv_results = cross_validate(modell, X,y, cv=kfold, return_train_score=True)\n\n\n\nDie Funktion cross_validate() gibt ein Dictionary zurück, das wie folgt\naufgebaut ist:\n\nprint(cv_results)\n\n\n\nIn diesem Dictionary sind zunächst die Rechenzeiten für das Training\n('fit_time') und die Prognose ('score_time') gespeichert. Danach folgen die\nScores der Testdaten ('test_score'). Falls das Argument\nreturn_train_score=True gesetzt wurde, enthält das Dictionary auch die Scores\nder Trainingsdaten ('train_score'). Die Scores können wir wie folgt anzeigen\nlassen:\n\nprint(cv_results['test_score'])\nprint(cv_results['train_score'])\n\n\n\nWeitere Details zu der Funktion cross_validate() finden Sie in der\n\n\nDokumentation Scikit-Learn →\ncross_validate.","type":"content","url":"/chapter11-sec01#automatische-kreuzvalidierung-mit-cross-validate","position":9},{"hierarchy":{"lvl1":"11.1 Kreuzvalidierung","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter11-sec01#zusammenfassung-und-ausblick","position":10},{"hierarchy":{"lvl1":"11.1 Kreuzvalidierung","lvl2":"Zusammenfassung und Ausblick"},"content":"Die Kreuzvalidierung ist ein wichtiges Werkzeug, insbesondere wenn es um die\nFeinjustierung der Hyperparameter geht, also das sogenannte\nHyperparameter-Tuning. Im nächsten Kapitel werden wir uns mit der Kombination\nvon Kreuzvalidierung (Cross Validation) und einer Gittersuche (Grid Search)\nbeschäftigen, um die optimalen Hyperparameter für ein Modell zu finden.","type":"content","url":"/chapter11-sec01#zusammenfassung-und-ausblick","position":11},{"hierarchy":{"lvl1":"11.2 Gittersuche"},"type":"lvl1","url":"/chapter11-sec02","position":0},{"hierarchy":{"lvl1":"11.2 Gittersuche"},"content":"Die Kreuzvalidierung wird selten isoliert verwendet. Sie ist jedoch ein\nunverzichtbares Werkzeug, wenn es darum geht, die Hyperparameter eines Modells\nzu optimieren. In diesem Kapitel vertiefen wir daher zunächst das Verständnis\nder Kreuzvalidierung, bevor wir sie im Rahmen der Gittersuche anwenden.","type":"content","url":"/chapter11-sec02","position":1},{"hierarchy":{"lvl1":"11.2 Gittersuche","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter11-sec02#lernziele","position":2},{"hierarchy":{"lvl1":"11.2 Gittersuche","lvl2":"Lernziele"},"content":"Lernziele\n\nSie verstehen, dass Daten für die Modellauswahl in Trainingsdaten,\nValidierungsdaten und Testdaten unterteilt werden.\n\nSie sind in der Lage, Hyperparameter mittels Gittersuche und Kreuzvalidierung\nmithilfe von GridSearchCV zu optimieren.","type":"content","url":"/chapter11-sec02#lernziele","position":3},{"hierarchy":{"lvl1":"11.2 Gittersuche","lvl2":"Kreuzvalidierung zur Modellauswahl"},"type":"lvl2","url":"/chapter11-sec02#kreuzvalidierung-zur-modellauswahl","position":4},{"hierarchy":{"lvl1":"11.2 Gittersuche","lvl2":"Kreuzvalidierung zur Modellauswahl"},"content":"Im letzten Kapitel haben wir die Kreuzvalidierung eingeführt. Ihr Ziel ist es,\neine robustere Bewertung der Modellleistung zu ermöglichen. Besonders bei der\nBeurteilung und der Verbesserung der Verallgemeinerungsfähigkeit eines Modells\n(Reduktion von Overfitting), ist die Kreuzvalidierung ein wertvolles Werkzeug.\nIn diesem Abschnitt nutzen wir die Kreuzvalidierung, um zwischen zwei Modellen\nzu wählen.\n\nAus didaktischen Gründen verwenden wir weiterhin künstliche Daten, die mit der\nFunktion make_moons() aus dem Modul sklearn.datasets erzeugt werden. Diese\nspeichern wir in einem Pandas DataFrame und visualisieren sie anschließend mit\nPlotly Express.\n\nimport pandas as pd\nimport plotly.express as px\nfrom sklearn.datasets import make_moons\n\nX_array, y_array = make_moons(noise = 0.5, n_samples=100, random_state=3)\ndaten = pd.DataFrame({\n    'Merkmal 1': X_array[:,0],\n    'Merkmal 2': X_array[:,1],\n    'Wirkung': y_array\n})\ndaten['Wirkung'] = daten['Wirkung'].astype('bool')\n\nfig = px.scatter(daten, x = 'Merkmal 1', y = 'Merkmal 2', color='Wirkung',\n    title='Künstliche Daten')\nfig.show()\n\n\n\nAls nächstes trainieren wir einen Entscheidungsbaum. Da Entscheidungsbäume\nhäufig zur Überanpassung (Overfitting) neigen, entscheiden wir uns, die\nBaumtiefe zu begrenzen. Aber welche Baumtiefe ist optimal? Die Baumtiefe ist ein\nHyperparameter, der vor dem Training des Modells festgelegt wird. Mithilfe der\nKreuzvalidierung können wir untersuchen, wie sich die Baumtiefe auf die\nModellqualität auswirkt. Wir testen die Baumtiefen 3, 4, 5 und 6 und geben die\nScores auf den Testdaten aus, wobei wir uns mit einer for-Schleife die Arbeit\nerleichtern.\n\nfrom sklearn.model_selection import cross_validate, KFold\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Adaption der Daten\nX = daten[['Merkmal 1', 'Merkmal 2']]\ny = daten['Wirkung']\n\n# Vorbereitung der Kreuzvalidierung mit 10 Splits\nkfold = KFold(n_splits=10)\n\n# wiederholte Kreuzvalidierung für Baumtiefe 3, 4, 5 und 6\nfor max_tiefe in [3, 4, 5, 6]:\n    modell = DecisionTreeClassifier(max_depth=max_tiefe)\n    cv_results = cross_validate(modell, X,y, cv=kfold)\n    test_scores = cv_results['test_score']\n    print(f'Testscores: {test_scores}')\n\n\n\nDie Ausgabe von 10 Testscores ist jedoch unübersichtlich. Stattdessen berechnen\nwir besser den Mittelwert (Mean) und die Standardabweichung (Standard Deviation)\nder Scores. Dazu importieren wir mean() und std() aus dem NumPy-Modul und\npassen die print()-Anweisung entsprechend an.\n\nfrom numpy import mean, std\n\nfor max_tiefe in [3, 4, 5, 6]:\n    modell = DecisionTreeClassifier(max_depth=max_tiefe)\n    cv_results = cross_validate(modell, X,y, cv=kfold)\n    test_scores = cv_results['test_score']\n    print(f'Mittelwert Testscores: {mean(test_scores):.2f}, Standardabweichung: {std(test_scores):.2f}')\n\n\n\nDas beste Ergebnis erzielen wir mit einem Entscheidungsbaum der Tiefe 3. Diesen\nkönnten wir nun als finales Modell wählen.\n\nEs gibt jedoch ein Problem: Wir haben die Modellauswahl mit den Scores der\nTestdaten begründet, wodurch diese in das Modelltraining eingeflossen sind. Daher\nbenötigen wir einen frischen Datensatz, um die Prognosequalität zu testen. Die\nLösung dafür ist train_test_split().\n\nZuerst teilen wir die Daten in Trainings- und Testdaten. Dann verwenden wir die\nKreuzvalidierung auf den Trainingsdaten, um die Hyperparameter zu bewerten. Die\nKreuzvalidierung teilt die Trainingsdaten erneut in Trainings- und Testdaten\nauf.  Damit diese »internen« Testdaten nicht mit den richtigen Testdaten\nverwechselt werden, nennt man sie auch Validierungsdaten. Die Mittelwerte\nder Scores speichern wir in einem Dictionary, um später das beste Modell zu\nermitteln. Schließlich trainieren wir das beste Modell auf allen Trainingsdaten\nund bewerten es mit den Testdaten.\n\nDas Hyperparameter-Tuning bzw. die Modellwahl mit Kreuzvalidierung funktioniert\nkomplett also wie folgt:\n\nfrom sklearn.model_selection import cross_validate, KFold, train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\nX = daten[['Merkmal 1', 'Merkmal 2']]\ny = daten['Wirkung']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y)\n\nkfold = KFold(n_splits=10)\n\nmean_scores = {}\nfor max_tiefe in [3, 4, 5, 6]:\n    modell = DecisionTreeClassifier(max_depth=max_tiefe)\n    cv_results_modell = cross_validate(modell, X_train, y_train, cv=kfold)\n    test_scores = cv_results_modell['test_score']\n    mean_scores[max_tiefe] = mean(test_scores)\n    print(f'Mittelwert Testscores: {mean(test_scores):.2f}, Standardabweichung: {std(test_scores):.2f}')\n\n# Ermitteln der besten Baumtiefe (argmax o.ä. wäre einfacher)\ntiefe = 3\nscore = mean_scores[3]\nfor t in [4,5,6]:\n    if mean_scores[t] > score:\n        tiefe = t\n        score = mean_scores[t]\nprint(f'\\nWähle Baumtiefe {tiefe} mit dem besten Score {score:.2f}.')\n\n# Finale Modellauswahl, Training und Bewertung\nfinales_modell = DecisionTreeClassifier(max_depth=tiefe)\nfinales_modell.fit(X_train, y_train)\nfinaler_score = finales_modell.score(X_test, y_test)\nprint(f'Testscore finales Modell: {finaler_score:.2f}')\n\n\n\nUm die Hyperparameter zu optimieren und das beste Modell zu finden, haben wir\neine for-Schleife und manuelle Auswahl verwendet. Scikit-Learn bietet jedoch\neine einfachere Lösung, die wir im nächsten Abschnitt behandeln: die Gittersuche\nmit Kreuzvalidierung GridSearchCV.","type":"content","url":"/chapter11-sec02#kreuzvalidierung-zur-modellauswahl","position":5},{"hierarchy":{"lvl1":"11.2 Gittersuche","lvl2":"Gittersuche mit Kreuzvalidierung: GridSearchCV"},"type":"lvl2","url":"/chapter11-sec02#gittersuche-mit-kreuzvalidierung-gridsearchcv","position":6},{"hierarchy":{"lvl1":"11.2 Gittersuche","lvl2":"Gittersuche mit Kreuzvalidierung: GridSearchCV"},"content":"Die Gittersuche mit Kreuzvalidierung wird als GridSearchCV aus dem Modul\nsklearn.model_selection importiert. Zunächst legen wir fest, welche Parameter\noptimiert werden sollen und welche Werte dafür in Betracht kommen. Technisch\nbenötigen wir dafür ein Dictionary, in dem die Schlüssel die Parameternamen und\ndie Werte Listen der möglichen Einstellungen sind. In unserem Fall soll die\nBaumtiefe 'max_depth' des Entscheidungsbaums justiert werden. Wie zuvor in der\nfor-Schleife, untersuchen wir die Baumtiefen 3, 4, 5 und 6, die im folgenden\nDictionary parameter_gitter definiert werden.\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Festlegung des Suchraumes\nparameter_gitter = {'max_depth': [3, 4, 5, 6]}\n\n\n\nNun instanziieren wir ein neues GridSearchCV-Modell. Als erstes Argument\nübergeben wir das eigentliche Modell, hier also den Entscheidungsbaum, und als\nzweites das Dictionary mit den Hyperparametern. Das dritte Argument ist die\nMethode zur Kreuzvalidierung. Weitere Details können Sie der \n\nDokumentation\nScikit-Learn →\nGridSearchCV\nentnehmen.\n\noptimiertes_modell = GridSearchCV(DecisionTreeClassifier(), param_grid=parameter_gitter, cv=kfold)\n\n\n\nMit der Methode .fit() wird die Gittersuche samt Kreuzvalidierung\ndurchgeführt. Dabei werden systematisch alle Parameterkombinationen getestet,\nund das optimierte Modell wird abschließend erneut auf den gesamten\nTrainingsdaten trainiert.\n\noptimiertes_modell.fit(X_train, y_train)\n\n\n\nMit der Methode .score() können wir die Modellgüte sowohl auf den Trainings-\nals auch auf den Testdaten bewerten. Auch die Methode .predict() funktioniert\nwie gewohnt.\n\nopt_score_train = optimiertes_modell.score(X_train, y_train)\nopt_score_test  = optimiertes_modell.score(X_test, y_test)\n\nprint(f'optimierter Entscheidungsbaum Score Trainingsdaten: {opt_score_train:.2f}')\nprint(f'optimierter Entscheidungsbaum Score Testdaten: {opt_score_test:.2f}')\n\n\n\nZusätzlich zu den Standardmethoden wie .fit(), .predict() und .score()\nkönnen wir mit dem Attribut best_params_ herausfinden, welche\nHyperparameter-Kombination am besten abgeschnitten hat.\n\nprint(optimiertes_modell.best_params_)\n\n\n\nIn diesem Fall ergibt die Gittersuche, dass die optimale Baumtiefe 3 beträgt.\n\nWarum sprechen wir von einer Gittersuche? Normalerweise wollen wir nicht nur\neinen Hyperparameter optimieren, sondern mehrere gleichzeitig. Beispielsweise\nkönnten wir neben der Baumtiefe auch die minimale Anzahl an Datenpunkten pro\nBlatt (min_samples_leaf) optimieren. Dies führt dazu, dass wir jede\nKombination von max_depth mit jedem Wert von min_samples_leaf testen. So\nentsteht ein zweidimensionales Gitter, das die Gittersuche effizient durchläuft.\nWir müssen lediglich das Dictionary entsprechend erweitern. In diesem Beispiel\nwerden 4 Baumtiefen und 3 Werte für min_samples_leaf kombiniert, was zu\ninsgesamt 4 x 3 = 12 Hyperparameter-Kombinationen führt. Da wir 10-fache\nKreuzvalidierung verwenden, werden insgesamt 120 Modelle trainiert und bewertet.\n\nparameter_gitter = {\n    'max_depth': [3, 4, 5, 6],\n    'min_samples_leaf': [1, 2, 3]\n}\n\noptimiertes_modell = GridSearchCV(DecisionTreeClassifier(), param_grid=parameter_gitter, cv=kfold)\noptimiertes_modell.fit(X_train, y_train)\n\nopt_score_train = optimiertes_modell.score(X_train, y_train)\nopt_score_test  = optimiertes_modell.score(X_test, y_test)\n\nprint(f'optimierter Entscheidungsbaum Score Trainingsdaten: {opt_score_train:.2f}')\nprint(f'optimierter Entscheidungsbaum Score Testdaten: {opt_score_test:.2f}')\n\nprint(optimiertes_modell.best_params_)\n\n\n\nAuch wenn bei diesem einfachen Beispiel die Unterschiede zwischen den Modellen\ngering sind und die Vorteile der Gittersuche mit Kreuzvalidierung nicht sofort\nersichtlich werden, ist diese Methode bei größeren Datensätzen und komplexeren\nModellen ein sehr wertvolles Werkzeug zur Modelloptimierung, bei der alle\nmöglichen Kombinationen von Hyperparametern systematisch getestet werden. Dies\nkann jedoch sehr rechenintensiv sein, besonders wenn der Suchraum groß ist\noder komplexe Modelle verwendet werden. Daher unterstützt GridSearchCV die\nParallelisierung der Berechnungen, indem es mehrere Kerne verwendet, um die\nRechenzeit signifikant zu verkürzen, was besonders bei größeren Datensätzen von\nVorteil ist.\n\nEine Alternative zu GridSearchCV ist RandomizedSearchCV. Dieses Verfahren\ntestet eine zufällige Auswahl von Parametern testet und spart so Zeit, während\nes dennoch gute Ergebnisse liefert. Mehr Details dazu finden Sie in der\n\n\nDokumentation Scikit-Learn →\nRandomizedSearchCV.\n\nVideo “GridSearchCV” von Normalized Nerd","type":"content","url":"/chapter11-sec02#gittersuche-mit-kreuzvalidierung-gridsearchcv","position":7},{"hierarchy":{"lvl1":"11.2 Gittersuche","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter11-sec02#zusammenfassung-und-ausblick","position":8},{"hierarchy":{"lvl1":"11.2 Gittersuche","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Kapitel haben wir erstmals systematisch Hyperparameter optimiert und\ndabei die Gittersuche mit Kreuzvalidierung angewendet. Im nächsten Kapitel\nlernen wir ein weiteres Werkzeug kennen, das nicht nur verschiedene Modelle,\nsondern auch deren Hyperparameter optimiert und anschließend Modellvorschläge\nbasierend auf den besten Einstellungen macht.","type":"content","url":"/chapter11-sec02#zusammenfassung-und-ausblick","position":9},{"hierarchy":{"lvl1":"Übungen"},"type":"lvl1","url":"/chapter11-sec04","position":0},{"hierarchy":{"lvl1":"Übungen"},"content":"Der Datensatz Pinguine stammt von \n\nHuggingFace. Der Datensatz umfasst Daten von Pinguinen, insbesondere die Merkmale\n\nArt,\n\nInsel,\n\nSchnabellaenge und Schnabeltiefe in Millimetern\n\nFlossenlaenge in Millimetern,\n\nKoerpergewicht in Gramm,\n\nGeschlecht und\n\nJahr der Geburt.\n\nLaden Sie die deutsche Variante des Datensatzes aus campUAS und führen Sie eine\nexplorative Datenanalyse durch. Legen Sie 10 % der Daten als Testdaten zurück.\nTrainieren Sie dann ML-Modelle ggf. mit Gittersuche und wählen Sie das beste\nModell aus. Welchen Score erreicht Ihr Modell für die Testdaten?\n\nÜberblick über die Daten\n\nWelche Daten enthält der Datensatz? Wie viele Pinguine sind in der Tabelle enthalten? Wie viele Merkmale werden dort beschrieben? Sind die Daten vollständig?\n\nLösungimport pandas as pd \n\ndaten = pd.read_csv('pinguine.csv', skiprows=3)\ndaten.info()\n\nDie Tabelle enthält 344 Pinguine. Es sind 8 Merkmale enthalten, nämlich die Merkmale Art, Insel, Schnabellaenge_mm, Schnabeltiefe_mm, Flossenlaenge_mm, Koerpergewicht_g, Geschlecht, Jahr. Die Merkmale Schnabellaenge_mm, Schnabeltiefe_mm, Flossenlaenge_mm und Geschlecht sind unvollständig.\n\nDatentypen\n\nWelchen Datentyp haben die Merkmale? Welche Merkmale sind numerisch und welche sind kategorial?\n\nLösung\n\nArt --> object\n\nInsel --> object\n\nSchnabellaenge_mm --> float\n\nSchnabeltiefe_mm --> float\n\nFlossenlaenge_mm --> float\n\nKoerpergewicht_g --> float\n\nGeschlecht --> object\n\nJahr --> intmerkmale = daten.columns\n\nfor m in merkmale:\n    anzahl_einzigartiger_eintraege = len(daten[m].unique())\n    print(f'Merkmal {m} hat {anzahl_einzigartiger_eintraege} Einträge.')\n\nDie Merkmale Art, Insel, Geschlecht und Jahr sind kategorial. Die Merkmale Schnabellaenge_mm, Schnabeltiefe_mm, Flossenlaenge_mm, Koerpergewicht_g sind numerisch.\n\nFehlende Einträge\n\nIn welcher Spalte fehlen am meisten Einträge? Filtern Sie den Datensatz nach den fehlenden Einträgen und geben Sie eine Liste mit den Indizes (Zeilennummern) aus, wo Einträge fehlen. Löschen Sie anschließend diese Zeilen aus dem Datensatz. Sind jetzt alle Einträge gültig?\n\nLösungfehlende_geschlechtsangaben = daten[ daten['Geschlecht'].isnull() ].index\nprint(fehlende_geschlechtsangaben)daten = daten.drop(fehlende_geschlechtsangaben)\ndaten.info()\n\nJetzt sind alle Einträge gültig.\n\nAnalyse numerische Daten\n\nErstellen Sie eine Übersicht der statistischen Merkmale für die numerischen Daten. Visualisieren Sie anschließend die statistischen Merkmale mit Boxplots. Verwenden Sie ein Diagramm für die Merkmale, die in Millimetern gemessen werden und ein Diagramm für das Körpergewicht. Interpretieren Sie die statistischen Merkmale. Gibt es Ausreißer? Sind die Werte plausibel?\n\nLösungnumerische_merkmale = ['Schnabellaenge_mm', 'Schnabeltiefe_mm', 'Flossenlaenge_mm', 'Koerpergewicht_g']\n\ndaten[numerische_merkmale].describe()import plotly.express as px \n\nfig = px.box(daten[['Schnabellaenge_mm', 'Schnabeltiefe_mm', 'Flossenlaenge_mm']],\n    labels={'variable': 'Merkmal', 'value':'Wert'},\n    title='Numerische Merkmale der Pinguine (gemessen in Millimetern)')\nfig.show()\n\nDie Schnabeltiefe ist deutlich kleiner als die Schnabellänge. Bei der Flossenlänge fällt auf, das der Median deutlich kleiner ist als der Mittelwert. Insgesamt gibt es keine Ausreißer.import plotly.express as px \n\nfig = px.box(daten[['Koerpergewicht_g']],\n    labels={'variable': 'Merkmal', 'value':'Wert'},\n    title='Numerische Merkmale der Pinguine (gemessen in Gramm)')\nfig.show()\n\nBeim Körpergewicht gibt es keine Besonderheiten, die Werte erscheinen plausibel.\n\nAnalyse der kategorialen Werte\n\nUntersuchen Sie die kategorialen Daten. Sind es wirklich kategoriale Daten? Prüfen Sie für jedes kategoriale Merkmal die Einzigartigkeit der auftretenden Werte und erstellen Sie ein Balkendiagramm mit den Häufigkeiten.\n\nKommen alle Pinguin-Arten auf allen Inseln vor?\n\nLösungkategoriale_merkmale = ['Art', 'Insel', 'Geschlecht', 'Jahr']\n\nfor k in kategoriale_merkmale:\n    print(daten[k].unique())\n\nEs gibt drei Pinguin-Arten: Adelie, Gentoo und Chinstrap. Sie leben auf drei Inseln: Torgersen, Biscoe und Dream. Es gibt zwei Geschlechter (männlich/weiblich) und die Pinguine wurden in den Jahren 2007, 2008 und 2009 geboren. Bei allen Merkmalen handelt es sich tatsächlich um kategoriale Daten.for k in kategoriale_merkmale:\n\n    fig = px.bar(daten[k].value_counts(),\n        labels={'value': 'Anzahl'},\n        title=f'Histogramm des Merkmals {k}')\n    fig.show()\n\nBei den Jahren und dem Geschlecht gibt es jeweils ungefähr gleich viele\nPinguine, aber die Arten und die Inseln sind nicht gleich häufig.for art in ['Adelie', 'Gentoo', 'Chinstrap']:\n    pinguine_art = daten[ daten['Art'] == art ]\n    print(f'\\nArt: {art}')\n    print(pinguine_art['Insel'].value_counts())\n\n\nDie Pinguin-Art Adelie kommt auf allen drei Inseln vor. Die Pinguin-Art Gentoo ist nur auf der Insel Biscoe zu finden, während Chinstrap-Pinguine nur auf der Insel Dream zu finden sind.\n\nML-Modell\n\nIm Folgenden soll die Art der Pinguine anhand der numerischen Merkmale Schnabellaenge_mm, Schnabeltiefe_mm, Flossenlaenge_mm und Koerpergewicht_g klassifiziert werden.\n\nTrainieren Sie nun drei ML-Modelle:\n\nEntscheidungsbaum (Decision Tree),\n\nRandom Forests und\n\nSVM.\n\nFühren Sie dazu vorab einen Split in Trainings- und Testdaten durch. Verwenden Sie Kreuzvalidierung und/oder Gittersuche, um die Hyperparameter zu justieren. Für welches Modell würden Sie sich entscheiden? Begründen Sie Ihre Wahl.\n\nLösungX = daten[['Schnabellaenge_mm', 'Schnabeltiefe_mm', 'Flossenlaenge_mm', 'Koerpergewicht_g']]\n\nkodierung = {\n  'Adelie': '0',\n  'Gentoo': '1', \n  'Chinstrap': '2'\n}\n\ndaten['Art'] = daten['Art'].replace(kodierung)\ndaten['Art'] = daten['Art'].astype('int')\ny = daten['Art']from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)from sklearn.tree import DecisionTreeClassifier\n\n# Festlegung des Suchraumes\nparameter_gitter = {'max_depth': [None, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 50]}\n\n# Konfiguration der Kreuzvalidierung\nkfold = KFold(n_splits=10)\n\n# Gittersuche mit Kreuzvalidierung\nentscheidungsbaum = GridSearchCV(DecisionTreeClassifier(), param_grid=parameter_gitter, cv=kfold)\nentscheidungsbaum.fit(X_train, y_train)\nprint(entscheidungsbaum.score(X_train, y_train))\nprint(entscheidungsbaum.score(X_test, y_test))\n\n# Bestes Modell\nprint(entscheidungsbaum.best_params_)from sklearn.ensemble import RandomForestClassifier\n\n# Festlegung des Suchraumes\nrandom_forest_gitter = {'max_depth': [None, 1, 2, 3, 4, 5],\n                        'min_samples_leaf': [1, 2, 3, 4, 5],\n                        'n_estimators': [50, 100, 200]}\n\n# Konfiguration der Kreuzvalidierung\nkfold = KFold(n_splits=10)\n\n# Gittersuche mit Kreuzvalidierung\nrandom_forest_modell = GridSearchCV(RandomForestClassifier(), param_grid=random_forest_gitter, cv=kfold)\nrandom_forest_modell.fit(X_train, y_train)\nprint(random_forest_modell.score(X_train, y_train))\nprint(random_forest_modell.score(X_test, y_test))\n\n# Bestes Modell\nprint(random_forest_modell.best_params_)from sklearn.svm import SVC\n\nsvm_gitter = {'kernel': ['linear', 'rbf'],\n              'C': [0.1, 1, 10, 1000]\n} \n\n# Konfiguration der Kreuzvalidierung\nkfold = KFold(n_splits=10)\n\n# Gittersuche mit Kreuzvalidierung\nsvm_modell = GridSearchCV(SVC(), param_grid=svm_gitter, cv=kfold)\nsvm_modell.fit(X_train, y_train)\nprint(svm_modell.score(X_train, y_train))\nprint(svm_modell.score(X_test, y_test))\n\n# Bestes Modell\nprint(svm_modell.best_params_)\n\nRandomForest und SVM erzielen gleich gute Scores bei den Testdaten und sind daher beide sehr gut geeignet, als finales Modell verwendet zu werden.","type":"content","url":"/chapter11-sec04","position":1},{"hierarchy":{"lvl1":"12.1 Perzeptron = Grundbaustein neuronaler Netze"},"type":"lvl1","url":"/chapter12-sec01","position":0},{"hierarchy":{"lvl1":"12.1 Perzeptron = Grundbaustein neuronaler Netze"},"content":"Neuronale Netze sind sehr beliebte maschinelle Lernverfahren. Das einfachste\nkünstliche neuronale Netz ist das Perzeptron. In diesem Abschnitt werden wir\ndas Perzeptron vorstellen.","type":"content","url":"/chapter12-sec01","position":1},{"hierarchy":{"lvl1":"12.1 Perzeptron = Grundbaustein neuronaler Netze","lvl2":"Lernziele"},"type":"lvl2","url":"/chapter12-sec01#lernziele","position":2},{"hierarchy":{"lvl1":"12.1 Perzeptron = Grundbaustein neuronaler Netze","lvl2":"Lernziele"},"content":"Lernziele\n\nSie können das Perzeptron als mathematische Funktion formulieren und in diesem\nZusammenhang die folgenden Begriffe erklären:\n\ngewichtete Summe (Weighted Sum),\n\nBias oder Bias-Einheit (Bias),\n\nSchwellenwert (Threshold)\n\nHeaviside-Funktion (Heaviside Function) und\n\nAktivierungsfunktion (Activation Function).\n\nSie können das Perzeptron als ein binäres Klassifikationsproblem des\nüberwachten Lernens einordnen.","type":"content","url":"/chapter12-sec01#lernziele","position":3},{"hierarchy":{"lvl1":"12.1 Perzeptron = Grundbaustein neuronaler Netze","lvl2":"Die Hirnzelle dient als Vorlage für künstliche Neuronen"},"type":"lvl2","url":"/chapter12-sec01#die-hirnzelle-dient-als-vorlage-f-r-k-nstliche-neuronen","position":4},{"hierarchy":{"lvl1":"12.1 Perzeptron = Grundbaustein neuronaler Netze","lvl2":"Die Hirnzelle dient als Vorlage für künstliche Neuronen"},"content":"1943 haben die Forscher Warren McCulloch und Walter Pitts das erste Modell einer\nvereinfachten Hirnzelle präsentiert. Zu Ehren der beiden Forscher heißt dieses\nModell MCP-Neuron. Darauf aufbauend publizierte Frank Rosenblatt 1957 seine Idee\neiner Lernregel für das künstliche Neuron. Das sogenannte Perzeptron bildet die\nGrundlage der künstlichen neuronalen Netze. Inspiriert wurden die Forscher dabei\ndurch den Aufbau des Gehirns und der Verknüpfung der Nervenzellen.\n\n\n\nFigure 1:Schematische Darstellung einer Nervenzelle\n(Quelle:\n\nWikimedia;\nLizenz: \n\nCC BY-SA 3.0)\n\nElektrische und chemische Eingabesignale kommen bei den Dendriten an und laufen\nim Zellkörper zusammen. Sobald ein bestimmter Schwellwert überschritten wird,\nwird ein Ausgabesignal erzeugt und über das Axon weitergeleitet. Mehr Details zu\nNervenzellen finden Sie auf\n\n\nWikipedia/Nervenzelle.\n\n","type":"content","url":"/chapter12-sec01#die-hirnzelle-dient-als-vorlage-f-r-k-nstliche-neuronen","position":5},{"hierarchy":{"lvl1":"12.1 Perzeptron = Grundbaustein neuronaler Netze","lvl2":"Von logischem ODER zur mathematischen Ungleichung"},"type":"lvl2","url":"/chapter12-sec01#von-logischem-oder-zur-mathematischen-ungleichung","position":6},{"hierarchy":{"lvl1":"12.1 Perzeptron = Grundbaustein neuronaler Netze","lvl2":"Von logischem ODER zur mathematischen Ungleichung"},"content":"Das einfachste künstliche Neuron besteht aus zwei Inputs und einem Output. Dabei\nsind für die beiden Inputs nur zwei Zustände zugelassen und auch der Output\nbesteht nur aus zwei verschiedenen Zuständen. In der Sprache des maschinellen\nLernens liegt also eine binäre Klassifikationsaufgabe innerhalb des\nSupervised Learnings vor.\n\nBeispiel:\n\nInput 1: Es regnet oder es regnet nicht.\n\nInput 2: Der Rasensprenger ist an oder nicht.\n\nOutput: Der Rasen wird nass oder nicht.\n\nDen Zusammenhang zwischen Regen, Rasensprenger und nassem Rasen können wir in\neiner Tabelle abbilden:\n\nRegnet es?\n\nIst Sprenger an?\n\nWird Rasen nass?\n\nnein\n\nnein\n\nnein\n\nja\n\nnein\n\nja\n\nnein\n\nja\n\nja\n\nja\n\nja\n\nja\n\nMini-Übung\n\nSchreiben Sie ein kurzes Python-Programm, das abfragt, ob es regnet und ob der\nRasensprenger eingeschaltet ist. Dann soll der Python-Interpreter ausgeben, ob\nder Rasen nass wird oder nicht.\n\n# Ihr Code:\n\n\n\nLösung# Eingabe\nx1 = input('Regnet es (j/n)?')\nx2 = input('Ist der Rasensprenger eingeschaltet? (j/n)')\n\n# Verarbeitung\ny = (x1 == 'j') or (x2 == 'j')\n\n# Ausgabe\nif y == True:\n    print('Der Rasen wird nass.')\nelse:\n    print('Der Rasen wird nicht nass.')\n\nFür das maschinelle Lernen müssen die Daten als Zahlen aufbereitet werden, damit\ndie maschinellen Lernverfahren in der Lage sind, Muster in den Daten zu\nerlernen. Anstatt “Regnet es? Nein.” oder Variablen mit True/False setzen wir\njetzt Zahlen ein. Die Eingabewerte bezeichnen wir mit x1 für Regen und x2 für\nRasensprenger. Die 1 steht für ja, die 0 für nein. Den Output bezeichnen wir mit\ny. Dann lautet die obige Tabelle für das “Ist-der-Rasen-nass-Problem”:\n\nx1\n\nx2\n\ny\n\n0\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n1\n\n1\n\n1\n\n1\n\n1\n\nMini-Übung\n\nSchreiben Sie Ihren Programm-Code der letzten Mini-Übung um. Verwenden Sie\nInteger 0 und 1 für die Eingaben.\n\n# Ihr Code:\n\n\n\nLösung# Eingabe\nx1 = int(input('Regnet es (ja = 1 | nein = 0)?'))\nx2 = int(input('Ist der Rasensprenger eingeschaltet? (ja = 1 | nein = 0)'))\n\n# Verarbeitung\ny = (x1 == 1) or (x2 == 1)\n\n# Ausgabe\nif y == True:\n    print('Der Rasen wird nass.')\nelse:\n    print('Der Rasen wird nicht nass.')\n\nNun ersetzen wir das logische ODER durch ein mathematisches Konstrukt:\nWenn die Ungleichungx_1 w_1  +  x_2 w_2 \\geq \\theta\n\nerfüllt ist, dann ist y = 1 oder anders ausgedrückt, der Rasen wird nass. Und\nansonsten ist y = 0, der Rasen wird nicht nass. Allerdings müssen wir noch die\nGewichte w_1 und w_2 (auf Englisch: weights) geschickt wählen. Die Zahl\n\\theta ist der griechische Buchstabe Theta und steht als Abkürzung für den\nsogenannten Schwellenwert (auf Englisch: threshold).\n\nBeispielsweise würde w_1 = 0.3, w_2=0.3 und \\theta = 0.2 passen:\n\n0 \\cdot 0.3 + 0 \\cdot 0.3 = 0.0 \\geq 0.2 nicht erfüllt\n\n0 \\cdot 0.3 + 1 \\cdot 0.3 = 0.3 \\geq 0.2 erfüllt\n\n1 \\cdot 0.3 + 0 \\cdot 0.3 = 0.3 \\geq 0.2 erfüllt\n\n1 \\cdot 0.3 + 1 \\cdot 0.3 = 0.6 \\geq 0.2 erfüllt\n\nMini-Übung\n\nSchreiben Sie Ihren Programm-Code der letzten Mini-Übung um. Ersetzen Sie das\nlogische ODER durch die linke Seite der Ungleichung und vergleichen Sie\nanschließend mit 0.2, um zu entscheiden, ob der Rasen nass wird oder nicht.\n\n# Ihr Code:\n\n\n\nLösung# Eingabe\nx1 = int(input('Regnet es (ja = 1 | nein = 0)?'))\nx2 = int(input('Ist der Rasensprenger eingeschaltet? (ja = 1 | nein = 0)'))\n\n# Verarbeitung\ny = 0.3 * x1 + 0.3 * x2\n\n# Ausgabe\nif y >= 0.2:\n    print('Der Rasen wird nass.')\nelse:\n    print('Der Rasen wird nicht nass.')\n\n","type":"content","url":"/chapter12-sec01#von-logischem-oder-zur-mathematischen-ungleichung","position":7},{"hierarchy":{"lvl1":"12.1 Perzeptron = Grundbaustein neuronaler Netze","lvl2":"Die Heaviside-Funktion ersetzt die Ungleichungsprüfung"},"type":"lvl2","url":"/chapter12-sec01#die-heaviside-funktion-ersetzt-die-ungleichungspr-fung","position":8},{"hierarchy":{"lvl1":"12.1 Perzeptron = Grundbaustein neuronaler Netze","lvl2":"Die Heaviside-Funktion ersetzt die Ungleichungsprüfung"},"content":"Noch sind wir aber nicht fertig, denn auch die Frage “Ist die Ungleichung\nerfüllt oder nicht?” muss noch in eine mathematische Funktion umgeschrieben\nwerden. Dazu subtrahieren wir zuerst auf beiden Seiten der Ungleichung den\nSchwellenwert \\theta:-\\theta + x_1 w_1  +  x_2 w_2 \\geq 0.\n\nDamit haben wir jetzt nicht mehr einen Vergleich mit dem Schwellenwert, sondern\nmüssen nur noch entscheiden, ob der Ausdruck -\\theta + x_1 w_1 + x_2 w_2\nnegativ oder positiv ist. Bei negativen Werten, soll y = 0 sein und bei\npositiven Werten (inklusive der Null) soll y = 1 sein. Dafür gibt es in der\nMathematik eine passende Funktion, die sogenannte\n\n\nHeaviside-Funktion (manchmal\nauch Theta-, Stufen- oder Treppenfunktion genannt).\n\n\n\nFigure 2:Schaubild der Heaviside-Funktion\n(Quelle: \n\nWikimedia von\nLennart Kudling; Lizenz: gemeinfrei)\n\nDefiniert ist die Heaviside-Funktion folgendermaßen:\\Phi(x) = \\begin{cases}0:&x<0\\\\1:&x\\geq 0\\end{cases}\n\nIn dem Modul NumPy ist die Heaviside-Funktion schon hinterlegt, siehe\n\nhttps://​numpy​.org​/doc​/stable​/reference​/generated​/numpy​.heaviside​.html\n\nMini-Übung\n\nVisualisieren Sie die Heaviside-Funktion für das Intervall [-3,3] mit 101\nPunkten. Setzen Sie das zweite Argument einmal auf 0 und einmal auf 2. Was\nbewirkt das zweite Argument? Sehen Sie einen Unterschied in der Visualisierung?\n\nErhöhen Sie auch die Anzahl der Punkte im Intervall. Wählen Sie dabei immer eine\nungerade Anzahl an Punkten, damit die 0 dabei ist und der Sprung an der Stelle\nx = 0 besser sichtbar wird.\n\n# Ihr Code:\n\n\n\nLösungimport pandas as pd\nimport plotly.express as px\nimport numpy as np\n\nx = np.linspace(-3, 3, 101)\ny0 = np.heaviside(x, 0)  # an der Stelle x=0 ist y=0\ny1 = np.heaviside(x, 2)  # an der Stelle x=0 ist y=2\n\n# Daten für Plotly Express vorbereiten\ndf = pd.DataFrame({'x': x, 'y0': y0, 'y1': y1})\n\n# Visualisierung\nfig = px.scatter(df, x='x', y=['y0', 'y1'], title='Heaviside-Funktion')\nfig.update_layout(\n    xaxis_title='x',\n    yaxis_title='y'\n)\nfig.show()\n\nDas zweite Argument der Heaviside-Funktion heaviside in NumPy gibt an, welchen\nFunktionswert die Heaviside-Funktion an der Stelle x=0 hat. Der Standard ist\n0.5.\n\nMit der Heaviside-Funktion können wir nun den Vergleich in der\nProgrammverzweigung mit 0.2 durch eine direkte Berechnung ersetzen. Betrachten\nwir den folgenden Programm-Code, der zeigt, wie wir ohne logisches Oder bzw.\nohne Programmverzweigung if-else auskommen.\n\n# Import der notwendigen Module\nimport numpy as np\n\n# Eingabe\nx1 = int(input('Regnet es (ja = 1 | nein = 0)?'))\nx2 = int(input('Ist der Rasensprenger eingeschaltet? (ja = 1 | nein = 0)'))\n\n# Verarbeitung\ny = np.heaviside(-0.2 + 0.3 * x1 + 0.3 * x2, 1.0)\n\n# Ausgabe\nergebnis_als_text = ['Der Rasen wird nicht nass.', 'Der Rasen wird nass.']\nprint(ergebnis_als_text[int(y)])\n\n","type":"content","url":"/chapter12-sec01#die-heaviside-funktion-ersetzt-die-ungleichungspr-fung","position":9},{"hierarchy":{"lvl1":"12.1 Perzeptron = Grundbaustein neuronaler Netze","lvl2":"Das Perzeptron mit mehreren Eingabewerten"},"type":"lvl2","url":"/chapter12-sec01#das-perzeptron-mit-mehreren-eingabewerten","position":10},{"hierarchy":{"lvl1":"12.1 Perzeptron = Grundbaustein neuronaler Netze","lvl2":"Das Perzeptron mit mehreren Eingabewerten"},"content":"Das Perzeptron für zwei Eingabewerte lässt sich in sehr natürlicher Weise auf\nviele Eingabewerte verallgemeinern, die auch mehrere Zustände annehmen können.\nBei den Outputs bleiben wir jedoch dabei, dass nur zwei Zustände angenommen\nwerden können, die wir mit 0 und 1 bezeichnen. Wir betrachten also weiterhin\nbinäre Klassifikationsaufgaben.\n\nWenn wir nicht nur zwei, sondern n Eingabewerte x_i haben, brauchen wir\nentsprechend auch n Gewichte w_i. Um die Notation zu vereinfachen, fassen\nwir die Eingabewerte in einem Spaltenvektor zusammen, also\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}.\n\nAuch die Gewichte fassen wir in einem Spaltenvektor zusammen, also\\mathbf{w} =\n\\begin{pmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n\\end{pmatrix}.\n\nNun lässt sich die Ungleichung recht einfach durch das Skalarprodukt abkürzen:\\mathbf{x}^{T}\\mathbf{w} =\nx_1 w_1 +  x_2 w_2 + \\ldots + x_n w_n \\geq \\theta.\n\nWie bei dem Perzeptron mit zwei Eingängen wird der Schwellenwert \\theta durch\nSubtraktion auf die linke Seite gebracht. Wenn wir jetzt bei dem Vektor\n\\mathbf{w} mit den Gewichten vorne den Vektor um das Element w_0 = -\\theta\nergänzen und den Vektor \\mathbf{x} mit x_0 = 1 erweitern, dann erhalten wir\\mathbf{x}^{T}\\mathbf{w} =\n1 \\cdot (-\\theta) + x_1 w_1 + x_2 w_2 + \\ldots + x_n w_n \\geq 0.\n\nGenaugenommen hätten wir jetzt natürlich für die Vektoren \\mathbf{w} und\n\\mathbf{x} neue Bezeichnungen einführen müssen, aber ab sofort gehen wir immer\ndavon aus, dass ab jetzt immer die erweiterten Vektoren gemeint sind, die um den\nnegativen Schwellenwert -\\theta bzw. die 1 ergänzt wurden. Der negative\nSchwellenwert wird in der ML-Community Bias oder Bias-Einheit (Bias\nUnit) genannt.\n\nUm jetzt klassifizieren zu können, wird auf die gewichtete Summe\n\\mathbf{x}^{T}\\mathbf{w} die Heaviside-Funktion angewendet. In späteren\nKapiteln werden wir sehen, dass auch andere Funktionstypen anstatt der\nHeaviside-Funktion verwendet werden, die bessere mathematische Eigenschaften\nhaben. Im Folgenden nennen wir die Funktion, die auf die gewichtete Summe\nangewendet wird, Aktivierungsfunktion.\n\nWas ist ... ein Perzeptron?\n\nDas Perzeptron ist ein Modell, das Eingaben verarbeitet, indem es erst eine\ngewichtete Summe der Eingaben bildet und dann darauf eine Aktivierungsfunktion\nanwendet.\n\nEine typische Visualisierung des Perzeptrons ist in der folgenden Abbildung\ngezeigt. Die Eingaben werden durch Kreise symbolisiert. Die Multiplikation der\nInputs x_i mit den Gewichten w_i wird durch Kanten dargestellt. Die\neinzelnen Summanden x_i w_i treffen sich sozusagen im mittleren Kreis, wo auf\ndie gewichtete Summe dann eine Aktivierungsfunktion angewendet wird. Das\nErgebnis, der Output \\hat{y} wird dann berechnet und wiederum als Kreis\ngezeichnet.\n\n\n\nFigure 3:Schematische Darstellung eines Perzeptrons (Quelle: eigene Darstellung; Lizenz\n\n\nCC BY-NC-SA 4.0)","type":"content","url":"/chapter12-sec01#das-perzeptron-mit-mehreren-eingabewerten","position":11},{"hierarchy":{"lvl1":"12.1 Perzeptron = Grundbaustein neuronaler Netze","lvl2":"Zusammenfassung und Ausblick"},"type":"lvl2","url":"/chapter12-sec01#zusammenfassung-und-ausblick","position":12},{"hierarchy":{"lvl1":"12.1 Perzeptron = Grundbaustein neuronaler Netze","lvl2":"Zusammenfassung und Ausblick"},"content":"In diesem Kapitel haben wir gelernt, wie ein Perzeptron aufgebaut ist und wie\naus den Daten mit Hilfe von Gewichten und einer Aktivierungsfunktion der binäre\nZustand prognostiziert wird. Im nächsten Kapitel werden wir mehrere Perzeptrons\nzu mehrschichtigen Netzen verbinden und erhalten ein neuronales Netz.","type":"content","url":"/chapter12-sec01#zusammenfassung-und-ausblick","position":13},{"hierarchy":{"lvl1":"12.2 Mehrschichtiges Perzeptron"},"type":"lvl1","url":"/chapter12-sec02","position":0},{"hierarchy":{"lvl1":"12.2 Mehrschichtiges Perzeptron"},"content":"Lernziele\n\nSie wissen, was ein Multilayer-Perzeptron (MLP), also ein mehrschichtiges Perzeptron, ist.\n\nSie können den Begriff Deep Learning erklären.\n\nSie können mit Scikit-Learn ein neuronales Netz trainieren.\n\n","type":"content","url":"/chapter12-sec02","position":1},{"hierarchy":{"lvl1":"12.2 Mehrschichtiges Perzeptron","lvl2":"Viele Perzeptronen sind ein neuronales Netz"},"type":"lvl2","url":"/chapter12-sec02#viele-perzeptronen-sind-ein-neuronales-netz","position":2},{"hierarchy":{"lvl1":"12.2 Mehrschichtiges Perzeptron","lvl2":"Viele Perzeptronen sind ein neuronales Netz"},"content":"In einem vorhergehenden Kapitel haben wir das Perzeptron, ein künstliches Neuron\nkennengelernt. Schematisch können wir es folgendermaßen darstellen:\n\n\n\nSchematische Darstellung eines Perzeptrons\n\nJedes Eingangssignal wird mit einem Gewicht multipliziert. Anschließend werden\ndie gewichteten Eingangssignale summiert. Übersteigt die gewichtete Summe einen\nSchwellenwert, feuert sozusagen das künstliche Neuron. Das Ausgabesignal wird\naktiviert.\n\nMathematisch gesehen, wurde nach dem Bilden der gewichteten Summe die\nHeaviside-Funktion angewendet. Aber es können auch andere Funktionen zum Einsatz\nkommen. Bei der logistischen Regression wird beispielsweise die Sigmoid-Funktion\nverwendet. Bei neuronalen Netzen sind insbesondere die\n\n\nReLU-Funktion\n(rectified linear unit)\n\n\n\nReLU-Funktion\n\nund der \n\nTangens hyperbolicus\n\n\n\nTangens hyperbolicus\n\nhäufig eingesetzte Aktivierungsfunktionen.\n\nOft werden diese beiden Schritte — Bilden der gewichteten Summe und Anwenden\nder Aktivierungsfunktion — in einem Symbol gemeinsam dargestellt, wie in der\nfolgenden Abbildung zu sehen.\n\n\n\nVereinfachte schematische Darstellung eines Perzeptrons\n\nTatsächlich sind sogar häufig Darstellungen verbreitet, bei denen nur noch durch\ndie Kreise das Perzeptron oder das künstliche Neuron symbolisiert wird.\n\n\n\nSymbolbild eines Perzeptrons bzw. eines künstlichen Neurons\n\nDie Idee des mehrschichtigen Perzeptrons ist es, eine oder mehrere\nZwischenschichten einzuführen. In dem folgenden Beispiel wird eine\nZwischenschichtmit zwei Neuronen eingeführt:\n\n\n\nEin mehrschichtiges Perzeptron (Mulitilayer Perceptron)\n\nEs können beliebig viele Zwischenschichten eingeführt werden. Jede neue\nZwischenschicht kann dabei unterschiedliche Anzahlen von Neuronen enthalten.\nInsgesamt nennen wir die so entstehende Rechenvorschrift mehrschichtiges\nPerzeptron oder Multilayer Perceptron oder neuronales Netz.\n\nDas folgende Video fasst die Struktur eines neuronalen Netzes noch einmal zusammen.\n\nVideo","type":"content","url":"/chapter12-sec02#viele-perzeptronen-sind-ein-neuronales-netz","position":3},{"hierarchy":{"lvl1":"12.2 Mehrschichtiges Perzeptron","lvl2":"Viele Schichten = Deep Learning"},"type":"lvl2","url":"/chapter12-sec02#viele-schichten-deep-learning","position":4},{"hierarchy":{"lvl1":"12.2 Mehrschichtiges Perzeptron","lvl2":"Viele Schichten = Deep Learning"},"content":"Bei neuronalen Netzen werden viele Schichten mit vielen Neuronen in die\nRechenvorschrift einbezogen. Das führt dazu, dass vor allem sogenannte tiefe\nneuronale Netze, also solche mit vielen Schichten, extrem leistungsfähig sind.\nUmgekehrt benötigen neuronale Netze aber auch eine große Anzahl an\nTrainingsdaten mit guter Qualität.\n\nDie Firma Linguee verfügte genau über solche Deutsch-Englisch-Übersetzungen.\n2017 trainierten Mitarbeiter dieses Unternehmens auf Basis dieser Übersetzungen\nein neuronales Netz, das die bisher dahin existierenden Übersetzungsdienste von\nbeispielsweise Google Translate bei Weitem übertraf. 2022 wurde das daraus\ngegründete Start-Up DeepL zum sogenannten Einhorn, also zu einem Start-Up, das\nmit mehr als 1 Milliarde Dollar bewertet wird (siehe\n\n\nArtikel).","type":"content","url":"/chapter12-sec02#viele-schichten-deep-learning","position":5},{"hierarchy":{"lvl1":"12.3 Neuronale Netze mit Scikit-Learn"},"type":"lvl1","url":"/chapter12-sec03","position":0},{"hierarchy":{"lvl1":"12.3 Neuronale Netze mit Scikit-Learn"},"content":"Lernziele\n\nSie können mit Scikit-Learn ein neuronales Netz zur Klassifikation trainieren.\n\n","type":"content","url":"/chapter12-sec03","position":1},{"hierarchy":{"lvl1":"12.3 Neuronale Netze mit Scikit-Learn","lvl2":"Neuronale Netze zur Klassifikation"},"type":"lvl2","url":"/chapter12-sec03#neuronale-netze-zur-klassifikation","position":2},{"hierarchy":{"lvl1":"12.3 Neuronale Netze mit Scikit-Learn","lvl2":"Neuronale Netze zur Klassifikation"},"content":"Schauen wir uns an, wie das Training eines tiefen neuronalen Netzes in\nScikit-Learn funktioniert. Dazu benutzen wir aus dem Untermodul\nsklearn.neural_network den MLPClassifier, also ein\nMulti-Layer-Perzeptron für Klassifikationsaufgaben:\n\nhttps://​scikit​-learn​.org​/stable​/modules​/generated​/sklearn​.neural​_network​.MLPClassifier​.html​#sklearn​.neural​_network​.MLPClassifier\n\nWir benutzen künstliche Daten, um die Anwendung des MLPClassifiers zu\ndemonstrieren.\n\nimport pandas as pd\nimport plotly.express as px\nfrom sklearn.datasets import make_circles\n\n\n# Generiere künstliche Daten\nX, y = make_circles(noise=0.2, factor=0.5, random_state=1)\n\n# Konvertierung in ein DataFrame-Objekt für Plotly Express\ndf = pd.DataFrame({\n    'Feature 1': X[:, 0],\n    'Feature 2': X[:, 1],\n    'Category': pd.Series(y, dtype='category')\n})\n\n# Visualisierung\nfig = px.scatter(df, x='Feature 1', y='Feature 2', color='Category',\n                 title='Künstliche Daten')\nfig.show()\n\n\n\n\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Auswahl des Models\n# solver = 'lbfgs' für kleine Datenmengen, solver = 'adam' für große Datenmengen, eher ab 10000\n# hidden_layer: Anzahl der Neuronen pro verdeckte Schicht und Anzahl der verdeckten Schichten\nmodel = MLPClassifier(solver='lbfgs', hidden_layer_sizes=[5, 5])\n\n# Split Trainings- / Testdaten\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n\n# Training\nmodel.fit(X_train, y_train)\n\n# Validierung \nscore_train = model.score(X_train, y_train)\nscore_test = model.score(X_test, y_test)\nprint(f'Score für Trainingsdaten: {score_train:.2f}')\nprint(f'Score für Testdaten: {score_test:.2f}')\n\n\n\nFunktioniert gar nicht mal schlecht :-) Wir zeichen die Entscheidungsgrenzen\nein, um zu sehen, wo das neuronale Netz die Trennlinien zieht.\n\nimport plotly.graph_objects as go\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_circles\n\n# Generate synthetic data\nX, y = make_circles(noise=0.2, factor=0.5, random_state=1)\n\n# Create grid for contour plot\ngridX, gridY = np.meshgrid(np.linspace(-1.5, 1.5, 50), np.linspace(-1.5, 1.5, 50))\ngridZ = model.predict_proba(np.column_stack([gridX.ravel(), gridY.ravel()]))[:, 1]\nZ = gridZ.reshape(gridX.shape)\n\n# Create scatter plot\nscatter = go.Scatter(x=df['Feature 1'], y=df['Feature 2'], mode='markers',\n                     marker=dict(color=df['Category'], colorscale='BlueRed_r'))\n\n# Create contour plot\ncontour = go.Contour(x=np.linspace(-1.5, 1.5, 50), y=np.linspace(-1.5, 1.5, 50), z=Z, \n                     opacity=0.2, colorscale='BlueRed_r')\n\n# Create figure and add plots\nfig = go.Figure()\nfig.add_trace(contour)\nfig.add_trace(scatter)\nfig.update_layout(title='Künstliche Messdaten und Konturen des Modells',\n                  xaxis_title='Feature 1',\n                  yaxis_title='Feature 2')\nfig.show()\n\n\n\nIm Folgenden wollen wir uns ansehen, welche Bedeutung die optionalen Parameter\nhaben. Dazu zunächst noch einmal der komplette Code, aber ohne einen Split in\nTrainings- und Testdaten. Probieren Sie nun unterschiedliche Werte für die\nArchitektur der verdeckten Schicht ‘hidden_layer_sizes’ aus.\n\n# setze verschiedene Werte für die Architektur der verdeckten Schicht\nmy_hidden_layers = [10,10]\n\n# erzeuge künstliche Daten\nX,y = make_circles(noise=0.2, factor=0.5, random_state=1)\n\n# Auswahl des Model\nmodel = MLPClassifier(solver='lbfgs', hidden_layer_sizes=my_hidden_layers)\n\n# Training und Validierung\nmodel.fit(X, y)\nprint('Score: {:.2f}'.format(model.score(X, y)))\n\n# Visualisierung\n# Create grid for contour plot\ngridX, gridY = np.meshgrid(np.linspace(-1.5, 1.5, 50), np.linspace(-1.5, 1.5, 50))\ngridZ = model.predict_proba(np.column_stack([gridX.ravel(), gridY.ravel()]))[:, 1]\nZ = gridZ.reshape(gridX.shape)\n\n# Create scatter plot\nscatter = go.Scatter(x=df['Feature 1'], y=df['Feature 2'], mode='markers',\n                     marker=dict(color=df['Category'], colorscale='BlueRed_r'))\n\n# Create contour plot\ncontour = go.Contour(x=np.linspace(-1.5, 1.5, 50), y=np.linspace(-1.5, 1.5, 50), z=Z, \n                     opacity=0.2, colorscale='BlueRed_r')\n\n# Create figure and add plots\nfig = go.Figure()\nfig.add_trace(contour)\nfig.add_trace(scatter)\nfig.update_layout(title='Künstliche Messdaten und Konturen des Modells',\n                  xaxis_title='Feature 1',\n                  yaxis_title='Feature 2')\nfig.show()\n\n\n\n\n\n\n\nWie Sie sehen, ist es schwierig, eine gute Architektur des neuronalen Netzes (=\nAnzahl der Neuronen pro verdeckter Schicht und Anzahl verdeckter Schichten) zu\nfinden. Auch fällt das Ergebnis jedesmal ein wenig anders aus, weil\nstochastische Verfahren im Hintergrund für das Trainieren der Gewichte benutzt\nwerden. Aus diesem Grund sollten neuronale Netze nur eingesetzt werden, wenn\nsehr große Datenmengen vorliegen und dann noch ist das Finden der besten\nArchitektur eine große Herausforderung.","type":"content","url":"/chapter12-sec03#neuronale-netze-zur-klassifikation","position":3},{"hierarchy":{"lvl1":"Willkommen zu ML4ING"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Willkommen zu ML4ING"},"content":"Verfasst von \n\nSimone Gramsch // Powered by \n\nJupyter Book","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Willkommen zu ML4ING","lvl2":"Inhaltsverzeichnis"},"type":"lvl2","url":"/#inhaltsverzeichnis","position":2},{"hierarchy":{"lvl1":"Willkommen zu ML4ING","lvl2":"Inhaltsverzeichnis"},"content":"Maschinelles Lernen für Ingenieurwissenschaften\n\n1. Grundbegriffe des maschinellen Lernens\n\n1.1 Was ist maschinelles Lernen?\n\n1.2 Überwachtes, unüberwachtes und verstärkendes Lernen\n\n1.3 Technische Voraussetzungen\n\n2. Crashkurs Python\n\n2.1 Datentypen, Variablen und print()\n\n2.2 Listen und for-Schleifen\n\n2.3 Dictionaries, Funktionen und Methoden\n\nÜbungen\n\n3. Pandas und Plotly anstatt Excel\n\n3.1 Pandas Series\n\n3.2 Statistik mit Pandas\n\n3.3 Boxplots mit Plotly\n\nÜbungen\n\n4. Tabellarische Daten\n\n4.1 Datenstruktur DataFrame\n\n4.2 Arbeiten mit Tabellendaten\n\n4.3 Scatterplots und Scattermatrix\n\nÜbungen\n\n5. Kategoriale Daten\n\n5.1 Was sind kategoriale Daten?\n\n5.2 Barplots und Histogramme\n\n5.3 Daten filtern und gruppieren\n\nÜbungen\n\n6. Entscheidungsbäume (Decision Trees)\n\n6.1 Was ist ein Entscheidungsbaum?\n\n6.2 Entscheidungsbäume visualisieren und trainieren\n\n6.3 Entscheidungsbäume in der Praxis\n\nÜbung\n\n7. Lineare Regression\n\n7.1 Einfache lineare Regression\n\n7.2 Multiple lineare Regression\n\n7.3 Polynomiale Regression\n\nÜbungen\n\n8. ML-Workflow Datenvorverarbeitung\n\n8.1 Fehlende Daten\n\n8.2 Trainings- und Testdaten\n\n8.3 Kodierung und Skalierung\n\nÜbungen\n\n9. Ensemble-Methoden (Random Forests und XGBoost)\n\n9.1 Grundideen der Ensemble-Methoden\n\n9.2 Random Forests\n\n9.3 XGBoost\n\nÜbungen\n\n10. Support Vector Machines\n\n10.1 Maximiere den Rand, aber soft\n\n10.2 Training SVM mit Scikit-Learn\n\n10.3 Nichtlineare SVM\n\nÜbung\n\n11. ML-Workflow Modellbewertung und Auswahl\n\n11.1 Kreuzvalidierung\n\n11.2 Gittersuche\n\nÜbungen\n\n12. Neuronale Netze\n\n12.1 Perzeptron = Grundbaustein neuronaler Netze\n\n12.2 Mehrschichtiges Perzeptron\n\n12.3 Neuronale Netze mit Scikit-Learn","type":"content","url":"/#inhaltsverzeichnis","position":3}]}