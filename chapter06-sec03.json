{"version":3,"kind":"Notebook","sha256":"9fa20d9a38d8fe041278a0f7ab7719226db120d7874ef9affc516396454020a5","slug":"chapter06-sec03","location":"/chapter06/chapter06_sec03.md","dependencies":[],"frontmatter":{"title":"6.3 Entscheidungsbäume in der Praxis","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"jupytext":{"formats":"ipynb,md:myst","text_representation":{"extension":".md","format_name":"myst","format_version":"0.13","jupytext_version":"1.15.2"}},"content_includes_title":false,"authors":[{"id":"Simone Gramsch","name":"Simone Gramsch"}],"open_access":true,"license":{"content":{"id":"CC-BY-NC-SA-4.0","url":"https://creativecommons.org/licenses/by-nc-sa/4.0/","name":"Creative Commons Attribution Non Commercial Share Alike 4.0 International","CC":true}},"github":"https://github.com/gramschs/book_ml4ing","numbering":{"title":{"offset":1}},"source_url":"https://github.com/gramschs/book_ml4ing/blob/main/chapter06/chapter06_sec03.md","edit_url":"https://github.com/gramschs/book_ml4ing/edit/main/chapter06/chapter06_sec03.md","exports":[{"format":"md","filename":"chapter06_sec03.md","url":"/build/chapter06_sec03-449ff3cbda3d9e0395993f9d8dfefd54.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":17,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"Entscheidungsbäume bieten viele Vorteile, haben aber auch Nachteile, die wir in\ndiesem Kapitel diskutieren werden. Darüber hinaus lernen wir Methoden kennen,\num bei Entscheidungsbäumen diese Nachteile zu reduzieren.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"F6AeyAH89e"}],"key":"RVfpXpFrxH"},{"type":"heading","depth":2,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"Lernziele","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"fPcHBlVa7f"}],"identifier":"lernziele","label":"Lernziele","html_id":"lernziele","implicit":true,"key":"rWsbc82k35"},{"type":"admonition","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Lernziele","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"l7F0oXVz4l"}],"key":"wScADwOACk"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":25,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":25,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Sie können in eigenen Worten erklären, was ","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"NBoJw18Kyf"},{"type":"strong","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"Overfitting","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"vGLEjwWDmN"}],"key":"wizqrBh1ay"},{"type":"text","value":" (deutsch:\n","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"N94xz2b3gc"},{"type":"strong","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"Überanpassung","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"YcG0YOwJwy"}],"key":"k7BQY9HnNn"},{"type":"text","value":") ist.","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"osAHbNW6Zz"}],"key":"jWBfSjE5Jg"}],"key":"GVEMilNRWn"},{"type":"listItem","spread":true,"position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Sie wissen, was ","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"qgdNdJDCZW"},{"type":"strong","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"Underfitting","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"yZOuE70pqc"}],"key":"aeNxMFRoIF"},{"type":"text","value":" bedeutet.","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"UHlOKDjbSu"}],"key":"sTFw2ZgGyI"}],"key":"vaC3O2FvW1"},{"type":"listItem","spread":true,"position":{"start":{"line":28,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Sie wissen, dass Entscheidungsbäume eine Tendenz zu Overfitting haben und\nMaßnahmen zur Reduzierung von Overfitting ergriffen werden müssen.","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"ROAEA5YtLf"}],"key":"v9Eh8opN1L"}],"key":"dtNdsN7l2M"},{"type":"listItem","spread":true,"position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Sie wissen, was ","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"V5ZIMwwTFW"},{"type":"strong","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"text","value":"Hyperparameter","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"GkIdneWHVX"}],"key":"Juk4EUNqXc"},{"type":"text","value":" sind.","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"ZDFTHKUyO2"}],"key":"XwezfSap9d"}],"key":"msu5Dj0o9E"},{"type":"listItem","spread":true,"position":{"start":{"line":31,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Sie kennen Hyperparameter der Entscheidungsbäume wie beispielsweise","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"H3LaaIW2kI"}],"key":"C0v23cBqVE"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":32,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"maximale Baumtiefe,","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"etohuMpjeH"}],"key":"RENDplDNlJ"}],"key":"nLTRLnoCdT"},{"type":"listItem","spread":true,"position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"minimale Anzahl an Datenpunkten in Knoten oder","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"M8tcdpERAB"}],"key":"scYH9AdBus"}],"key":"djJr9PZ3yr"},{"type":"listItem","spread":true,"position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"minimale Anzahl an Datenpunkten in Blättern.","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"tzRnpAgT4k"}],"key":"xJu2D5vAjy"}],"key":"BfluPjhDyH"}],"key":"KUO0d7IZQk"}],"key":"tTt2eJfX3Y"},{"type":"listItem","spread":true,"position":{"start":{"line":35,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Sie können die Hyperparameter zum ","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"G9lQ9PIEjq"},{"type":"strong","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"Prä-Pruning","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"etcWR8RfWu"}],"key":"oWD0EYjvpq"},{"type":"text","value":" (deutsch: vorab\nZurechtschneiden) geeignet wählen.","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"x3zihZeVGi"}],"key":"Z6GMpuZ37f"}],"key":"LHfl8Gc8ZZ"}],"key":"ZdfedFLDzP"}],"class":"attention","key":"SzG5bOuhRa"},{"type":"heading","depth":2,"position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"text","value":"Die Tendenz von Entscheidungsbäumen zum Overfitting","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"y93hoDPqFm"}],"identifier":"die-tendenz-von-entscheidungsb-umen-zum-overfitting","label":"Die Tendenz von Entscheidungsbäumen zum Overfitting","html_id":"die-tendenz-von-entscheidungsb-umen-zum-overfitting","implicit":true,"key":"NJdpLep7uh"},{"type":"paragraph","position":{"start":{"line":41,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"Entscheidungsbaummodelle bieten zahlreiche Vorteile. Ein wesentlicher Vorzug ist\ndie Möglichkeit, den trainierten Entscheidungsbaum zu visualisieren, wodurch es\nleicht nachvollziehbar wird, welche Merkmale einen signifikanten Einfluss haben.\nEin weiterer Vorteil ist ihre Effizienz bei heterogenen Daten; sowohl numerische\nals auch kategoriale Eigenschaften können problemlos verarbeitet werden.\nEntscheidungsbäume sind selbst bei unterschiedlichen Datenskalen robust und\nerfordern nur wenig Vorverarbeitung.","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"SMkEPJsY84"}],"key":"hD9HKmx1Rw"},{"type":"paragraph","position":{"start":{"line":49,"column":1},"end":{"line":55,"column":1}},"children":[{"type":"text","value":"Trotz dieser Stärken besitzen Entscheidungsbäume eine Neigung zum\n","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"y6q0WhmrYY"},{"type":"strong","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"children":[{"type":"text","value":"Overfitting","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"Ui8Ns4zumb"}],"key":"uFJ9XosViq"},{"type":"text","value":". Overfitting, auch als Überanpassung bekannt, beschreibt ein\nProblem im maschinellen Lernen, bei dem ein Modell die Trainingsdaten zu genau\nlernt. Das klingt zunächst gut, aber das Modell kann dadurch seine Fähigkeit\nverlieren, Vorhersagen für neue, unbekannte Daten zu treffen. Im Gegensatz dazu\nsteht das ","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"O7GaeO5cBR"},{"type":"strong","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"children":[{"type":"text","value":"Underfitting","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"AYmTVyecKG"}],"key":"PKKKwY7IAv"},{"type":"text","value":", das eine zu geringe Anpassung an die Daten bedeutet\nund ebenfalls unerwünscht ist.","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"taQr7Ojuh6"}],"key":"y050QrPClF"},{"type":"paragraph","position":{"start":{"line":57,"column":1},"end":{"line":63,"column":1}},"children":[{"type":"text","value":"Um uns das Problem des Overfittings zu veranschaulichen, betrachten wir erneut\ndas Autohaus-Beispiel, aber diesmal mit mehr Autos. Wir lassen die Autos diesmal\nmit einer in Scikit-Learn eingebauten Funktion zur Generierung von künstlichen\nDaten erzeugen, der sogenannten ","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"key":"hAG4imrCea"},{"type":"inlineCode","value":"make_moons","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"key":"o8K1UMmU9r"},{"type":"text","value":"-Funktion (siehe ","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"key":"d4JJR4VHQj"},{"type":"link","url":"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"children":[{"type":"text","value":"Dokumentation\nScikit-Learn →\nmake_moons","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"key":"HjQJa4dT2d"}],"urlSource":"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html","key":"sI5j8NN2UE"},{"type":"text","value":")\naus dem Module ","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"key":"StjsNvfrug"},{"type":"inlineCode","value":"sklearn.datasets","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"key":"lxf0TAZGJo"},{"type":"text","value":".","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"key":"W01b1SV787"}],"key":"UNa29OFYEW"}],"key":"cDqFxAu318"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from sklearn.datasets import make_moons \n\nX_array, y_array = make_moons(noise = 0.5, n_samples=50, random_state=3)","key":"ytbooE8NVE"},{"type":"outputs","id":"PTLSKkibrsSYlW02rYqAo","children":[],"key":"yDBHcrbZCX"}],"key":"uelaaN11hs"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":71,"column":1},"end":{"line":73,"column":1}},"children":[{"type":"text","value":"Damit die künstlichen Daten besser zu dem Autohaus-Beispiel passen,\ntransformieren wir sie und nutzen die Pandas-Datenstrukturen, um sie effizient\nzu verwalten.","position":{"start":{"line":71,"column":1},"end":{"line":71,"column":1}},"key":"zKpPIVeWqk"}],"key":"E9pBrcR60q"}],"key":"v44KywRyKi"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\nimport pandas as pd\n\n# Transformation der Merkmalswerte in einen positiven Bereich und \n# Umwandlung in eine Integer-Matrix\nX_array = X_array + 1.2 * np.abs(np.min(X_array))\nX_array[:,0] = np.ceil(X_array[:,0] * 30000)\nX_array[:,1] = np.ceil(X_array[:,1] * 10000)\nX = pd.DataFrame(X_array, columns=['Kilometerstand [km]', 'Preis [EUR]'], dtype=(int, int))\n\n# Zuweisung von True/False basierend auf den Kategorien 1 bzw. 0\ny_array = (y_array - 1.0) * (-1)\ny = pd.Series(y_array, name='verkauft', dtype='bool')","key":"Iy19Ky8Fpy"},{"type":"outputs","id":"DVz22SI4sliAKlFqDGvy_","children":[],"key":"BLsdEzxTlh"}],"key":"MwlyZWZkEe"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":91,"column":1},"end":{"line":91,"column":1}},"children":[{"type":"text","value":"Nach der Datenvorbereitung visualisieren wir diese:","position":{"start":{"line":91,"column":1},"end":{"line":91,"column":1}},"key":"kUvkkeQ5jI"}],"key":"HdvpAZOM9f"}],"key":"ChyKtm98Uy"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import plotly.express as px\n\nfig = px.scatter(x = X['Kilometerstand [km]'], y = X['Preis [EUR]'], color=y,\n    title='Künstliche Daten Autohaus',\n    labels={'x': 'Kilometerstand [km]', 'y': 'Preis [EUR]', 'color': 'verkauft'})\nfig.show()","key":"xJkAej4NkI"},{"type":"outputs","id":"fGWKmTnfcB_v1yoPop47P","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.plotly.v1+json":{"content":"{\"data\":[{\"hovertemplate\":\"verkauft=True<br>Kilometerstand [km]=%{x}<br>Preis [EUR]=%{y}<extra></extra>\",\"legendgroup\":\"True\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"True\",\"orientation\":\"v\",\"showlegend\":true,\"x\":{\"dtype\":\"i4\",\"bdata\":\"8RUBAI4XAQDnuQAAdGgBABkEAQBOWQAAkwgBAGiLAAD9jQAAf4wAAH0lAQDU+QAAjhsBAP9GAQALNQAAIiEBADF0AACQUgAAWPkAAKxaAABqiAAAfS8BAEp7AQDOZAEAgb8AAA==\"},\"xaxis\":\"x\",\"y\":{\"dtype\":\"i4\",\"bdata\":\"62AAAOyBAAC2ZgAA1VkAABo3AAAlTgAACmkAALNaAADlbAAAGoAAAKw1AACgawAAcFMAAPwKAAD4OgAAVWMAABdVAAC8WQAAHG4AAH5PAAAeUwAAkTEAAEVuAABoRwAASkEAAA==\"},\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"verkauft=False<br>Kilometerstand [km]=%{x}<br>Preis [EUR]=%{y}<extra></extra>\",\"legendgroup\":\"False\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"False\",\"orientation\":\"v\",\"showlegend\":true,\"x\":{\"dtype\":\"i4\",\"bdata\":\"6X0BAEnOAQA5GQEA37IAALuQAQDFrwEAJV8BAOs/AQDLxAAAsBwBAKSxAQDEAwEABe0AAAS6AQDRlAEA0PQAAK4gAQArCAEAl+4AANDUAACobAEAtpEBAIN2AQCAswEA874BAA==\"},\"xaxis\":\"x\",\"y\":{\"dtype\":\"i2\",\"bdata\":\"xwtKNf9AVV1RdPMVgyXHR1xNo02JK/BOX0fOSq1NADskLfUwpjxlcbckzyVLbvpDyVM=\"},\"yaxis\":\"y\",\"type\":\"scatter\"}],\"layout\":{\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1,\"#f0f921\"]],\"sequentialminus\":[[0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0,1],\"title\":{\"text\":\"Kilometerstand [km]\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0,1],\"title\":{\"text\":\"Preis [EUR]\"}},\"legend\":{\"title\":{\"text\":\"verkauft\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Künstliche Daten Autohaus\"}},\"config\":{\"plotlyServerURL\":\"https://plot.ly\"}}","content_type":"application/vnd.plotly.v1+json"}}},"key":"ixPEahiUVV"}],"key":"xUmh6zWbbd"}],"key":"xmmjKu5Ccw"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":102,"column":1},"end":{"line":103,"column":1}},"children":[{"type":"text","value":"Das Training des Entscheidungsbaumes und dessen Visualisierung erledigt der\nfolgende Code.","position":{"start":{"line":102,"column":1},"end":{"line":102,"column":1}},"key":"Xvg4uZbKko"}],"key":"OGAUxprY3h"}],"key":"dCQwAQFRl2"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from sklearn.tree import DecisionTreeClassifier, plot_tree\n\nmodell = DecisionTreeClassifier(random_state=0)\nmodell.fit(X,y)\n\nplot_tree(modell,\n    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n    class_names=['nicht verkauft', 'verkauft']);","key":"rWF3djJjBP"},{"type":"outputs","id":"Da9hjoCRqhOHKyaUb0IHE","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"f04143ecb736809ebdae7e09fc4c8f79","path":"/build/f04143ecb736809ebdae7e09fc4c8f79.png"}}},"key":"EVbLbkvGWk"}],"key":"uTV1JuMa1X"}],"key":"wc6efUqfJ8"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":116,"column":1},"end":{"line":119,"column":1}},"children":[{"type":"text","value":"Die Visualisierung offenbart zahlreiche Verzweigungen und eine schwer lesbare\nBeschriftung. Die Entscheidungsgrenzen, die im Folgenden mit\n","position":{"start":{"line":116,"column":1},"end":{"line":116,"column":1}},"key":"TnmeGtIlLA"},{"type":"inlineCode","value":"DecisionBoundaryDisplay","position":{"start":{"line":116,"column":1},"end":{"line":116,"column":1}},"key":"xnTvaDXIsp"},{"type":"text","value":" visualisiert werden, zeigen eine zu starke Anpassung\nan die Trainingsdaten.","position":{"start":{"line":116,"column":1},"end":{"line":116,"column":1}},"key":"MwUWaWAjcD"}],"key":"w0S4yvyvFl"}],"key":"m7eulLg3dF"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nfig = DecisionBoundaryDisplay.from_estimator(modell, X, cmap=ListedColormap(['#EF553B33', '#636EFA33']), grid_resolution=1000)\nfig.ax_.scatter(X['Kilometerstand [km]'], X['Preis [EUR]'], c=y, cmap=ListedColormap(['#EF553B', '#636EFA']))\nfig.ax_.set_title('Entscheidungsgrenzen');","key":"XoHFyknGtt"},{"type":"outputs","id":"SvyQF_kWC8L3ANt4TByuz","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"5357933f05d4c7f64eb2daeebe329dbc","path":"/build/5357933f05d4c7f64eb2daeebe329dbc.png"}}},"key":"aWqrpeMsVo"}],"key":"NquYWpzveI"}],"key":"snrHzCB56T"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":132,"column":1},"end":{"line":141,"column":1}},"children":[{"type":"text","value":"Es ist fraglich, ob dieser Entscheidungsbaum nicht zu genau an die\nTrainingsdaten angepasst wurde. Der dünne blaue vertikale Streifen bei ungefähr\n97000 km ist wahrscheinlich keine sinnvolle Entscheidung, sondern eher einem\nAusreißer geschuldet (dem Auto mit einem Kilometerstand von 97098 km und einem\nPreis von 28229 EUR). Der Entscheidungsbaum hat sich zu stark an die Daten\nangepasst. Es ist wahrscheinlich, dass dieser Entscheidungsbaum für Autos mit\neinem Kilometerstand von ungefähr 97000 km falsche Prognosen treffen wird. Wenn\nwir mit den gleichen Daten erneut einen Entscheidungsbaum trainieren lassen und\nden Zufallszahlengenerator mit dem Zustand ","position":{"start":{"line":132,"column":1},"end":{"line":132,"column":1}},"key":"b9BXoSmEno"},{"type":"inlineCode","value":"random_state=1","position":{"start":{"line":132,"column":1},"end":{"line":132,"column":1}},"key":"ueKRDyNI7g"},{"type":"text","value":" initialisieren,\nerhalten wir ein völlig anderes Ergebnis.","position":{"start":{"line":132,"column":1},"end":{"line":132,"column":1}},"key":"aQ5iddcAnr"}],"key":"AzdVLvq0Le"}],"key":"tbtD8A7NoB"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"modell_alternative = DecisionTreeClassifier(random_state=1)\nmodell_alternative.fit(X,y)\n\nfig = DecisionBoundaryDisplay.from_estimator(modell_alternative, X, cmap=ListedColormap(['#EF553B33', '#636EFA33']), grid_resolution=1000)\nfig.ax_.scatter(X['Kilometerstand [km]'], X['Preis [EUR]'], c=y, cmap=ListedColormap(['#EF553B', '#636EFA']))\nfig.ax_.set_title('Entscheidungsgrenzen des alternativen Modells');","key":"ZiIgg3Qis5"},{"type":"outputs","id":"kme0WigIe608u0LV64VD1","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"95b161ea226c5b2ce6ac47becd21bce9","path":"/build/95b161ea226c5b2ce6ac47becd21bce9.png"}}},"key":"Yk2js6l3kp"}],"key":"LSVrgQR4gs"}],"key":"QIsvb0t1V1"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":152,"column":1},"end":{"line":157,"column":1}},"children":[{"type":"text","value":"Eine Möglichkeit, das Overfitting (Überanpassung) an die Daten zu bekämpfen, ist\ndas Zurechtschneiden (Pruning) der Entscheidungsbäume. Eine andere ist, aus\nmehreren Entscheidungbäumen einen »durchschnittlichen« Entscheidungsbaum zu\nbilden. Dieses Verfahren heißt Zufallswald (Random Forest) und wird ausführlich\nin einem eigenen Kapitel behandelt werden. In diesem Kapitel betrachten wir nur\ndas Zurechtschneiden der Entscheidungsbäume.","position":{"start":{"line":152,"column":1},"end":{"line":152,"column":1}},"key":"vJgn5yrt4X"}],"key":"f1njGWygvY"},{"type":"heading","depth":2,"position":{"start":{"line":159,"column":1},"end":{"line":159,"column":1}},"children":[{"type":"text","value":"Zurechtschneiden von Entscheidungsbäumen","position":{"start":{"line":159,"column":1},"end":{"line":159,"column":1}},"key":"CR53o3IltH"}],"identifier":"zurechtschneiden-von-entscheidungsb-umen","label":"Zurechtschneiden von Entscheidungsbäumen","html_id":"zurechtschneiden-von-entscheidungsb-umen","implicit":true,"key":"d4NyLsBXCw"},{"type":"paragraph","position":{"start":{"line":161,"column":1},"end":{"line":172,"column":1}},"children":[{"type":"text","value":"Eine effektive Strategie zur Bekämpfung des Overfittings bei Entscheidungsbäumen\nist das sogenannte ","position":{"start":{"line":161,"column":1},"end":{"line":161,"column":1}},"key":"QP4Te78spq"},{"type":"strong","position":{"start":{"line":161,"column":1},"end":{"line":161,"column":1}},"children":[{"type":"text","value":"Pruning","position":{"start":{"line":161,"column":1},"end":{"line":161,"column":1}},"key":"sCxBIAo3TM"}],"key":"iGHywCCKYa"},{"type":"text","value":", also das Beschneiden des Baumes. Pruning hilft,\ndie Komplexität des Modells zu reduzieren, indem weniger relevante\nEntscheidungszweige nach bestimmten Kriterien entfernt werden. Im Kontext\nunseres Autohaus-Beispiels würde dies bedeuten, dass Entscheidungszweige, die\nbeispielsweise aufgrund von Ausreißern entstanden sind, abgeschnitten werden.\nDies könnte beispielsweise den zuvor erwähnten dünnen blauen Streifen bei einem\nKilometerstand von ungefähr 97000 km betreffen, der wahrscheinlich durch einen\nAusreißer entstanden ist. Durch das Entfernen solcher spezifischen Anpassungen\nkann der Entscheidungsbaum besser verallgemeinern und wird robuster gegenüber\nneuen, unbekannten Daten. Das Ergebnis ist ein Modell, das eine bessere Balance\nzwischen Anpassung an die Trainingsdaten und Generalisierungsfähigkeit aufweist.","position":{"start":{"line":161,"column":1},"end":{"line":161,"column":1}},"key":"ebTBlxupMp"}],"key":"Q56CxVMGEb"},{"type":"paragraph","position":{"start":{"line":174,"column":1},"end":{"line":177,"column":1}},"children":[{"type":"text","value":"Für Entscheidungsbäume gibt es prinzipiell zwei Methoden des Prunings:\n","position":{"start":{"line":174,"column":1},"end":{"line":174,"column":1}},"key":"wNoBktFGUZ"},{"type":"strong","position":{"start":{"line":174,"column":1},"end":{"line":174,"column":1}},"children":[{"type":"text","value":"Prä-Pruning","position":{"start":{"line":174,"column":1},"end":{"line":174,"column":1}},"key":"uQgt4TswMq"}],"key":"kUk9Hc9J5V"},{"type":"text","value":" und ","position":{"start":{"line":174,"column":1},"end":{"line":174,"column":1}},"key":"eE8TCIwi56"},{"type":"strong","position":{"start":{"line":174,"column":1},"end":{"line":174,"column":1}},"children":[{"type":"text","value":"Post-Pruning","position":{"start":{"line":174,"column":1},"end":{"line":174,"column":1}},"key":"akgA3xEEuM"}],"key":"AxwTTVw4Us"},{"type":"text","value":". Das Prä-Pruning findet ","position":{"start":{"line":174,"column":1},"end":{"line":174,"column":1}},"key":"aEQA75NtLo"},{"type":"emphasis","position":{"start":{"line":174,"column":1},"end":{"line":174,"column":1}},"children":[{"type":"text","value":"vor","position":{"start":{"line":174,"column":1},"end":{"line":174,"column":1}},"key":"F5OONNyYPQ"}],"key":"hVuRBabXU2"},{"type":"text","value":" dem Training\ndes Entscheidungsbaumes statt, das Post-Pruning ","position":{"start":{"line":174,"column":1},"end":{"line":174,"column":1}},"key":"tZ8wrJ7ttu"},{"type":"emphasis","position":{"start":{"line":174,"column":1},"end":{"line":174,"column":1}},"children":[{"type":"text","value":"nach","position":{"start":{"line":174,"column":1},"end":{"line":174,"column":1}},"key":"gNi21GdqxG"}],"key":"jrNmsEtX8e"},{"type":"text","value":" dem Training. Die beiden\nwichtigsten Prä-Pruning-Maßnahmen sind","position":{"start":{"line":174,"column":1},"end":{"line":174,"column":1}},"key":"OEhTaF8dpm"}],"key":"h52zbt3s0K"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":179,"column":1},"end":{"line":182,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":179,"column":1},"end":{"line":179,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"die Begrenzung der maximalen Tiefe des Baumes und","position":{"start":{"line":179,"column":1},"end":{"line":179,"column":1}},"key":"WrhTicmKZr"}],"key":"VzPYVgNp9y"}],"key":"NZDlfdus5G"},{"type":"listItem","spread":true,"position":{"start":{"line":180,"column":1},"end":{"line":182,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"die Forderung nach einer Mindestanzahl von Datenpunkten (entweder pro Knoten\noder pro Blatt).","position":{"start":{"line":180,"column":1},"end":{"line":180,"column":1}},"key":"hOLwA4Uqst"}],"key":"Gqdzf7VtvW"}],"key":"HGVWOLjBz4"}],"key":"PVQ66a3xFR"},{"type":"paragraph","position":{"start":{"line":183,"column":1},"end":{"line":186,"column":1}},"children":[{"type":"text","value":"Beim Post-Pruning werden im Nachhinein Knoten mit wenig Informationen aus dem\nEntscheidungsbaum entfernt oder es werden Knoten zusammengelegt. Scikit-Learn\nhat nur Prä-Pruning implementiert, so dass wir hier nicht weiter auf\nPost-Pruning eingehen.","position":{"start":{"line":183,"column":1},"end":{"line":183,"column":1}},"key":"OzgaBFCcVs"}],"key":"T2KCgSQPAu"},{"type":"heading","depth":3,"position":{"start":{"line":188,"column":1},"end":{"line":188,"column":1}},"children":[{"type":"text","value":"Prä-Pruning: Baumtiefe","position":{"start":{"line":188,"column":1},"end":{"line":188,"column":1}},"key":"zS1378e8Dm"}],"identifier":"pr-pruning-baumtiefe","label":"Prä-Pruning: Baumtiefe","html_id":"pr-pruning-baumtiefe","implicit":true,"key":"sIdOdsn4pA"},{"type":"paragraph","position":{"start":{"line":190,"column":1},"end":{"line":196,"column":1}},"children":[{"type":"text","value":"Wir schauen uns zunächst an, wie bei Scikit-Learn-Entscheidungsbäumen die\nmaximale Tiefe festgelegt wird. Bisher haben wir das Modell ohne weitere\nParameter initialisiert (einzige Ausnahme: wir haben ggf. den\nZufallszahlengenerator aus didaktischen Gründen fixiert, damit die Ergebnisse\nvergleichbar sind). Nun verwenden wir bei der Initialisierung des\nDecisionTreeClassifiers das optionale Argument ","position":{"start":{"line":190,"column":1},"end":{"line":190,"column":1}},"key":"QLujAbL1GO"},{"type":"inlineCode","value":"max_depth=","position":{"start":{"line":190,"column":1},"end":{"line":190,"column":1}},"key":"ewy6cbyKL8"},{"type":"text","value":" und setzen es auf\n","position":{"start":{"line":190,"column":1},"end":{"line":190,"column":1}},"key":"A6qrQJxZaC"},{"type":"inlineCode","value":"1","position":{"start":{"line":190,"column":1},"end":{"line":190,"column":1}},"key":"HzRIHXUHh6"},{"type":"text","value":".","position":{"start":{"line":190,"column":1},"end":{"line":190,"column":1}},"key":"mTCyh7G4eG"}],"key":"sT9VDiVffR"}],"key":"u0mWxE6ipD"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"modell_tiefe1 = DecisionTreeClassifier(random_state=0, max_depth=1)\nmodell_tiefe1.fit(X,y)\n\nplot_tree(modell_tiefe1,\n    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n    class_names=['nicht verkauft', 'verkauft']);","key":"wEH4j5zHxD"},{"type":"outputs","id":"lLcv6lij4Dk5GKdNWYgvZ","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"cb8aba09652b28c8f6545fcb0ba207d5","path":"/build/cb8aba09652b28c8f6545fcb0ba207d5.png"}}},"key":"rHlZ5VjJYq"}],"key":"vdKHAJPGMb"}],"key":"CBpFWXHZR2"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":207,"column":1},"end":{"line":213,"column":1}},"children":[{"type":"text","value":"Eine Tiefe von 1 bedeutet, dass nur noch eine einzige Entscheidungsfrage\ngestellt wird. Das reicht nicht mehr, um die Autos in reine Blätter zu\nsortieren. Im linken Blatt sind 13 nicht verkaufte Autos und 24 verkaufte Autos,\nweshalb diesem Blatt die Kategorie »verkauft« zugeordnet wird. Im rechten Blatt\nsind 12 nicht verkaufte Autos und ein verkauftes Auto, so dass dieses Blatt\ninsgesamt als »nicht verkauft« gilt. Die Visualisierung der Entscheidungsgrenzen\nzeigt, um welche Autos es sich handelt.","position":{"start":{"line":207,"column":1},"end":{"line":207,"column":1}},"key":"Q0Q0f0IKhn"}],"key":"LTqrXOugTW"}],"key":"IPYkehfcGe"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"fig = DecisionBoundaryDisplay.from_estimator(modell_tiefe1, X, cmap=ListedColormap(['#EF553B33', '#636EFA33']), grid_resolution=1000)\nfig.ax_.scatter(X['Kilometerstand [km]'], X['Preis [EUR]'], c=y, cmap=ListedColormap(['#EF553B', '#636EFA']))\nfig.ax_.set_title('Entscheidungsgrenzen');","key":"YY8SPBG9tf"},{"type":"outputs","id":"Xb47LFVjXKbhs135NGncS","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"f7a778082f9a47b45e4a1b80340c2c50","path":"/build/f7a778082f9a47b45e4a1b80340c2c50.png"}}},"key":"CPGIsnt8M0"}],"key":"hvbmo75bid"}],"key":"WxpdPLGltR"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":221,"column":1},"end":{"line":223,"column":1}},"children":[{"type":"text","value":"Insbesondere die Visualisierung der Entscheidungsgrenzen zeigt aber auch, dass\ndieser Entscheidungsbaum nicht besonders gut die Daten erklärt. Der Score ist\nmit","position":{"start":{"line":221,"column":1},"end":{"line":221,"column":1}},"key":"aA6EtnvXT9"}],"key":"yYEn1wA7ds"}],"key":"KviFjo0eAo"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"print(f'Score des Entscheidungsbaumes mit Tiefe 1: {modell_tiefe1.score(X,y)}')","key":"BEqySS4rUz"},{"type":"outputs","id":"RuucA9-PfEIg0cY_YyzUY","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"Score des Entscheidungsbaumes mit Tiefe 1: 0.72\n"},"key":"aREUqSxZvz"}],"key":"e1UajcqcoT"}],"key":"eJwfZHkKfR"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":229,"column":1},"end":{"line":229,"column":1}},"children":[{"type":"text","value":"auch nicht so gut. Daher verwenden wir nun als maximale Tiefe des Entscheidungsbaumes einen Wert von 2.","position":{"start":{"line":229,"column":1},"end":{"line":229,"column":1}},"key":"Va9sUDlKkh"}],"key":"YjZgfrWn5b"}],"key":"Wr81TAottd"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"modell_tiefe2 = DecisionTreeClassifier(random_state=0, max_depth=2)\nmodell_tiefe2.fit(X,y)\n\nplot_tree(modell_tiefe2,\n    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n    class_names=['nicht verkauft', 'verkauft']);\n\nprint(f'Score des Entscheidungsbaumes mit Tiefe 2: {modell_tiefe2.score(X,y)}')","key":"QKa32exV7Y"},{"type":"outputs","id":"VAVt_fdNoRTIS17yNhtPo","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"Score des Entscheidungsbaumes mit Tiefe 2: 0.78\n"},"key":"m0PF6cJlFP"},{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"b3447c976699eee489d72a071f3a5660","path":"/build/b3447c976699eee489d72a071f3a5660.png"}}},"key":"gnyFHBMc7P"}],"key":"atjvBvM0ry"}],"key":"UALVo3zxU9"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":242,"column":1},"end":{"line":245,"column":1}},"children":[{"type":"text","value":"Mit einem Score von 0.78 ist der Entscheidungsbaum mit einer maximalen Tiefe von\n2 zwar besser als der Baum mit einer maximalen Tiefe von 1, aber deutlich\nentfernt von dem Score 1.0 bei einer Baumtiefe von 7. Die Entscheidungsgrenzen\nsehen folgendermaßen aus:","position":{"start":{"line":242,"column":1},"end":{"line":242,"column":1}},"key":"gYB4ycvawu"}],"key":"PUbhQ4dOTl"}],"key":"dVY6SQSGFN"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"fig = DecisionBoundaryDisplay.from_estimator(modell_tiefe2, X, cmap=ListedColormap(['#EF553B33', '#636EFA33']), grid_resolution=1000)\nfig.ax_.scatter(X['Kilometerstand [km]'], X['Preis [EUR]'], c=y, cmap=ListedColormap(['#EF553B', '#636EFA']))\nfig.ax_.set_title('Entscheidungsgrenzen');","key":"rXnxRGsi2b"},{"type":"outputs","id":"Qe56QxH0jiNJNwZC220if","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"16d4867c980f97c0d539a8d28c00a66d","path":"/build/16d4867c980f97c0d539a8d28c00a66d.png"}}},"key":"KyB8Hx5CwG"}],"key":"nVm7kKafBY"}],"key":"tnHDW5vYQf"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":253,"column":1},"end":{"line":258,"column":1}},"children":[{"type":"text","value":"Was ist jetzt besser, eine maximale Tiefe von 1 oder 2? Oder doch 3 vielleicht?\nDie Einführung der maximalen Tiefe bietet den Vorteil, das Overfitting zu\nbekämpfen. Der Nachteil davon ist, dass wir jetzt einen neuen Parameter haben,\nder das Training und die Prognose des Modells bestimmt. Und für diesen Parameter\nmuss ein passender Wert eingestellt werden. Solche Parameter nennt man\n","position":{"start":{"line":253,"column":1},"end":{"line":253,"column":1}},"key":"JbYTCBCKRJ"},{"type":"strong","position":{"start":{"line":253,"column":1},"end":{"line":253,"column":1}},"children":[{"type":"text","value":"Hyperparameter","position":{"start":{"line":253,"column":1},"end":{"line":253,"column":1}},"key":"CcUcuUsYBN"}],"key":"JmhUqUex6G"},{"type":"text","value":".","position":{"start":{"line":253,"column":1},"end":{"line":253,"column":1}},"key":"M9nK8LMvNw"}],"key":"SFOy1FtWfa"},{"type":"admonition","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Was ist ... ein Hyperparameter?","position":{"start":{"line":260,"column":1},"end":{"line":260,"column":1}},"key":"NngXAcGdrN"}],"key":"JYmsamo1lN"},{"type":"paragraph","position":{"start":{"line":262,"column":1},"end":{"line":265,"column":1}},"children":[{"type":"text","value":"Ein Hyperparameter ist ein Parameter, der vor dem Training eines Modells\nfestgelegt wird und nicht aus den Daten während des Trainings gelernt wird. Die\nHyperparameter steuern den gesamten Lernprozess und haben einen wesentlichen\nEinfluss auf die Leistung des Modells.","position":{"start":{"line":262,"column":1},"end":{"line":262,"column":1}},"key":"g7oeb25gjP"}],"key":"mbY55OB4X5"}],"class":"note","key":"FBGTGJgNvC"},{"type":"paragraph","position":{"start":{"line":268,"column":1},"end":{"line":274,"column":1}},"children":[{"type":"text","value":"Ein Score von 1.0 auf den Trainingsdaten deutet auf Overfitting hin, d.h. das\nModell hat die Daten auswendig gelernt. Ein sehr niedriger Score (z.B. 0.72)\ndeutet auf Underfitting hin, d.h. das Modell ist zu einfach. Das Ziel ist ein\nGleichgewicht: ein Score, der hoch genug ist, um die Daten gut zu beschreiben,\naber nicht 1.0, um Generalisierung zu ermöglichen. Werte zwischen 0.8 und 0.95\nsind oft ein guter Kompromiss, aber dies muss mit separaten Testdaten validiert\nwerden.","position":{"start":{"line":268,"column":1},"end":{"line":268,"column":1}},"key":"YfjDDxKxtC"}],"key":"lGLZeuh3GF"},{"type":"paragraph","position":{"start":{"line":276,"column":1},"end":{"line":277,"column":1}},"children":[{"type":"text","value":"Kommen wir nun zu einem anderen Hyperparameter der Entscheidungsbäume, der\nMindestanzahl von Datenpunkten.","position":{"start":{"line":276,"column":1},"end":{"line":276,"column":1}},"key":"UbuaXQbcd9"}],"key":"wsUSPQIr3r"},{"type":"heading","depth":3,"position":{"start":{"line":279,"column":1},"end":{"line":279,"column":1}},"children":[{"type":"text","value":"Prä-Pruning: Mindestanzahl Datenpunkte","position":{"start":{"line":279,"column":1},"end":{"line":279,"column":1}},"key":"rGfkuO8KPJ"}],"identifier":"pr-pruning-mindestanzahl-datenpunkte","label":"Prä-Pruning: Mindestanzahl Datenpunkte","html_id":"pr-pruning-mindestanzahl-datenpunkte","implicit":true,"key":"rv8ojpHkAR"},{"type":"paragraph","position":{"start":{"line":281,"column":1},"end":{"line":288,"column":1}},"children":[{"type":"text","value":"Genau wie der Hyperparameter zur Begrenzung der Baumtiefe wird die Mindestanzahl\nder Datenpunkte vorab bei der Initialisierung des Entscheidungsbaumes\nfestgelegt. Scikit-Learn bietet wiederum zwei Möglichkeiten, über die minimale\nAnzahl von Datenpunkten den Entscheidungsbaum zurechtzuschneiden. Zum einen kann\nfür die ","position":{"start":{"line":281,"column":1},"end":{"line":281,"column":1}},"key":"zf9R5g108m"},{"type":"emphasis","position":{"start":{"line":281,"column":1},"end":{"line":281,"column":1}},"children":[{"type":"text","value":"Knoten","position":{"start":{"line":281,"column":1},"end":{"line":281,"column":1}},"key":"dH8mf0bEIC"}],"key":"GLhGsUZn6J"},{"type":"text","value":" eine minimal erforderliche Anzahl von Datenpunkten festgelegt\nwerden, ab der es erlaubt ist, durch Entscheidungsfragen weiter zu verzweigen.\nZum anderen kann eine minimale Anzahl an Datenpunkten für jedes ","position":{"start":{"line":281,"column":1},"end":{"line":281,"column":1}},"key":"j4j4YhHDcn"},{"type":"emphasis","position":{"start":{"line":281,"column":1},"end":{"line":281,"column":1}},"children":[{"type":"text","value":"Blatt","position":{"start":{"line":281,"column":1},"end":{"line":281,"column":1}},"key":"Kr64NveIT4"}],"key":"nJxcvcejOi"},{"type":"text","value":"\nfestgelegt werden, das am Ende der Verzweigungen erreicht werden muss.","position":{"start":{"line":281,"column":1},"end":{"line":281,"column":1}},"key":"e2rMrtQCJ7"}],"key":"iGp3jqZhhr"},{"type":"paragraph","position":{"start":{"line":290,"column":1},"end":{"line":298,"column":1}},"children":[{"type":"text","value":"Wir probieren beide Möglichkeiten aus und vergleichen die Ergebnisse\nmiteinander. Die Option zur Einstellung der Mindestanzahl pro Knoten heißt\n","position":{"start":{"line":290,"column":1},"end":{"line":290,"column":1}},"key":"sPHeL2UVLt"},{"type":"inlineCode","value":"min_samples_split","position":{"start":{"line":290,"column":1},"end":{"line":290,"column":1}},"key":"riYikQSKNC"},{"type":"text","value":" und die Option zur Einstellung des Mindestanzahl Datenpunkte\npro Blatt heißt ","position":{"start":{"line":290,"column":1},"end":{"line":290,"column":1}},"key":"KuToLeLhW7"},{"type":"inlineCode","value":"min_samples_leaf","position":{"start":{"line":290,"column":1},"end":{"line":290,"column":1}},"key":"XpOzZzfMyB"},{"type":"text","value":". Beiden optionalen Argumenten kann entweder\nein Integer übergeben werden oder ein Float. Wird ein Integer übergeben, so ist\ndamit die tatsächliche minimale Anzahl an Datenpunkten gemeint. Ein Float wird\nals Bruch interpretiert und meint die relative Anzahl der Datenpunkte. Der Bruch\nwird mit der Gesamtzahl der Datenpunkte multipliziert und dann wird auf die\nnächste ganze Zahl aufgerundet.","position":{"start":{"line":290,"column":1},"end":{"line":290,"column":1}},"key":"tohMwFMe3t"}],"key":"kVseAP7k9Y"},{"type":"paragraph","position":{"start":{"line":300,"column":1},"end":{"line":302,"column":1}},"children":[{"type":"text","value":"Schauen wir uns beide Varianten an. Zunächst begrenzen wir die Knoten und\nfordern, dass sich in jedem Entscheidungsknoten mindestens sechs Datenpunkte\nbefinden müssen.","position":{"start":{"line":300,"column":1},"end":{"line":300,"column":1}},"key":"NqrN4BxSlO"}],"key":"RCUwpdJuPq"}],"key":"nWCd2KILYc"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"modell_knotenbegrenzung = DecisionTreeClassifier(random_state=0, min_samples_split=6)\nmodell_knotenbegrenzung.fit(X,y)\n\nplot_tree(modell_knotenbegrenzung,\n    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n    class_names=['nicht verkauft', 'verkauft']);\n\nprint(f'Score des Entscheidungsbaumes mit Prä-Pruning Mindestanzahl Datenpunkte pro Knoten: {modell_knotenbegrenzung.score(X,y)}')","key":"i1hDZr2VuQ"},{"type":"outputs","id":"ZdFJRI0YoEVw009avSl6w","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"Score des Entscheidungsbaumes mit Prä-Pruning Mindestanzahl Datenpunkte pro Knoten: 0.92\n"},"key":"rNUJLBgIDT"},{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"60faffff720b19e19d64011f6321509d","path":"/build/60faffff720b19e19d64011f6321509d.png"}}},"key":"iHeYcYSiZd"}],"key":"mUNpQbTGW4"}],"key":"YW8OBTO4cx"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":315,"column":1},"end":{"line":316,"column":1}},"children":[{"type":"text","value":"Der Score ist 0.92. Nun fordern wir, dass in jedem Blatt mindestens sechs\nDatenpunkte verbleiben müssen.","position":{"start":{"line":315,"column":1},"end":{"line":315,"column":1}},"key":"jNgUIu8Au4"}],"key":"cBsg8hjHpS"}],"key":"uWpI4XWSFk"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"modell_blattbegrenzung = DecisionTreeClassifier(random_state=0, min_samples_leaf=6)\nmodell_blattbegrenzung.fit(X,y)\n\nplot_tree(modell_blattbegrenzung,\n    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n    class_names=['nicht verkauft', 'verkauft']);\n\nprint(f'Score des Entscheidungsbaumes mit Prä-Pruning Mindestanzahl Datenpunkte pro Blatt: {modell_blattbegrenzung.score(X,y)}')","key":"aTsFEPK9fp"},{"type":"outputs","id":"GDhWj4VNpKZkd6fAdIU3x","children":[{"type":"output","children":[],"jupyter_data":{"output_type":"stream","name":"stdout","text":"Score des Entscheidungsbaumes mit Prä-Pruning Mindestanzahl Datenpunkte pro Blatt: 0.82\n"},"key":"O2vRmPOWJh"},{"type":"output","children":[],"jupyter_data":{"output_type":"display_data","metadata":{},"data":{"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"},"image/png":{"content_type":"image/png","hash":"d12f1b09a02342fe04b539d5bcee6e6a","path":"/build/d12f1b09a02342fe04b539d5bcee6e6a.png"}}},"key":"tmN46pMN80"}],"key":"GvHsIM7fE2"}],"key":"E6MyzSRd0B"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":329,"column":1},"end":{"line":334,"column":1}},"children":[{"type":"text","value":"In diesem Fall erhalten wir einen Entscheidungsbaum mit einem Score von 0.82.\nWas jetzt die bessere Wahl ist -- Begrenzung der Baumtiefe oder Festlegung einer\nMindestanzahl von Datenpunkten Knoten/Blatt -- und vor allem welchen Wert der\nHyperparameter haben soll, ist eine zentrale Herausforderung im maschinellen\nLernen. In späteren Kapiteln werden wir systematische Methoden wie Grid Search\nund Cross-Validation kennenlernen, um die besten Hyperparameter-Werte zu finden.","position":{"start":{"line":329,"column":1},"end":{"line":329,"column":1}},"key":"qCYOk0xLlJ"}],"key":"fTFyzQCVZi"},{"type":"admonition","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Mini-Übung","position":{"start":{"line":336,"column":1},"end":{"line":336,"column":1}},"key":"Nv1LXQkkXI"}],"key":"gd5NqTznNl"},{"type":"paragraph","position":{"start":{"line":338,"column":1},"end":{"line":339,"column":1}},"children":[{"type":"text","value":"Welcher Entscheidungsbaum zeigt vermutlich die stärkste Tendenz zum Overfitting?\nStellen Sie eine Vermuting an und überprüfen Sie Ihre Vermutung durch Ausprobieren.","position":{"start":{"line":338,"column":1},"end":{"line":338,"column":1}},"key":"oa1uH9akB5"}],"key":"LTmW19F6n0"},{"type":"paragraph","position":{"start":{"line":341,"column":1},"end":{"line":343,"column":1}},"children":[{"type":"text","value":"A) ","position":{"start":{"line":341,"column":1},"end":{"line":341,"column":1}},"key":"WqGQx3jjJ1"},{"type":"inlineCode","value":"DecisionTreeClassifier(max_depth=2)","position":{"start":{"line":341,"column":1},"end":{"line":341,"column":1}},"key":"JaCUFAyO1w"},{"type":"break","position":{"start":{"line":341,"column":1},"end":{"line":341,"column":1}},"key":"r8aWaMMqR2"},{"type":"text","value":"B) ","position":{"start":{"line":341,"column":1},"end":{"line":341,"column":1}},"key":"OXOn903hYb"},{"type":"inlineCode","value":"DecisionTreeClassifier(max_depth=10)","position":{"start":{"line":341,"column":1},"end":{"line":341,"column":1}},"key":"Ka0DNBlpcv"},{"type":"break","position":{"start":{"line":341,"column":1},"end":{"line":341,"column":1}},"key":"qtilW9pJtF"},{"type":"text","value":"C) ","position":{"start":{"line":341,"column":1},"end":{"line":341,"column":1}},"key":"NFA1VDAyBh"},{"type":"inlineCode","value":"DecisionTreeClassifier(min_samples_leaf=20)","position":{"start":{"line":341,"column":1},"end":{"line":341,"column":1}},"key":"W2T00i2aiC"}],"key":"IXBlTNYx5i"}],"class":"tip","key":"f5FkcKNH5k"}],"key":"hSXmAo7wbG"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Hier Ihr Code","key":"ecuXABhGm7"},{"type":"outputs","id":"Rf_R97r1jo50g5FPcTmpi","children":[],"key":"Y1KPK85LNU"}],"key":"HNkvHGQuZU"},{"type":"block","children":[{"type":"admonition","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Lösung","position":{"start":{"line":350,"column":1},"end":{"line":350,"column":1}},"key":"mU7Jq0Rcu8"}],"key":"x0KY1yDU8A"},{"type":"paragraph","position":{"start":{"line":353,"column":1},"end":{"line":353,"column":1}},"children":[{"type":"text","value":"Antwort B, denn eine große maximale Tiefe erlaubt sehr komplexe Bäume.","position":{"start":{"line":353,"column":1},"end":{"line":353,"column":1}},"key":"i9huLYqX3o"}],"key":"albTXp5S0m"},{"type":"paragraph","position":{"start":{"line":355,"column":1},"end":{"line":355,"column":1}},"children":[{"type":"text","value":"Überprüfung durch Code:","position":{"start":{"line":355,"column":1},"end":{"line":355,"column":1}},"key":"kZqaZZzhiC"}],"key":"U3vYHHqvUg"},{"type":"code","lang":"python","value":"# Die drei Modelle trainieren und Scores vergleichen\nmodell_a = DecisionTreeClassifier(max_depth=2, random_state=0)\nmodell_a.fit(X, y)\nprint(f'Score A (max_depth=2): {modell_a.score(X, y):.3f}')\n\nmodell_b = DecisionTreeClassifier(max_depth=10, random_state=0)\nmodell_b.fit(X, y)\nprint(f'Score B (max_depth=10): {modell_b.score(X, y):.3f}')\n\nmodell_c = DecisionTreeClassifier(min_samples_leaf=20, random_state=0)\nmodell_c.fit(X, y)\nprint(f'Score C (min_samples_leaf=20): {modell_c.score(X, y):.3f}')\n\n# Modell B hat vermutlich den höchsten Score (nahe 1.0) → Overfitting!","position":{"start":{"line":356,"column":1},"end":{"line":371,"column":1}},"key":"qv41zxJxDB"}],"class":"tip dropdown, dropdown","key":"eEf93Mc7U9"},{"type":"details","children":[{"type":"summary","children":[{"type":"text","value":"Video “How to Implement Decision Trees in Python / Scikit-Learn” von Misra Turp","position":{"start":{"line":374,"column":1},"end":{"line":374,"column":1}},"key":"UmxoubZZx7"}],"key":"DinGCh867H"},{"type":"iframe","src":"https://www.youtube.com/embed/wxS5P7yDHRA?si=rawkRWRmUi0ZZAub","width":"100%","title":"YouTube video player","key":"D2EOcdWkWN"}],"key":"LQhOA76JP1"},{"type":"heading","depth":2,"position":{"start":{"line":380,"column":1},"end":{"line":380,"column":1}},"children":[{"type":"text","value":"Zusammenfassung und Ausblick","position":{"start":{"line":380,"column":1},"end":{"line":380,"column":1}},"key":"xBAyZjRC3I"}],"identifier":"zusammenfassung-und-ausblick","label":"Zusammenfassung und Ausblick","html_id":"zusammenfassung-und-ausblick","implicit":true,"key":"TAYOmNul1b"},{"type":"paragraph","position":{"start":{"line":382,"column":1},"end":{"line":390,"column":1}},"children":[{"type":"text","value":"In diesem Kapitel haben wir die Tendenz der Entscheidungsbäume zum Overfitting\ndiskutiert. Um dem Problem des Overfittings zu begegnen, bietet Scikit-Learn die\nMöglichkeit des Prä-Prunings. Durch die Begrenzung der maximalen Baumtiefe oder\ndie Festlegung einer Mindestanzahl von Datenpunkten in Knoten oder Blättern kann\nOverfitting reduziert werden. Diese zusätzlichen Parameter des\nEntscheidungsbaums werden Hyperparameter genannt und müssen angepasst werden.\nEine weitere Alternative, das Overfitting von Entscheidungsbäumen zu minimieren,\nbieten die Random Forests, die wir in einem späteren Kapitel kennenlernen\nwerden.","position":{"start":{"line":382,"column":1},"end":{"line":382,"column":1}},"key":"SAAVtAWigY"}],"key":"Ocyrk5FxAs"}],"key":"s5oM0qKp1d"}],"key":"wPhPkdDdSX"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"6.2 Entscheidungsbäume visualisieren und trainieren","url":"/chapter06-sec02","group":"6. Entscheidungsbäume (Decision Trees)"},"next":{"title":"Übung","url":"/chapter06-sec04","group":"6. Entscheidungsbäume (Decision Trees)"}}},"domain":"http://localhost:3000"}