{"version":3,"kind":"Notebook","sha256":"2b0e0aa5484119987cca28432285812cedeb1f9b4245fe0ec41c5b1292404388","slug":"chapter09-sec01","location":"/chapter09/chapter09_sec01.md","dependencies":[],"frontmatter":{"title":"9.1 Grundideen der Ensemble-Methoden","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"jupytext":{"formats":"ipynb,md:myst","text_representation":{"extension":".md","format_name":"myst","format_version":"0.13","jupytext_version":"1.15.2"}},"content_includes_title":false,"authors":[{"id":"Simone Gramsch","name":"Simone Gramsch"}],"open_access":true,"license":{"content":{"id":"CC-BY-NC-SA-4.0","url":"https://creativecommons.org/licenses/by-nc-sa/4.0/","name":"Creative Commons Attribution Non Commercial Share Alike 4.0 International","CC":true}},"github":"https://github.com/gramschs/book_ml4ing","numbering":{"title":{"offset":1}},"source_url":"https://github.com/gramschs/book_ml4ing/blob/main/chapter09/chapter09_sec01.md","edit_url":"https://github.com/gramschs/book_ml4ing/edit/main/chapter09/chapter09_sec01.md","thumbnail":"/book_ml4ing/build/concept_voting-aafee3ce5cf0e957148ec9b4ebfdf754.svg","exports":[{"format":"md","filename":"chapter09_sec01.md","url":"/book_ml4ing/build/chapter09_sec01-740bede0a38be1d4175324c30cc0cb6b.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":17,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"Eins, zwei, viele ... im Bereich des maschinellen Lernens sind Ensemble-Methoden\nleistungsstarke Techniken zur Verbesserung der Modellgenauigkeit und Robustheit.\nDiese Methoden kombinieren mehrere Modelle, um die Gesamtleistung zu steigern,\nindem sie die individuellen Stärken der Modelle nutzen und deren Schwächen\nausgleichen. In diesem Kapitel werden wir die grundlegenden Konzepte und\nUnterschiede zwischen diesen Methoden erläutern, um ein besseres\nVerständnis ihrer Funktionsweise und Anwendungen zu vermitteln.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"JHpx2OTEGf"}],"key":"Qlx0xH7woP"},{"type":"heading","depth":2,"position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"Lernziele","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"smKWMejyuC"}],"identifier":"lernziele","label":"Lernziele","html_id":"lernziele","implicit":true,"key":"ytGp0t5fPO"},{"type":"admonition","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Lernziele","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"N5TKEQHk9R"}],"key":"j9U0j730ww"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":29,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Sie können in eigenen Worten erklären, was ","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"EzGiG6GwpV"},{"type":"strong","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"Ensemble-Methoden","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"tEAkCCpY57"}],"key":"FBo3Hhxnm8"},{"type":"text","value":" sind.","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"HH9Lx9vGiJ"}],"key":"PWE74KoGw1"}],"key":"sLqpM5WTuj"},{"type":"listItem","spread":true,"position":{"start":{"line":30,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Sie kennen die Grundideen der Ensemble-Methoden","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"HPceDkhu0W"}],"key":"fWSZ2HP25q"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":31,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"text","value":"Voting","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"cla0agSR0c"}],"key":"BQh0mBzOAg"},{"type":"text","value":",","position":{"start":{"line":31,"column":1},"end":{"line":31,"column":1}},"key":"XuwbeD7pvA"}],"key":"z8nXZ839yP"}],"key":"rzvvWBq89e"},{"type":"listItem","spread":true,"position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"children":[{"type":"text","value":"Averaging","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"V6Jqkg9sOg"}],"key":"CWkQaIfEMA"},{"type":"text","value":",","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"rwK2gukdnK"}],"key":"nkapM4Ovoc"}],"key":"W01y8DvZcz"},{"type":"listItem","spread":true,"position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"Stacking","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"Zx0cKzejne"}],"key":"fsyVs4UP2q"},{"type":"text","value":",","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"RbAw14Pq5v"}],"key":"N5wdr3a0Lp"}],"key":"gonitSOKc4"},{"type":"listItem","spread":true,"position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"text","value":"Bagging","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"MXIx660zMk"}],"key":"FFmiIRTlL4"},{"type":"text","value":" und","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"jTBib5Ev04"}],"key":"ecWcunYeki"}],"key":"ERgwCjkk1f"},{"type":"listItem","spread":true,"position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"Boosting","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"Sr7vvaMWQ0"}],"key":"CLEINOKhan"},{"type":"text","value":".","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"WjipxJGY3X"}],"key":"ve4UxrMvQT"}],"key":"IJgO6T0VQu"}],"key":"hktCqGLZEa"}],"key":"ASUsKH6WZt"}],"key":"mR2xZO6vkG"}],"class":"attention","key":"WG32Uj0cWc"},{"type":"heading","depth":2,"position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"text","value":"Ensemble-Methoden","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"A527ZW3EPL"}],"identifier":"ensemble-methoden","label":"Ensemble-Methoden","html_id":"ensemble-methoden","implicit":true,"key":"JXH2lEtbRv"},{"type":"paragraph","position":{"start":{"line":40,"column":1},"end":{"line":46,"column":1}},"children":[{"type":"text","value":"Der Begriff »Ensemble« wird im Allgemeinen eher mit Musik und Kunst in\nVerbindung gebracht als mit Informatik. In der Musik bezeichnet ein Ensemble\neine kleine Gruppe von Musikern, die entweder das gleiche Instrument spielen\noder verschiedene Instrumente kombinieren. Im Theater bezeichnet man eine Gruppe\nvon Schauspielern ebenfalls als Ensemble, und in der Architektur beschreibt der\nBegriff eine Gruppe von Gebäuden, die in einem besonderen Zusammenhang\nzueinander stehen.","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"key":"JlneOIjuj7"}],"key":"z57RSPe37W"},{"type":"paragraph","position":{"start":{"line":48,"column":1},"end":{"line":57,"column":1}},"children":[{"type":"text","value":"Auch im Bereich des maschinellen Lernens hat sich der Begriff Ensemble\netabliert. Mit ","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"U36DUpdLRP"},{"type":"strong","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"children":[{"type":"text","value":"Ensemble-Methoden","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"at3rJBWHwi"}],"key":"AMySO927xF"},{"type":"text","value":" (Ensemble Learning) wird eine Gruppe von\nmaschinellen Modellen bezeichnet, die zusammen eine Prognose treffen sollen.\nÄhnlich wie bei Musik-Ensembles können bei den Ensemble-Methoden entweder\nidentische Modelle oder verschiedene Modelle kombiniert werden. Diese Modelle\nkönnen entweder gleichzeitig eine Prognose treffen, die dann kombiniert wird,\noder nacheinander verwendet werden, wobei ein nachfolgendes Modell die Fehler\ndes vorherigen korrigiert. Je nach Vorgehensweise unterscheidet man im\nmaschinellen Lernen zwischen ","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"aLbCxqvC97"},{"type":"strong","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"children":[{"type":"text","value":"Voting","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"Sanzq86CrQ"}],"key":"yM0rzmyezr"},{"type":"text","value":", ","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"gghvpr0AgD"},{"type":"strong","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"children":[{"type":"text","value":"Averaging","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"jcvlLTdvA2"}],"key":"WpePUUVRzw"},{"type":"text","value":", ","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"To2sK1Y0b0"},{"type":"strong","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"children":[{"type":"text","value":"Stacking","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"n8vLtrhF3x"}],"key":"w4TsKRxwZV"},{"type":"text","value":",\n","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"r5at5aBf8M"},{"type":"strong","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"children":[{"type":"text","value":"Bagging","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"kEMrII9b2t"}],"key":"IQDNcc0ZXI"},{"type":"text","value":" und ","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"r5sDOFMtYj"},{"type":"strong","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"children":[{"type":"text","value":"Boosting","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"Q7FQpvT47U"}],"key":"u74YXdxhnB"},{"type":"text","value":".","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"bOGdELWxD1"}],"key":"iYv4q14WCJ"},{"type":"paragraph","position":{"start":{"line":59,"column":1},"end":{"line":64,"column":1}},"children":[{"type":"text","value":"In dieser Vorlesung konzentrieren wir uns auf Bagging und Boosting mit ihren\nbekanntesten Vertretern, den Random Forests und XGBoost. Die Konzepte Voting und\nAveraging sowie Stacking werden hier nur kurz ohne weitere Details vorgestellt.\nEine allgemeine Einführung in Ensemble-Methoden mit Scikit-Learn findet sich in\nder ","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"key":"Tup1V1ONWh"},{"type":"link","url":"https://scikit-learn.org/stable/modules/ensemble.html","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"children":[{"type":"text","value":"Dokumentation Scikit-Learn →\nEnsemble","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"key":"kSq9sYxhyr"}],"urlSource":"https://scikit-learn.org/stable/modules/ensemble.html","key":"YTVt2xGhqm"},{"type":"text","value":".","position":{"start":{"line":59,"column":1},"end":{"line":59,"column":1}},"key":"OBt8RZWMBN"}],"key":"qvZtBOljvg"},{"type":"heading","depth":2,"position":{"start":{"line":66,"column":1},"end":{"line":66,"column":1}},"children":[{"type":"text","value":"Voting, Averaging und Stacking","position":{"start":{"line":66,"column":1},"end":{"line":66,"column":1}},"key":"BSmub4CyS8"}],"identifier":"voting-averaging-und-stacking","label":"Voting, Averaging und Stacking","html_id":"voting-averaging-und-stacking","implicit":true,"key":"EmJOa03E7l"},{"type":"container","kind":"figure","children":[{"type":"image","url":"/book_ml4ing/build/concept_voting-aafee3ce5cf0e957148ec9b4ebfdf754.svg","alt":"Die Prognosen von mehreren unterschiedlichen ML-Modellen werden zu einer\nfinalen Prognose kombiniert. Die Kombination kann beispielsweise durch\nMehrheitsentscheidung (Voting), aber auch Mittelwertbildung (Averaging)\nerfolgen.","width":"100%","data":{"altTextIsAutoGenerated":true},"key":"v9gYivawCj","urlSource":"pics/concept_voting.svg"},{"type":"caption","children":[{"type":"paragraph","position":{"start":{"line":72,"column":1},"end":{"line":75,"column":1}},"children":[{"type":"text","value":"Die Prognosen von mehreren ","position":{"start":{"line":72,"column":1},"end":{"line":72,"column":1}},"key":"h22Yglg2Rj"},{"type":"emphasis","position":{"start":{"line":72,"column":1},"end":{"line":72,"column":1}},"children":[{"type":"text","value":"unterschiedlichen","position":{"start":{"line":72,"column":1},"end":{"line":72,"column":1}},"key":"keLpljeejr"}],"key":"rOxZOAI07E"},{"type":"text","value":" ML-Modellen werden zu einer\nfinalen Prognose kombiniert. Die Kombination kann beispielsweise durch\nMehrheitsentscheidung (Voting), aber auch Mittelwertbildung (Averaging)\nerfolgen.","position":{"start":{"line":72,"column":1},"end":{"line":72,"column":1}},"key":"zOwm0UxPnY"}],"key":"zmuNS7zWyB"}],"key":"X2ivWfkD9C"}],"enumerator":"1","key":"Gee9rviwMl"},{"type":"paragraph","position":{"start":{"line":78,"column":1},"end":{"line":87,"column":1}},"children":[{"type":"text","value":"In einem ersten Schritt werden mehrere ML-Modelle unabhängig voneinander auf den\nTrainingsdaten trainiert. Jedes dieser Modelle liefert eine Prognose, die dann\nauf verschiedene Arten miteinander kombiniert werden können. Bei\nKlassifikationsaufgaben ist ","position":{"start":{"line":78,"column":1},"end":{"line":78,"column":1}},"key":"fQJRGO0uKL"},{"type":"strong","position":{"start":{"line":78,"column":1},"end":{"line":78,"column":1}},"children":[{"type":"text","value":"Voting","position":{"start":{"line":78,"column":1},"end":{"line":78,"column":1}},"key":"GGpk4ryLIN"}],"key":"Dfe5Y5bm9u"},{"type":"text","value":", also die Wahl durch\nMehrheitsentscheidung, eine beliebte Methode, um die Einzelprognosen zu\nkombinieren. Wurden beispielsweise drei ML-Modelle gewählt, die jeweils ja oder\nnein prognostizieren, dann wird für die finale Prognose das Ergebnis genommen,\ndas die Mehrheit der einzelnen Modelle vorausgesagt hat. Scikit-Learn bietet\ndafür einen Voting Classifier an, siehe ","position":{"start":{"line":78,"column":1},"end":{"line":78,"column":1}},"key":"P5g317JkyF"},{"type":"link","url":"https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier","position":{"start":{"line":78,"column":1},"end":{"line":78,"column":1}},"children":[{"type":"text","value":"Dokumentation Scikit-Learn → Voting\nClassifier","position":{"start":{"line":78,"column":1},"end":{"line":78,"column":1}},"key":"PAWI0hTina"}],"urlSource":"https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier","key":"IP54omQm9R"},{"type":"text","value":".","position":{"start":{"line":78,"column":1},"end":{"line":78,"column":1}},"key":"T2VfTd6fbD"}],"key":"kM63MHSocG"},{"type":"paragraph","position":{"start":{"line":89,"column":1},"end":{"line":95,"column":1}},"children":[{"type":"text","value":"Bei Regressionsaufgaben werden die einzelnen Prognosen häufig gemittelt. Beim\n","position":{"start":{"line":89,"column":1},"end":{"line":89,"column":1}},"key":"XLaLtJItNq"},{"type":"strong","position":{"start":{"line":89,"column":1},"end":{"line":89,"column":1}},"children":[{"type":"text","value":"Averaging","position":{"start":{"line":89,"column":1},"end":{"line":89,"column":1}},"key":"sRw8CPswG3"}],"key":"tpdSo5owhD"},{"type":"text","value":" kann entweder der übliche arithmetische Mittelwert verwendet\nwerden oder ein gewichteter Mittelwert, was als Weighted Averaging bezeichnet\nwird. Dennoch wird die Mittelwertbildung bei Regressionsaufgaben von\nScikit-Learn ebenfalls als Voting bezeichnet, siehe ","position":{"start":{"line":89,"column":1},"end":{"line":89,"column":1}},"key":"GwzCTwOv2X"},{"type":"link","url":"https://scikit-learn.org/stable/modules/ensemble.html#voting-regressor","position":{"start":{"line":89,"column":1},"end":{"line":89,"column":1}},"children":[{"type":"text","value":"Dokumentation Scikit-Learn\n→ Voting\nRegressor","position":{"start":{"line":89,"column":1},"end":{"line":89,"column":1}},"key":"uw96Vudq7r"}],"urlSource":"https://scikit-learn.org/stable/modules/ensemble.html#voting-regressor","key":"uVmcjsG3ky"},{"type":"text","value":".","position":{"start":{"line":89,"column":1},"end":{"line":89,"column":1}},"key":"C6b87UBQdT"}],"key":"WlDz6MxtgK"},{"type":"paragraph","position":{"start":{"line":97,"column":1},"end":{"line":103,"column":1}},"children":[{"type":"text","value":"Eine alternative Kombinationsmethode ist die Verwendung eines weiteren\nML-Modells. In diesem Fall werden die Modelle, die die einzelnen Prognosen\nliefern, als Basismodelle bezeichnet. Die Basismodelle liefern Features für ein\nweiteres ML-Modell, das als Meta-Modell bezeichnet wird. Diese Ensemble-Methode\nwird ","position":{"start":{"line":97,"column":1},"end":{"line":97,"column":1}},"key":"HdnVoborxx"},{"type":"strong","position":{"start":{"line":97,"column":1},"end":{"line":97,"column":1}},"children":[{"type":"text","value":"Stacking","position":{"start":{"line":97,"column":1},"end":{"line":97,"column":1}},"key":"Z5fPCIRhkw"}],"key":"JIWFyncEpD"},{"type":"text","value":" genannt. Weitere Informationen liefert die ","position":{"start":{"line":97,"column":1},"end":{"line":97,"column":1}},"key":"NQ27a6XJJT"},{"type":"link","url":"https://scikit-learn.org/stable/modules/ensemble.html#stacked-generalization","position":{"start":{"line":97,"column":1},"end":{"line":97,"column":1}},"children":[{"type":"text","value":"Scikit-Learn\nDokumentation → Stacked\nGeneralization","position":{"start":{"line":97,"column":1},"end":{"line":97,"column":1}},"key":"ioSpg4xjCq"}],"urlSource":"https://scikit-learn.org/stable/modules/ensemble.html#stacked-generalization","key":"WLc2Jh0jtX"},{"type":"text","value":".","position":{"start":{"line":97,"column":1},"end":{"line":97,"column":1}},"key":"YgF15Z0EvM"}],"key":"rDuKut4Fjg"},{"type":"paragraph","position":{"start":{"line":105,"column":1},"end":{"line":111,"column":1}},"children":[{"type":"text","value":"Stacking bietet viele Vorteile. Der wichtigste Vorteil ist, dass die\nPrognosefähigkeit des Gesamtmodells in der Regel deutlich besser ist als die der\neinzelnen Basismodelle. Die Stärken der Basismodelle werden kombiniert und die\nSchwächen ausgeglichen. Allerdings erfordert Stacking sehr viel Feinarbeit. Auch\nsteigt die Trainingszeit für das Gesamtmodell, selbst wenn die Basismodelle bei\ngenügend Rechenleistung parallel trainiert werden können. Aus diesem Grund\nwerden wir in dieser Vorlesung kein Stacking verwenden.","position":{"start":{"line":105,"column":1},"end":{"line":105,"column":1}},"key":"KB25mTVKaq"}],"key":"dXJ44yb1fH"},{"type":"heading","depth":2,"position":{"start":{"line":113,"column":1},"end":{"line":113,"column":1}},"children":[{"type":"text","value":"Bagging","position":{"start":{"line":113,"column":1},"end":{"line":113,"column":1}},"key":"YPiA9MDqXe"}],"identifier":"bagging","label":"Bagging","html_id":"bagging","implicit":true,"key":"WQN0WCngSE"},{"type":"container","kind":"figure","children":[{"type":"image","url":"/book_ml4ing/build/concept_bagging-8c7a36edab4307c19a7ab398b2233cea.svg","alt":"Beim Bagging wird das gleiche ML-Modell auf unterschiedlichen Stichproben der\nTrainingsdaten trainiert (Bootstrapping). Die Einzelprognosen der Modelle werden\ndann zu einer finalen Prognose kombiniert (Aggregating).","width":"100%","data":{"altTextIsAutoGenerated":true},"key":"sSAf8jMzEh","urlSource":"pics/concept_bagging.svg"},{"type":"caption","children":[{"type":"paragraph","position":{"start":{"line":119,"column":1},"end":{"line":121,"column":1}},"children":[{"type":"text","value":"Beim Bagging wird das gleiche ML-Modell auf ","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"ixyYBwG396"},{"type":"emphasis","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"children":[{"type":"text","value":"unterschiedlichen","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"Ax7hIweOGv"}],"key":"yKuQTKedf3"},{"type":"text","value":" Stichproben der\nTrainingsdaten trainiert (Bootstrapping). Die Einzelprognosen der Modelle werden\ndann zu einer finalen Prognose kombiniert (Aggregating).","position":{"start":{"line":119,"column":1},"end":{"line":119,"column":1}},"key":"lwOeN5UvMV"}],"key":"rV59UUA1QU"}],"key":"xwYCvS2nuX"}],"enumerator":"2","key":"XxCGeuVIT5"},{"type":"paragraph","position":{"start":{"line":124,"column":1},"end":{"line":127,"column":1}},"children":[{"type":"text","value":"Bagging ist eine Ensemble-Methode, bei der stets dasselbe Modell für die\nEinzelprognosen verwendet wird. Die Unterschiede in den Einzelprognosen\nentstehen dadurch, dass für das Training der einzelnen Modelle unterschiedliche\nDaten verwendet werden.","position":{"start":{"line":124,"column":1},"end":{"line":124,"column":1}},"key":"PsA5fKECp7"}],"key":"LfTFB8CLyO"},{"type":"paragraph","position":{"start":{"line":129,"column":1},"end":{"line":136,"column":1}},"children":[{"type":"text","value":"Im ersten Schritt werden zufällige Datenpunkte aus den Trainingsdaten ausgewählt\nund in einen neuen Datensatz, „Stichprobe 1“, aufgenommen. Nachdem ein\nDatenpunkt ausgewählt wurde, kehrt er in die ursprüngliche Menge der\nTrainingsdaten zurück und kann erneut ausgewählt werden. Dieser Prozess wird in\nder Mathematik als ","position":{"start":{"line":129,"column":1},"end":{"line":129,"column":1}},"key":"ZIVSPpoKKB"},{"type":"strong","position":{"start":{"line":129,"column":1},"end":{"line":129,"column":1}},"children":[{"type":"text","value":"Ziehen mit Zurücklegen","position":{"start":{"line":129,"column":1},"end":{"line":129,"column":1}},"key":"r3d0l4xUNC"}],"key":"wtG1pwZueG"},{"type":"text","value":" bezeichnet, auf Englisch\n","position":{"start":{"line":129,"column":1},"end":{"line":129,"column":1}},"key":"nxa2MP7BjV"},{"type":"strong","position":{"start":{"line":129,"column":1},"end":{"line":129,"column":1}},"children":[{"type":"text","value":"Bootstrapping","position":{"start":{"line":129,"column":1},"end":{"line":129,"column":1}},"key":"hYYaUt3Zz8"}],"key":"q58Bj5q7k4"},{"type":"text","value":". Durch Bootstrapping werden dann noch weitere Stichproben\ngebildet, wobei jede Stichprobe üblicherweise die gleiche Anzahl an Datenpunkten\nwie der ursprüngliche Datensatz enthält.","position":{"start":{"line":129,"column":1},"end":{"line":129,"column":1}},"key":"v3ALREk5Ca"}],"key":"plRJ4YWR7v"},{"type":"paragraph","position":{"start":{"line":138,"column":1},"end":{"line":144,"column":1}},"children":[{"type":"text","value":"Im zweiten Schritt wird ein ML-Modell gewählt und für jede Bootstrap-Stichprobe\ntrainiert. Da die Stichproben unterschiedliche Trainingsdaten enthalten,\nentstehen unterschiedlich trainierte Modelle, die für neue Daten verschiedene\nEinzelprognosen liefern. Diese Einzelprognosen werden kombiniert bzw. nach\nfestgelegten Regeln zu einer finalen Prognose zusammengefasst. In der Statistik\nwird die Zusammenfassung von Daten als Aggregation bezeichnet. Auf Englisch\nheißt der Vorgang des Zusammenfassens ","position":{"start":{"line":138,"column":1},"end":{"line":138,"column":1}},"key":"AUYTRD0Ake"},{"type":"strong","position":{"start":{"line":138,"column":1},"end":{"line":138,"column":1}},"children":[{"type":"text","value":"Aggregating","position":{"start":{"line":138,"column":1},"end":{"line":138,"column":1}},"key":"GahLVOUEcG"}],"key":"UclYRd41Bv"},{"type":"text","value":".","position":{"start":{"line":138,"column":1},"end":{"line":138,"column":1}},"key":"NIg4cIycL5"}],"key":"wV1Yvm9tnL"},{"type":"paragraph","position":{"start":{"line":146,"column":1},"end":{"line":156,"column":1}},"children":[{"type":"text","value":"Die beiden wesentlichen Schritte der Bagging-Methode sind also ","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"HFSG9R33nm"},{"type":"strong","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"children":[{"type":"text","value":"B","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"qp7wwlPK2g"}],"key":"R5aAEy8J7V"},{"type":"text","value":"ootstrapping\nund ","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"HxlCkYUm6X"},{"type":"strong","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"children":[{"type":"text","value":"Agg","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"G69L4nlln7"}],"key":"V47lHG94x0"},{"type":"text","value":"regat","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"WBd3K9GKTO"},{"type":"strong","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"children":[{"type":"text","value":"ing","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"JYLElX9Lgp"}],"key":"gmLbHfmB9G"},{"type":"text","value":", woraus sich die Abkürzung »Bagging« ableitet.\nScikit-Learn bietet sowohl für Klassifikations- als auch für Regressionsaufgaben\neine allgemeine Implementierung der Bagging-Methode an (siehe ","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"wGqwXEYrFC"},{"type":"link","url":"https://scikit-learn.org/stable/modules/ensemble.html#bagging-meta-estimator","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"children":[{"type":"text","value":"Dokumentation\nScikit-Learn →\nBagging","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"RAH59IxozC"}],"urlSource":"https://scikit-learn.org/stable/modules/ensemble.html#bagging-meta-estimator","key":"shDw0yqwPn"},{"type":"text","value":").\nDie bekannteste Bagging-Methode ist ","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"aMrX0E0Oo4"},{"type":"strong","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"children":[{"type":"text","value":"Random Forests","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"FyQktCDaGM"}],"key":"GCzbqbuzML"},{"type":"text","value":", bei dem\nEntscheidungsbäume (Decision Trees) auf unterschiedlichen Stichproben trainiert\nund aggregiert werden. Random Forests werden wir im nächsten Kapitel\ndetaillierter betrachten. Vorab beschäftigen wir uns noch mit dem Konzept der\nBoosting-Methoden.","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"Vrvg17kc53"}],"key":"cNgKbKTy3k"},{"type":"heading","depth":2,"position":{"start":{"line":158,"column":1},"end":{"line":158,"column":1}},"children":[{"type":"text","value":"Boosting","position":{"start":{"line":158,"column":1},"end":{"line":158,"column":1}},"key":"y3vnIdcxFu"}],"identifier":"boosting","label":"Boosting","html_id":"boosting","implicit":true,"key":"fr3Z5ct8Om"},{"type":"container","kind":"figure","children":[{"type":"image","url":"/book_ml4ing/build/concept_boosting-59fef25a9e2e127bf3da4114bd879f12.svg","alt":"Der Fehler in der Prognose wird benutzt, um das nächste Modell zu trainieren.\nBeim hier gezeigten Adaboost-Verfahren werden die Daten neu gewichtet, beim\n(Stochastic) Gradient Boosting werden Modelle zur Fehlerkorrektur trainiert.","width":"100%","data":{"altTextIsAutoGenerated":true},"key":"RwU5gpXzRG","urlSource":"pics/concept_boosting.svg"},{"type":"caption","children":[{"type":"paragraph","position":{"start":{"line":164,"column":1},"end":{"line":166,"column":1}},"children":[{"type":"text","value":"Der Fehler in der Prognose wird benutzt, um das nächste Modell zu trainieren.\nBeim hier gezeigten Adaboost-Verfahren werden die Daten neu gewichtet, beim\n(Stochastic) Gradient Boosting werden Modelle zur Fehlerkorrektur trainiert.","position":{"start":{"line":164,"column":1},"end":{"line":164,"column":1}},"key":"SGAL3uqBun"}],"key":"GLWCyiGe0P"}],"key":"C7zVUdHnWV"}],"enumerator":"3","key":"JngQj21Pd9"},{"type":"paragraph","position":{"start":{"line":169,"column":1},"end":{"line":176,"column":1}},"children":[{"type":"text","value":"Das englische Verb „to boost sth.“ hat viele Bedeutungen. Insbesondere wird es\nim Deutschen mit „etwas verstärken“ übersetzt. Im Kontext des maschinellen\nLernens bezeichnet ","position":{"start":{"line":169,"column":1},"end":{"line":169,"column":1}},"key":"ZphhjHyCvT"},{"type":"strong","position":{"start":{"line":169,"column":1},"end":{"line":169,"column":1}},"children":[{"type":"text","value":"Boosting","position":{"start":{"line":169,"column":1},"end":{"line":169,"column":1}},"key":"QjZx73LpdB"}],"key":"VIyMLgpQmU"},{"type":"text","value":" eine Ensemble-Methode, bei der mehrere ML-Modelle\nhintereinander geschaltet werden, um die Genauigkeit der Prognose zu verstärken.\nDie Idee des Boosting besteht darin, dass jedes Modell die Fehler des\nVorgängermodells reduziert. Es gibt mehrere Varianten zur Fehlerreduktion, aus\ndenen sich unterschiedliche Boosting-Methoden ableiten. Die wichtigsten\nVarianten sind:","position":{"start":{"line":169,"column":1},"end":{"line":169,"column":1}},"key":"jwLqaRpgn5"}],"key":"ZdvoaRR4FX"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":178,"column":1},"end":{"line":181,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":178,"column":1},"end":{"line":178,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Adaboost,","position":{"start":{"line":178,"column":1},"end":{"line":178,"column":1}},"key":"UHEImkWQkV"}],"key":"CkExD5L3Wv"}],"key":"lo0V8VQQ5N"},{"type":"listItem","spread":true,"position":{"start":{"line":179,"column":1},"end":{"line":179,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Gradient Boosting und","position":{"start":{"line":179,"column":1},"end":{"line":179,"column":1}},"key":"MxAm3PpwKs"}],"key":"m1WqGMttF3"}],"key":"Qvff4BD9ZL"},{"type":"listItem","spread":true,"position":{"start":{"line":180,"column":1},"end":{"line":181,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Stochastic Gradient Boosting.","position":{"start":{"line":180,"column":1},"end":{"line":180,"column":1}},"key":"BcDRHC3oWD"}],"key":"bREUNMMJ7c"}],"key":"URuR4w9BsP"}],"key":"b7TlAf4bs7"},{"type":"paragraph","position":{"start":{"line":182,"column":1},"end":{"line":192,"column":1}},"children":[{"type":"text","value":"Beim ","position":{"start":{"line":182,"column":1},"end":{"line":182,"column":1}},"key":"MIs3dDApxX"},{"type":"strong","position":{"start":{"line":182,"column":1},"end":{"line":182,"column":1}},"children":[{"type":"text","value":"Adaboost","position":{"start":{"line":182,"column":1},"end":{"line":182,"column":1}},"key":"paYCBivSrU"}],"key":"l5NycqGB2m"},{"type":"text","value":"-Verfahren wird im ersten Schritt ein Modell (z.B. ein\nEntscheidungsbaum) auf den Trainingsdaten trainiert. Anschließend werden die\nPrognosen dieses Modells mit den tatsächlichen Werten verglichen. Im zweiten\nSchritt wird ein neuer Datensatz erstellt, wobei die falsch prognostizierten\nDatenpunkte ein größeres Gewicht erhalten. Nun wird erneut ein Modell trainiert;\nund dessen Prognosen werden wieder mit den echten Werten verglichen. Dieser\nVorgang wird mehrfach wiederholt. Das Training der Modelle erfolgt sequentiell,\nda jedes Vorgängermodell die neue Gewichtung der Trainingsdaten liefert. Am Ende\nwerden alle Einzelprognosen gewichtet zu einer finalen Prognose kombiniert.\nWeitere Details finden sich in der ","position":{"start":{"line":182,"column":1},"end":{"line":182,"column":1}},"key":"N5cp8SUms9"},{"type":"link","url":"https://scikit-learn.org/stable/modules/ensemble.html#adaboost","position":{"start":{"line":182,"column":1},"end":{"line":182,"column":1}},"children":[{"type":"text","value":"Dokumentation Scikit-Learn →\nAdaboost","position":{"start":{"line":182,"column":1},"end":{"line":182,"column":1}},"key":"wqo5xtsyDQ"}],"urlSource":"https://scikit-learn.org/stable/modules/ensemble.html#adaboost","key":"zz8FapX5C6"},{"type":"text","value":".","position":{"start":{"line":182,"column":1},"end":{"line":182,"column":1}},"key":"gYsKjS8f5F"}],"key":"iuUhsyVZFZ"},{"type":"paragraph","position":{"start":{"line":194,"column":1},"end":{"line":209,"column":1}},"children":[{"type":"text","value":"Beim ","position":{"start":{"line":194,"column":1},"end":{"line":194,"column":1}},"key":"v2oMe9MN2e"},{"type":"strong","position":{"start":{"line":194,"column":1},"end":{"line":194,"column":1}},"children":[{"type":"text","value":"Gradient Boosting","position":{"start":{"line":194,"column":1},"end":{"line":194,"column":1}},"key":"uUjnG1jrGs"}],"key":"beAFC1pZE4"},{"type":"text","value":" wird ebenfalls ein sequentieller Ansatz verfolgt,\naber der Fokus liegt auf der Minimierung der Fehler. Im ersten Schritt wird ein\nML-Modell (häufig ein Entscheidungsbaum) trainiert. Danach wird für jeden\nDatenpunkt der Fehler des Modells, das sogenannte ","position":{"start":{"line":194,"column":1},"end":{"line":194,"column":1}},"key":"s9dCj0rtd0"},{"type":"strong","position":{"start":{"line":194,"column":1},"end":{"line":194,"column":1}},"children":[{"type":"text","value":"Residuum","position":{"start":{"line":194,"column":1},"end":{"line":194,"column":1}},"key":"p6FO1TyoDM"}],"key":"cWeMU9xQTu"},{"type":"text","value":", berechnet, indem\ndie Differenzen zwischen dem tatsächlichen Wert und den Prognosen bestimmt wird.\nIm nächsten Schritt wird ein neues Modell trainiert, das darauf abzielt, diese\nResiduen vorherzusagen. Dieses neue Modell wird dann zu dem vorherigen Modell\nhinzugefügt, um die Gesamtprognose zu verbessern. Dieser Prozess wird\nwiederholt, wobei in jeder Iteration ein neues Modell trainiert wird, das die\nFehler der bisherigen Modelle reduziert (mit Hilfe einer Verlustfunktion und\neines Gradientenverfahrens). Am Ende ergibt sich eine starke Vorhersage, indem\nalle Modelle kombiniert werden. Da häufig Entscheidungsbäume als Modell gewählt\nwerden, bietet Scikit-Learn eine Implementierung der sogenannten\n","position":{"start":{"line":194,"column":1},"end":{"line":194,"column":1}},"key":"KjD7PLzhhF"},{"type":"strong","position":{"start":{"line":194,"column":1},"end":{"line":194,"column":1}},"children":[{"type":"text","value":"Gradient-Boosted Decision Trees","position":{"start":{"line":194,"column":1},"end":{"line":194,"column":1}},"key":"A5A7ZSuGsD"}],"key":"IDYcCkdMok"},{"type":"text","value":" an, siehe ","position":{"start":{"line":194,"column":1},"end":{"line":194,"column":1}},"key":"NakWmtGdie"},{"type":"link","url":"https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosted-trees","position":{"start":{"line":194,"column":1},"end":{"line":194,"column":1}},"children":[{"type":"text","value":"Dokumentation Scikit-Learn →\nGradient-boosted\ntrees","position":{"start":{"line":194,"column":1},"end":{"line":194,"column":1}},"key":"KE0zQPzHOX"}],"urlSource":"https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosted-trees","key":"p9nbda4FyL"},{"type":"text","value":".","position":{"start":{"line":194,"column":1},"end":{"line":194,"column":1}},"key":"cJaS7WDtE4"}],"key":"rhU6nKvkVs"},{"type":"paragraph","position":{"start":{"line":211,"column":1},"end":{"line":221,"column":1}},"children":[{"type":"strong","position":{"start":{"line":211,"column":1},"end":{"line":211,"column":1}},"children":[{"type":"text","value":"Stochastic Gradient Boosting","position":{"start":{"line":211,"column":1},"end":{"line":211,"column":1}},"key":"MmNcBJ2TDF"}],"key":"GNqY89q8p1"},{"type":"text","value":" ist eine Erweiterung des Gradient Boosting, bei\nder zusätzlich Stochastik eingeführt wird. Hierbei wird in jedem Schritt nur\neine zufällige Stichprobe der Trainingsdaten verwendet, um ein Modell zu\ntrainieren. Der Trainingsprozess ähnelt dem von Gradient Boosting, wobei in\njeder Runde ein neues Modell trainiert wird, das die Fehler der vorherigen\nModelle korrigiert. Durch die zufällige Auswahl der Trainingsdaten in jeder\nIteration wird eine höhere Robustheit gegenüber Overfitting (Überanpassung)\nerreicht. Stochastic Gradient Boosting wird von Scikit-Learn nicht direkt\nunterstützt. Eine sehr bekannte Implementierung davon ist XGBoost (siehe\n","position":{"start":{"line":211,"column":1},"end":{"line":211,"column":1}},"key":"uOBDxioRUJ"},{"type":"link","url":"https://xgboost.readthedocs.io/en/stable/","position":{"start":{"line":211,"column":1},"end":{"line":211,"column":1}},"children":[{"type":"text","value":"https://​xgboost​.readthedocs​.io/","position":{"start":{"line":211,"column":1},"end":{"line":211,"column":1}},"key":"B7O3RMvh6J"}],"urlSource":"https://xgboost.readthedocs.io/en/stable/","key":"NOSBUD7H4M"},{"type":"text","value":"),\ndie wir in einem der nächsten Kapitel noch näher betrachten werden.","position":{"start":{"line":211,"column":1},"end":{"line":211,"column":1}},"key":"kf75WTlsTi"}],"key":"KkHs4HJ2Pt"},{"type":"heading","depth":2,"position":{"start":{"line":223,"column":1},"end":{"line":223,"column":1}},"children":[{"type":"text","value":"Zusammenfassung und Ausblick","position":{"start":{"line":223,"column":1},"end":{"line":223,"column":1}},"key":"Ep0QP9k7ax"}],"identifier":"zusammenfassung-und-ausblick","label":"Zusammenfassung und Ausblick","html_id":"zusammenfassung-und-ausblick","implicit":true,"key":"WWQmwLGQot"},{"type":"paragraph","position":{"start":{"line":225,"column":1},"end":{"line":233,"column":1}},"children":[{"type":"text","value":"In diesem Kapitel haben Sie die Konzepte Voting, Averaging, Stacking, Bagging\nund Boosting kennengelernt. Alle Methoden sind Ensemble-Methoden, bei denen\nmehrere ML-Modelle parallel oder sequentiell kombiniert werden. Obwohl diese\nEnsemble-Methoden allgemein für verschiedene ML-Modelle eingesetzt werden\nkönnen, haben sich vor allem Random Forests (Bagging für Entscheidungsbäume) und\nStochastic Gradient Boosting als besonders effektiv erwiesen. Letztere sind\nnicht in Scikit-Learn implementiert, sondern werden durch eine eigene Bibliothek\nnamens XGBoost bereitgestellt. In den nächsten beiden Kapiteln werden wir beide\nauch mit praktischen Beispielen vertiefen.","position":{"start":{"line":225,"column":1},"end":{"line":225,"column":1}},"key":"G15GLmUKnB"}],"key":"kTN9CndK96"}],"key":"Er1n6MNcIh"}],"key":"ybg8s0ltMM"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Übungen","url":"/chapter08-sec04","group":"8. ML-Workflow Datenvorverarbeitung"},"next":{"title":"9.2 Random Forests","url":"/chapter09-sec02","group":"9. Ensemble-Methoden (Random Forests und XGBoost)"}}},"domain":"http://localhost:3001"}