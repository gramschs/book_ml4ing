{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3473e41c",
   "metadata": {},
   "source": [
    "# Training eines Perzeptrons mit Scikit-Learn\n",
    "\n",
    "Nachdem wir im letzten Abschnitt ein Perzeptron händisch für die Klassifikationsaufgabe des logischen Oders trainiert haben, lernen wir in diesem Abschnitt eine Bibliothek kennen, die das Training für uns übernimmt.\n",
    "\n",
    "## Lernziele\n",
    "\n",
    "```{admonition} Lernziele\n",
    ":class: hint\n",
    "* Sie kennen die Bibliothek Scikit-Learn.\n",
    "* Sie können den grundlegenden Ablauf eines ML-Projektes mit Scikit-Learn beschreiben.\n",
    "* Sie wissen, wie Sie die Trainingsdaten als Matrix bzw. Vektor mit dem Modul Numpy repräsentieren.\n",
    "* Sie können das Perzeptron-Modell von Scikit-Learn laden und mit den gegebenen Trainingsdaten trainieren.\n",
    "* Sie wissen, wie Sie auf die Gewichte des gelernten Modells zugreifen.\n",
    "```\n",
    "\n",
    "## Scikit-Learn\n",
    "\n",
    "Gute Nachricht vorneweg: Sie müssen die ML-Algorithen nicht selbst\n",
    "implementieren, das haben bereits Wissenschaftler:innen aus der Mathematik und\n",
    "der Informatik erledigt. Eine der bekanntesten Bibliotheken bzw. eines der\n",
    "bekanntesten Module ist Scikit-Learn\n",
    "\n",
    "> https://scikit-learn.org/\n",
    "\n",
    "das wir auch für diese Vorlesung verwenden werden. Bitte stellen Sie jetzt\n",
    "sicher, dass Scikit-Learn bei Ihnen installiert ist. \n",
    "\n",
    "Das Modul Scikit-Learn wird mit ``sklearn`` abgekürzt, alle Funktionen werden\n",
    "also z.B. mit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa564b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2a0131",
   "metadata": {},
   "source": [
    "in den Namensraum importiert. Da das Modul aber so mächtig ist, werden wir immer\n",
    "nur einzelne Funktionen aus Scikit-Learn importieren. \n",
    "\n",
    "## Grundlegender ML-Workflow mit Scikit-Learn \n",
    "\n",
    "Die ML-Algorithmen des Scikit-Learn-Moduls haben dabei immer die gleiche\n",
    "Schnittstelle (API). Damit ist gemeint, dass die Methoden- und Funktionsnamen\n",
    "für alle ML-Modelle gleich gewählt sind. Auch ist die Art und Weise, wie die\n",
    "Trainingsdaten verarbeitet werden, stets gleich. Wir können den grundlegenden\n",
    "Ablauf eines typischen ML-Projektes folgendermaßen zusammenfassen:\n",
    "\n",
    "```{admonition} Wie funktioniert der grundlegende ML-Workflow mit Scikit-Learn?\n",
    ":class: note\n",
    "\n",
    "1. Zuerst packen wir die Inputdaten in eine Matrix X, bei der jede Spalte eine Eigenschaft repräsentiert. Die Zeilen stehen für die verschiedenen Datensätze.\n",
    "2. Falls wir ein überwachtes Lernverfahren anwenden wollen, packen wir die Outputdaten in einen Vektor y.\n",
    "3. Nun wählen wir ein Modell aus, das trainiert werden soll. \n",
    "4. Dazu spezifizieren wir die Hyperparameter des Modells. Hyperparameter sind Parameter des Modells, die wir vorab ohne die genaue Kenntnis der Daten festlegen.\n",
    "5. Wir trainieren das ML-Modell, indem wir die fit()-Methode des Modells aufrufen.\n",
    "6. Um das Modell zur Prognose neuer Daten zu verwenden, benutzten wir die predict()-Methode.\n",
    "7. Zuletzt wird das Modell analysiert, validiert und produktiv eingesetzt.\n",
    "```\n",
    "\n",
    "## Das logische Oder Klassifikationsproblem - diesmal mit Scikit-Learn \n",
    "\n",
    "Im letzten Abschnitt {ref}`perzeptron_training_logisches_oder` haben wir\n",
    "händisch ein Perzeptron trainiert. Zur Erinnerung, wenn wir die Bias-Einheit\n",
    "weglassen, lautet das logische Oder in Tabellenform wie folgt:\n",
    "\n",
    "x1 | x2 | y\n",
    "---|----|---\n",
    " 0 | 0  | 0\n",
    " 0 | 1  | 1\n",
    " 1 | 0  | 1\n",
    " 1 | 1  | 1\n",
    "\n",
    "Diese Daten formulieren wir nun als eine Inputmatrix $X$ mit den Spalten x1 und\n",
    "x2. Für die Erzeugung und Weiterverarbeitung der Matrizen laden wir das Modul\n",
    "Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7d65e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c7a9e6",
   "metadata": {},
   "source": [
    "Den Output formulieren wir als Vektor, ebenfalls mit Hilfe des Numpy-Moduls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d412bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0fef8c",
   "metadata": {},
   "source": [
    "Nun wählen wir das Perzeptron als das zu trainierende ML-Modell aus. Direkt beim\n",
    "Laden des Modells legen wir die Hyperparameter des Modells fest. Welche\n",
    "Hyperparameter ein Modell hat, steht in der\n",
    "[Perzeptron-Dokumentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html?highlight=perceptron#sklearn.linear_model.Perceptron).\n",
    "In diesem Fall wäre beispielsweise die Lernrate ein Hyperparameter. Laut\n",
    "Dokumentation wird die Lernrate beim Scikit-Learn-Perzeptron mit `eta0`\n",
    "bezeichnet. Der Python-Code, um das Perzeptron-Modell mit einer Lernrate von 1\n",
    "zu laden, lautet also wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "450af073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron \n",
    "modell = Perceptron(eta0 = 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1042d",
   "metadata": {},
   "source": [
    "Nun können wir das Perzeptron-Modell mit den Input- und Outputdaten trainieren, indem wir die `.fit()`-Methode aufrufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36eee080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modell.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11410582",
   "metadata": {},
   "source": [
    "Nachdem wir den letzten Python-Befehl ausgeführt haben, passiert scheinbar\n",
    "nichts. Nur der Klassenname `Perceptron()` des Objekts `modell` wird ausgegeben\n",
    "(wenn Sie den Code interaktiv ausführen). Intern wurde jedoch das\n",
    "Perzeptron-Modell trainiert, d.h. die Gewichte des Perzeptrons wurden iterativ\n",
    "bestimmt. Die Gewichte sind nun in dem Objekt `modell` gespeichert. Davon können\n",
    "wir uns überzeugen, indem wir auf die Attribute des Objekts zugreifen und diese\n",
    "anzeigen lassen. Die Gewichte sind in dem Attribut `.coef_` gespeichert, während\n",
    "das Gewicht der Bias-Einheit sich im Attribut `.intercept_` befindet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06bdd045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 2.]]\n",
      "[-1.]\n"
     ]
    }
   ],
   "source": [
    "print(modell.coef_)\n",
    "print(modell.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf913b41",
   "metadata": {},
   "source": [
    "Zuletzt können wir das trainierte Perzeptron-Modell Prognosen treffen lassen.\n",
    "Was prognostiziert das Modell beispielsweise für $x_1=0$ und $x_2=1$? Das\n",
    "tatsächliche Ergebnis der logischen Oder-Verknüpfung ist $y=1$, was liefert das\n",
    "Perzeptron?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0826b48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "y_prognose = modell.predict(np.array([[0, 1]]))\n",
    "print(y_prognose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7a9cc6",
   "metadata": {},
   "source": [
    "Wir können auch gleich für alle Datensätze eine Prognose erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "835f941c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_prognose = modell.predict(X)\n",
    "print(y_prognose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e810ba43",
   "metadata": {},
   "source": [
    "Tatsächlich funktioniert unser trainiertes Perzeptron zu 100 % korrekt und ist\n",
    "damit validiert. Bei nur vier Datensätzen lässt sich relativ leicht überblicken,\n",
    "dass alle vier Prognosen korrekt sind. Sobald die Datenanzahl zunimmt, wird es\n",
    "schwieriger, den Überblick zu behalten. Daher stellt Scikit-Learn die Methode\n",
    "`.score()` zur Verfügung, die bei Klassifikatoren die Anzahl der korrekt\n",
    "prognostizierten Outputs im Verhältnis zur Gesamtanzahl berechnet. Das Ergbnis\n",
    "ist also eine Bewertung zwischen 0 (keine einzige korrekte Prognose) und 1\n",
    "(perfekt Prognose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5adaf367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "genauigkeit = modell.score(X, y)\n",
    "print(genauigkeit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c91f0e",
   "metadata": {},
   "source": [
    "## Zusammenfassung und Ausblick\n",
    "\n",
    "Mit Scikit-Learn steht schon eine Implementierung des Perzeptrons zur Verfügung,\n",
    "die auch bei größeren Datenmengen eine binäre Klassifikation erlaubt. Welche\n",
    "Daten dabei überhaupt binär klassifiziert können, klären wir in einem der\n",
    "folgenden Abschnitte."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.14.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "source_map": [
   13,
   45,
   47,
   89,
   98,
   102,
   104,
   115,
   118,
   122,
   124,
   135,
   138,
   145,
   148,
   152,
   155,
   166,
   169
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}