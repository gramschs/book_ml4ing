---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.14.7
kernelspec:
  display_name: Python 3.9.12 ('python39')
  language: python
  name: python3
---

# Modellvalidierung

Mit dem Bestimmtheitsma√ü R¬≤-Score haben wir eine M√∂glichkeit kennengelernt,
einzusch√§tzen, wie gut unser Modell ist. Je n√§her der R¬≤-Score an der 1 ist,
desto besser. Aber selbst ein perfektes Bewertungsma√ü bedeutet nicht, dass das
Modell geeignet ist. Im Folgenden besch√§ftigen wir uns mit der Aufteilung von
Daten in Test- und Trainingsdaten.

## Lernziele 

```{admonition} Lernziele
:class: important
* Sie wissen, warum man Daten in **Trainingsdaten** und **Testdaten** aufteilt.
* Sie k√∂nnen mit der Funktion **train_test_split()** NumPy-Arrays und
  Pandas-DataFrames in Trainings- und Testdaten aufteilen.
* Sie kennen die Idee der **Kreuzvalidierung**.
```

+++

## Auswendiglernen n√ºtzt nichts

Um die Probleme bei der Modellauswahl zu verdeutlichen, betrachten wir einen
k√ºnstlich generierten Datensatz. Angenommen, wir h√§tten die folgenden sieben
Messwerte gemessen. 

```{code-cell} ipython3
import matplotlib.pylab as plt
import numpy as np

# styling of plots
plt.style.use('bmh')

# artificial data: f(x) = ‚àí2ùë•^2 + 8ùë• + 15 + error
X = np.array([-1, 0, 1, 2,  4, 4, 5], np.newaxis)
y = np.array([5.4384, 14.3252, 19.2451, 23.3703, 18.2885, 13.8978, 3.7586])

# visualization
fig, ax = plt.subplots()
ax.scatter(X,y)
ax.set_xlabel('Ursache')
ax.set_ylabel('Wirkung')
ax.set_title('K√ºnstlich generierte Messdaten');
```

Und nun w√ºrden wir das folgende Modell implementieren, der Name sagt alles! Um
nicht selbst den R¬≤-Score implementieren zu m√ºssen, nehmen wir den allgemeinen
von Scikit-Learn (siehe [Dokumentation Scikit-Learn ‚Üí
r2_score}(https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)).

```{code-cell} ipython3
from sklearn.metrics import r2_score

class AuswendigLerner:
    def __init__(self) -> None:
        self.X = None
        self.y = None

    def fit(self, X,y):
        self.X = X
        self.y = y

    def predict(self, X):
        return self.y

# choose model and train
model = AuswendigLerner()
model.fit(X, y)

# prediction
y_predict = model.predict(X)

# check quality
r2 = r2_score(y,y_predict)
print('Der R2-Score ist: {:.2f}'.format(r2))


```

Ein R¬≤-Score von 1, unser Modell scheint perfekt zu funktionieren! Aber wie
prognostiziert es etwas Neues? Das Modell funktioniert zwar perfekt f√ºr die
vorgegebenen Trainingsdaten, aber ist **nicht verallgemeinerbar**. 

+++

## Daten f√ºr sp√§ter aufheben

Bei der Modellauswahl und dem Training des Modells m√ºssen wir also zus√§tzlich
ber√ºcksichtigen, ob das Modell verallgemeinerbar ist, also auch f√ºr neue,
zuk√ºnftige Daten eine verl√§ssliche Prognose liefern wird. Da wir mit den uns
jetzt zur Verf√ºgung stehenden Daten ein geeignetes Modell ausw√§hlen und nicht
auf sp√§ter warten k√∂nnen, legen wir einen Teil der Daten zur Seite. Diese Daten
nennen wir **Testdaten**. Mit den √ºbriggebliebenen Daten trainieren wir das
ML-Modell. Diese Daten nennen wir **Trainingsdaten**. Sp√§ter nutzen wir dann die
Testdaten, um zu √ºberpr√ºfen, wie gut das Modell f√ºr neue Daten funktioniert, mit
denen nicht trainiert wurde.

F√ºr die Aufteilung in Trainings- und Testdaten nehmen wir eine ma√ügeschneiderte
Funktion von Scikit-Learn namens `train_test_split()` (siehe auch [Dokumentation
Scikit-Learn ‚Üí
train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)).
Liegen die Daten als NumPy-Arrays f√ºr Input und Output vor, werden beide als
Argument der Funktion `train_test_split()` √ºbergeben. Die Funktion
`train_test_split()` liefert dann vier Numpy-Arrays zur√ºck, jeweils zuerst die
Trainings-, dann die Testdaten. Neuerdings beherrscht die
train_test-split-Funktion auch DataFrames. Dann w√ºrde der Aufruf

```python
df_train, df_test = train_test_split(df)
```

lauten. Wir wenden jetzt train_test_split() auf unsere k√ºnstlichen Messdaten an.

```{code-cell} ipython3
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y)

fig, ax = plt.subplots()
ax.scatter(X_train, y_train, label='Trainingsdaten')
ax.scatter(X_test, y_test, label='Testdaten')
ax.set_xlabel('Ursache')
ax.set_ylabel('Wirkung')
ax.set_title('K√ºnstlich generierte Messdaten')
ax.legend(bbox_to_anchor=(1.35, 0.5));
```

Voreingestellt ist, dass die Funktion `test_train_split()` 25 % der Daten als
Testdaten zur√ºckh√§lt. Dabei werden die Daten, die in die Testdaten-Menge
wandern, zuf√§llig ausgew√§hlt. Daher erzeugt jeder Durchlauf des obigen Codes
auch eine andere Aufteilung in Test- und Trainingsdaten.

Die Funktion bietet aber auch Optionen, um den Split nach eigenen W√ºnschen zu
steuern.

* `test_size`: Mit der Option test_size kann auch ein anderer Wert als 25 %
  eingestellt werden. M√∂chte man beispielsweise nur 10 % der Daten f√ºr das
  Testen aufheben, so kann man `test_size=0.1` einstellen. Die prozentuale
  Angabe erfolgt dabei als ein Float zwischen 0.0 und 1.0. Verwenden wir
  hingegen einen Integer, so geht Scikit-Learn davon aus, dass wir die Anzahl
  der Testdaten spezifieren m√∂chten. `test_size = 2` bedeutet, dass zwei
  Datenpunkte f√ºr sp√§ter aufbewahren.
* `random_state`: Das zuf√§llige Ausw√§hlen der Testdaten wird mit einem
  Zufallszahlengenerator durchgef√ºhrt, der bei jedem Durchgang neu gestartet
  wird. Wenn wir zwar eine zuf√§llige Auswahl haben wollen, aber den Neustart des
  Zufallszahlengenerators verhindern wollen, k√∂nnen wir den Ausganszustand des
  Zufallszahlengenerators fixieren. Dazu wird ein Integer benutzt. Das ist vor
  allem f√ºr Pr√§sentationen oder Lehrmaterialien interessant.

```{code-cell} ipython3
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=2, random_state=0)

fig, ax = plt.subplots()
ax.scatter(X_train, y_train, label='Trainingsdaten')
ax.scatter(X_test, y_test, label='Testdaten')
ax.set_xlabel('Ursache')
ax.set_ylabel('Wirkung')
ax.set_title('K√ºnstlich generierte Messdaten')
ax.legend(bbox_to_anchor=(1.35, 0.5));
```

## Idee der Kreuzvalidierung

Wenn wir einen Teil der Daten als Testdaten f√ºr die sp√§tere Validierung
zur√ºckbehalten, hat das auch den Nachteil, dass wir weniger Datens√§tze f√ºr das
Training haben. Und sollten wir ohnehin nur wenige Daten insgesamt haben, kann
das zu ungenauen schlecht trainierten Modellen f√ºhren. Sinnvoll ist
daher die **Kreuzvalidierung**.

Die Idee der zweifachen Kreuzvalidierung ist wie folgt. Wir teilen die Daten in
zwei Datens√§tze A und B und trainieren zweimal und testen dann auch zweimal. Um
die Performance des Modells zu beurteilen, nehmen wir den Mittelwerte der beiden
Tests.

Die Idee der dreifachen Kreuzvalidierung ist, drei Durchl√§ufe zu machen. Wir
teilen die Daten in drei Datens√§tze A, B und C auf. Beim ersten Durchlauf
trainieren wir mit A, B und testen mit C. Beim zweiten Durchlauf trainieren wir
mit B,C und testen mit A. Und beim dritten Durchlauf findet das Training mit C,A
statt und der Test mit B. Zuletzt bilden wir wieder den Mittelwert aus den drei
Testwerten.

Nat√ºrlich l√§sst sich diese Idee immer weiter treiben.

+++

## Zusammenfassung

In diesem Abschnitt haben wir uns mit der Idee befasst, Daten in Trainings- und
Testdaten aufzuteilen. Dabei gibt es den einfachen Split oder die
Kreuzvalidierung.
