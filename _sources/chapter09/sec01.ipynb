{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e14bef90",
   "metadata": {},
   "source": [
    "# 9.1 Stacking, Bagging und Boosting\n",
    "\n",
    "Eins, zwei, viele ... im Bereich des maschinellen Lernens sind Ensemble-Methoden\n",
    "eine leistungsstarke Technik zur Verbesserung der Modellgenauigkeit und\n",
    "Robustheit. Diese Methoden kombinieren mehrere Modelle, um die Gesamtleistung zu\n",
    "steigern, indem sie die individuellen Stärken der Modelle nutzen und deren\n",
    "Schwächen ausgleichen. In diesem Kapitel werden wir die grundlegenden\n",
    "Konzepte und Unterschiede zwischen diesen drei Methoden erläutern, um ein\n",
    "besseres Verständnis ihrer Funktionsweise und Anwendungen zu vermitteln.\n",
    "\n",
    "```{admonition} Lernziele\n",
    ":class: goals\n",
    "* Sie können in eigenen Worten erklären, was **Ensemble-Methoden** sind.\n",
    "* Sie können die drei Ensemble-Methoden\n",
    "  * **Stacking**,\n",
    "  * **Bagging** und \n",
    "  * **Boosting**\n",
    "\n",
    "  mit Hilfe einer Skizze erklären.\n",
    "```\n",
    "\n",
    "## Ensemble-Methoden\n",
    "\n",
    "Der Begriff »Ensemble« wird im Allgemeinen eher mit Musik und Kunst in\n",
    "Verbindung gebracht als mit Informatik. In der Musik bezeichnet ein Ensemble\n",
    "eine kleine Gruppe von Musikern, die entweder das gleiche Instrument spielen\n",
    "oder verschiedene Instrumente kombinieren. Im Theater bezeichnet man eine Gruppe\n",
    "von Schauspielern ebenfalls als Ensemble, und in der Architektur beschreibt der\n",
    "Begriff eine Gruppe von Gebäuden, die in einem besonderen Zusammenhang\n",
    "zueinander stehen.   \n",
    "\n",
    "Auch im Bereich des maschinellen Lernens hat sich der Begriff Ensemble\n",
    "etabliert. Mit **Ensemble-Methoden** (Ensemble Learning) wird eine Gruppe von\n",
    "maschinellen Modellen bezeichnet, die zusammen eine Prognose treffen sollen.\n",
    "Ähnlich wie bei Musik-Ensembles können beim **Ensemble Learning** entweder\n",
    "identische Modelle oder verschiedene Modelle kombiniert werden. Diese Modelle\n",
    "können entweder gleichzeitig eine Prognose treffen, die dann kombiniert wird,\n",
    "oder nacheinander verwendet werden, wobei ein Modell auf den Ergebnissen eines\n",
    "anderen aufbaut. Je nach Vorgehensweise unterscheidet man im maschinellen Lernen\n",
    "zwischen **Stacking**, **Bagging** und **Boosting**.\n",
    "\n",
    "In dieser Vorlesung konzentrieren wir uns auf Bagging und Boosting mit ihren\n",
    "bekanntesten Vertretern, den Random Forests und XGBoost. Das Konzept des\n",
    "Stackings wird hier nur kurz ohne weitere Details vorgestellt. Eine allgemeine\n",
    "Einführung in Ensemble-Methoden mit Scikit-Learn findet sich in der\n",
    "[Dokumentation Scikit-Learn →\n",
    "Ensemble](https://scikit-learn.org/stable/modules/ensemble.html).\n",
    "\n",
    "\n",
    "## Stacking\n",
    "\n",
    "```{figure} pics/concept_stacking.svg\n",
    "---\n",
    "width: 100%\n",
    "---\n",
    "Die Prognosen von mehreren *unterschiedlichen* ML-Modellen werden zu einer\n",
    "finalen Prognose kombiniert. Die Kombination kann beispielsweise durch\n",
    "Mehrheitsentscheidung (Voting) oder Mittelwertbildung (Averaging) erfolgen.\n",
    "Werden die Einzelprognosen durch ein weiteres ML-Modell zu einer finalen\n",
    "Prognose kombiniert, nennt man das *Stacking*.\n",
    "```\n",
    "\n",
    "Stacking bedeutet auf Deutsch »Stapeln«, es werden sozusagen verschiedene\n",
    "ML-Modelle gestapelt. In einem ersten Schritt werden mehrere ML-Modelle\n",
    "unabhängig voneinander auf den Trainingsdaten trainiert. Jedes dieser Modelle\n",
    "liefert eine Prognose, die dann auf verschiedene Arten miteinander kombiniert\n",
    "werden können. Bei Klassifikationsaufgaben ist **Voting**, also die Wahl durch\n",
    "**Mehrheitsentscheidung**, eine beliebte Methode, um die Einzelprognosen zu\n",
    "kombinieren. Wurden beispielsweie für das Stacking drei ML-Modellen gewählt, die\n",
    "jeweils ja oder nein prognostizieren, dann wird für die finale Prognose das\n",
    "Ergebnis genommen, das die Mehrheit der einzelnen Modelle vorausgesagt hat.\n",
    "Scikit-Learn bietet dafür einen Voting Classifier an, siehe [Dokumentation\n",
    "Scikit-Learn → Voting\n",
    "Classifier](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier). \n",
    "\n",
    "Bei Regressionsaufgaben werden die einzelnen Prognosen häufig gemittelt. Dabei\n",
    "kann entweder der übliche arithmetische Mittelwert verwendet werden oder ein\n",
    "**gewichteter Mittelwert**, was als  **Weighted Averaging** bezeichnet wird.\n",
    "Nichtsdestotrotz wird die Mittelwertbildung bei Regressionsaufgaben von\n",
    "Scikit-Learn ebenfalls als Voting bezeichnet, siehe [Dokumentation Scikit-Learn\n",
    "→ Voting\n",
    "Regressor](https://scikit-learn.org/stable/modules/ensemble.html#voting-regressor). \n",
    "\n",
    "Eine alternative Kombinationsmethode ist die Verwendung eines weiteren\n",
    "ML-Modells. In diesem Fall werden die Modelle, die die einzelnen Prognosen\n",
    "liefern, als Basismodelle bezeichnet. In der ML-Community ist auch der\n",
    "Fachbegriff **Weak Learner**, also schwache Lerner, für diese Basismodelle\n",
    "gebräuchlich. Die Prognosen der Basismodelle dienen dann als Trainingsdaten für\n",
    "ein weiteres ML-Modell, das als **Meta-Modell** bezeichnet wird. Diese\n",
    "Ensemble-Methode wird **Stacking** genannt. Weitere Informationen liefert die\n",
    "[Scikit-Learn Dokumentation → Stacked\n",
    "Generalization](https://scikit-learn.org/stable/modules/ensemble.html#stacked-generalization).\n",
    "\n",
    "Stacking bietet viele Vorteile. Der wichtigste Vorteil ist, dass die\n",
    "Prognosefähigkeit des Gesamtmodells in der Regel deutlich besser ist als die der\n",
    "einzelnen Basismodelle. Die Stärken der Basismodelle werden kombiniert und die\n",
    "Schwächen ausgeglichen. Allerdings erfordert Stacking sehr viel Feinarbeit. Auch\n",
    "steigt die Trainingszeit für das Gesamtmodell, selbst wenn die Basismodelle bei\n",
    "genügend Rechenleistung parallel trainiert werden können. Aus diesem Grund\n",
    "werden wir in dieser Vorlesung kein Stacking verwenden.\n",
    "\n",
    "\n",
    "## Bagging \n",
    "\n",
    "```{figure} pics/concept_bagging.svg\n",
    "---\n",
    "width: 100%\n",
    "---\n",
    "Beim Bagging wird das gleiche ML-Modell auf *unterschiedlichen* Stichproben der\n",
    "Trainingsdaten trainiert (Bootstrapping). Die Einzelprognosen der Modelle werden\n",
    "dann zu einer finalen Prognose kombiniert (Aggregating).\n",
    "```\n",
    "\n",
    "Bagging ist eine Ensemble-Methode, ähnlich wie Stacking. Im Gegensatz zum\n",
    "Stacking wird beim Bagging jedoch dasselbe Modell für die Einzelprognosen\n",
    "verwendet. Die Unterschiede in den Einzelprognosen entstehen dadurch, dass für\n",
    "das Training der einzelnen Modelle unterschiedliche Daten verwendet werden.\n",
    "\n",
    "Im ersten Schritt werden zufällige Datenpunkte aus den Trainingsdaten ausgewählt\n",
    "und in einen neuen Datensatz, „Stichprobe 1“, aufgenommen. Nachdem ein\n",
    "Datenpunkt ausgewählt wurde, kehrt er in die ursprüngliche Menge der\n",
    "Trainingsdaten zurück und kann erneut ausgewählt werden. Dieser Prozess wird in\n",
    "der Mathematik als **Ziehen mit Zurücklegen** bezeichnet, auf Englisch\n",
    "**Bootstrapping**. Durch Bootstrapping werden dann noch weitere Stichproben\n",
    "gebildet.\n",
    "\n",
    "Im zweiten Schritt wird ein ML-Modell gewählt und für jede Bootstrap-Stichprobe\n",
    "trainiert. Da die Stichproben unterschiedliche Trainingsdaten enthalten,\n",
    "entstehen unterschiedlich trainierte Modelle, die für neue Daten verschiedene\n",
    "Einzelprognosen liefern. Diese Einzelprognosen werden kombiniert bzw. nach\n",
    "festgelegten Regeln zu einer finalen Prognose zusammengefasst. In der Statistik\n",
    "wird die Zusammenfassung von Daten als Aggregation bezeichnet. Auf Englisch\n",
    "heißt der Vorgang des Zusammenfassens **Aggregating**.\n",
    "\n",
    "Die beiden wesentlichen Schritte der Bagging-Methode sind also **B**ootstrapping\n",
    "und **Agg**regat**ing**, was zu der Abkürzung Bagging geführt hat. Scikit-Learn\n",
    "bietet sowohl für Klassifikations- als auch für Regressionsaufgaben eine\n",
    "allgemeine Implementierung der Bagging-Methode an (siehe [Dokumentation\n",
    "Scikit-Learn →\n",
    "Bagging](https://scikit-learn.org/stable/modules/ensemble.html#bagging-meta-estimator)).\n",
    "Die bekannteste Bagging-Methode ist das Verfahren **Random Forests**, bei dem\n",
    "Entscheidungsbäume (Decision Trees) auf unterschiedlichen Stichproben trainiert\n",
    "und aggregiert werden. Random Forests werden wir im nächsten Kapitel\n",
    "detaillierter betrachten. Vorab beschäftigen wir uns noch mit dem Konzept der\n",
    "Boosting-Methoden.\n",
    "\n",
    "\n",
    "## Boosting\n",
    "\n",
    "```{figure} pics/concept_boosting.svg\n",
    "---\n",
    "width: 100%\n",
    "---\n",
    "Der Fehler in der Prognose wird benutzt, um das nächste Modell zu trainieren.\n",
    "Beim hier gezeigten Adaboost-Verfahren werden die Daten neu gewichtet, beim\n",
    "(Stochastic) Gradient Boosting werden Modelle zur Fehlerkorrektur trainiert.\n",
    "```\n",
    "\n",
    "Das englische Verb „to boost sth.“ hat viele Bedeutungen. Insbesondere wird es\n",
    "im Deutschen mit „etwas verstärken“ übersetzt. Im Kontext des maschinellen\n",
    "Lernens bezeichnet **Boosting** eine Ensemble-Methode, bei der mehrere ML-Modelle\n",
    "hintereinander geschaltet werden, um die Genauigkeit der Prognose zu verstärken.\n",
    "Die Idee des Boosting besteht darin, dass jedes Modell die Fehler des\n",
    "Vorgängermodells reduziert. Es gibt mehrere Varianten zur Fehlerreduktion, aus\n",
    "denen sich unterschiedliche Boosting-Methoden ableiten. Die wichtigsten\n",
    "Varianten sind:\n",
    "\n",
    "* Adaboost,\n",
    "* Gradient Boosting und\n",
    "* Stochastic Gradient Boosting.\n",
    "\n",
    "Beim **Adaboost**-Verfahren wird im ersten Schritt ein Modell (z.B. ein\n",
    "Entscheidungsbaum) auf den Trainingsdaten trainiert. Anschließend werden die\n",
    "Prognosen dieses Modells mit den tatsächlichen Werten verglichen. Im zweiten\n",
    "Schritt wird ein neuer Datensatz erstellt, wobei die Datenpunkte, die falsch\n",
    "prognostiziert wurden, ein größeres Gewicht erhalten. Nun wird erneut ein Modell\n",
    "trainiert; und dessen Prognosen werden wieder mit den echten Werten verglichen.\n",
    "Dieser Vorgang wird mehrfach wiederholt. Das Training der Modelle erfolgt\n",
    "sequentiell, da jedes Vorgängermodell die neue Gewichtung der Trainingsdaten\n",
    "liefert. Am Ende werden alle Einzelprognosen gewichtet zu einer finalen Prognose\n",
    "kombiniert. Weitere Details finden sich in der [Dokumentation Scikit-Learn →\n",
    "Adaboost](https://scikit-learn.org/stable/modules/ensemble.html#adaboost).\n",
    "\n",
    "Beim **Gradient Boosting** wird ebenfalls ein sequentieller Ansatz verfolgt,\n",
    "aber der Fokus liegt auf der Minimierung der Fehler. Im ersten Schritt wird ein\n",
    "ML-Modell (häufig ein Entscheidungsbaum) trainiert. Danach wird für jeden\n",
    "Datenpunkt der Fehler des Modells, das sogenannte **Residuum**, berechnet, indem\n",
    "die Differenzen zwischen dem tatsächlichen Wert und der Prognosen bestimmt wird.\n",
    "Im nächsten Schritt wird ein neues Modell trainiert, das darauf abzielt, diese\n",
    "Residuen vorherzusagen. Dieses neue Modell wird dann zu dem vorherigen Modell\n",
    "hinzugefügt, um die Gesamtprognose zu verbessern. Dieser Prozess wird\n",
    "wiederholt, wobei in jeder Iteration ein neues Modell trainiert wird, das die\n",
    "Fehler der bisherigen Modelle reduziert (mit Hilfe einer Verlustfunktion und\n",
    "eines Gradientenverfahrens). Am Ende ergibt sich eine starke Vorhersage, indem\n",
    "alle Modelle kombiniert werden. Da sehr häufig Entscheidungsbäume als Modell\n",
    "gewählt werden, bietet Scikit-Learn eine Implementierung der sogenannten\n",
    "**Gradient Boosted Decision Trees** an, siehe [Dokumentation Scikit-Learn →\n",
    "Gradient-boosted\n",
    "trees](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosted-trees).\n",
    "\n",
    "**Stochastic Gradient Boosting** ist eine Erweiterung des Gradient Boosting, bei\n",
    "der zusätzlich Stochastik eingeführt wird. Hierbei wird in jedem Schritt nur\n",
    "eine zufällige Stichprobe der Trainingsdaten verwendet, um ein Modell zu\n",
    "trainieren. Der Trainingsprozess ähnelt dem von Gradient Boosting, wobei in\n",
    "jeder Runde ein neues Modell trainiert wird, das die Fehler der vorherigen\n",
    "Modelle korrigiert. Durch die zufällige Auswahl der Trainingsdaten in jeder\n",
    "Iteration wird eine höhere Robustheit gegenüber Overfitting (Überanpassung)\n",
    "erreicht. Stochastic Gradient Boosting wird nicht direkt von Scikit-Learn\n",
    "unterstützt. Eine sehr bekannte Implmentierung davon ist XGBoost (siehe\n",
    "[https://xgboost.readthedocs.io/](https://xgboost.readthedocs.io/en/stable/)),\n",
    "die wir in einem der nächsten Kapitel noch näher betrachten werden.\n",
    "\n",
    "\n",
    "## Zusammenfassung und Ausblick\n",
    "\n",
    "In diesem Kapitel haben Sie die drei Konzepte Stacking, Bagging und Boosting\n",
    "eher theoretisch kennengelernt. Alle drei Methoden sind Ensemble-Methoden, bei\n",
    "denen mehrere ML-Modelle parallel oder sequentiell kombiniert werden. Obwohl\n",
    "diese Ensemble-Methoden allgemein für verschiedene ML-Modelle eingesetzt werden\n",
    "können, haben sich vor allem Random Forests (Bagging für Entscheidungsbäume) und\n",
    "Stochastic Gradient Boosting als besonders effektiv erwiesen. Letztere sind\n",
    "nicht in Scikit-Learn implementiert, sondern werden durch eine eigene Bibliothek\n",
    "namens XGBoost bereitgestellt. In den nächsten beiden Kapiteln werden wir beide\n",
    "auch mit praktischen Beispielen vertiefen."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.15.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "source_map": [
   13
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}