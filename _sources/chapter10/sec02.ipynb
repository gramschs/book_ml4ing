{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db1e7250",
   "metadata": {},
   "source": [
    "# 10.2 Training SVM mit Scikit-Learn\n",
    "\n",
    "```{admonition} Lernziele\n",
    ":class: goals\n",
    "* Sie können ein SVM-Modell mit Scikit-Learn trainieren.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8da54c",
   "metadata": {},
   "source": [
    "## Scikit-Learn bietet mehrere Implementierungen\n",
    "\n",
    "Wenn wir in der Dokumentation von Scikit-Learn\n",
    "[Scikit-Learn/SVM](https://scikit-learn.org/stable/modules/svm.html) die Support\n",
    "Vector Machines nachschlagen, so finden wir drei Einträge\n",
    "\n",
    "* SVC, \n",
    "* NuSVC und \n",
    "* LinearSVC.\n",
    "\n",
    "Die Beispiele des letzten Abschnittes sind linearer Natur, so dass sich\n",
    "eigentlich die Klasse \"LinearSVC\" aus Effiziengründen anbieten würde. Da wir\n",
    "aber im nächsten Abschnitt uns auch mit nichtlinearen Problemen beschäftigen\n",
    "werden, fokussieren wir uns gleich auf den SVC-Algorithmus mit seinen Optionen.\n",
    "NuSVC ist ähnlich zu SVC, bietet aber die zusätzliche Möglichkeit, die Anzahl\n",
    "der Stützvektoren einzuschränken.\n",
    "\n",
    "Vielleicht wundern Sie sich, dass die Klasse SVC und nicht SVM heißt. Das C in\n",
    "SVC soll deutlich machen, dass wir die Support Vector Machines nutzen wollen, um\n",
    "ein Klassifikationsproblem (= Classification Problem) zu lösen.\n",
    "\n",
    "## Training mit fit und score\n",
    "\n",
    "Zuerst importieren wir aus Scikit-Learn das entsprechende Modul 'SVM' und\n",
    "instantiieren ein Modell. Da wir die etwas allgemeinere Klasse SVC anstatt\n",
    "LinearSVC verwenden, müssen wir bereits bei der Erzeugung die Option `kernel=`\n",
    "auf linear setzen, also `kernel='linear'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6de54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm_modell = svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f279c7",
   "metadata": {},
   "source": [
    "Wir erzeugen uns erneut künstliche Messdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "917b49b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_blobs\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpylab\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m; plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbmh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# generate artificial data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m X, y \u001b[38;5;241m=\u001b[39m make_blobs(n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m, centers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, cluster_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.50\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pylab as plt; plt.style.use('bmh')\n",
    "\n",
    "# generate artificial data\n",
    "X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.50)\n",
    "\n",
    "# plot artificial data\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(x = X[:,0], y = X[:,1],  color=y, color_continuous_scale=['#3b4cc0', '#b40426'],\n",
    "                 title='Künstliche Daten',\n",
    "                 labels={'x': 'Feature 1', 'y': 'Feature 2'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d04c2",
   "metadata": {},
   "source": [
    "Als nächstes teilen wir die Messdaten in Trainings- und Testdaten auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f414f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c6c0b",
   "metadata": {},
   "source": [
    "Nun können wir unser SVM-Modell trainieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_modell.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ca9fe8",
   "metadata": {},
   "source": [
    "Und als nächstes analysieren, wie viele der Testdaten mit dem trainierten Modell\n",
    "korrekt klassifiziert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe902927",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_modell.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319c0e82",
   "metadata": {},
   "source": [
    "Ein super Ergebnis! Schön wäre es jetzt noch, die gefundene Trenngerade zu\n",
    "visualisieren. Dazu modifizieren wir einen Code-Schnippsel aus dem Buch: »Data\n",
    "Science mit Python« von Jake VanderPlas (mitp Verlag 2017), ISBN 978-3-95845-\n",
    "695-2, siehe\n",
    "[https://github.com/jakevdp/PythonDataScienceHandbook](https://github.com/jakevdp/PythonDataScienceHandbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad8af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelle: VanderPlas \"Data Science mit Python\", S. 482\n",
    "# modified by Simone Gramsch\n",
    "import numpy as np\n",
    "\n",
    "def plot_svc_grenze(model):\n",
    "    # aktuelles Grafik-Fenster auswerten\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    # Raster für die Auswertung erstellen\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    # Entscheidungsgrenzen und Margins darstellen\n",
    "    ax.contour(X, Y, P, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "    # Stützvektoren darstellen\n",
    "    ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, linewidth=1, facecolors='none', edgecolors='orange');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7c72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('SVM mit Soft Margin');\n",
    "\n",
    "plot_svc_grenze(svm_modell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb70c46b",
   "metadata": {},
   "source": [
    "## Der Parameter C\n",
    "\n",
    "Im letzten Abschnitt haben wir uns mit dem Parameter `C` beschäftigt, der\n",
    "Ausnahmen innerhalb des Sicherheitsstreifens erlaubt. Ein großes `C` bedeutet\n",
    "ja, dass die Wand des Margins hoch ist und kaum (oder gar keine) Punkte\n",
    "innerhalb des Sicherheitsstreifens liegen dürfen. Als nächstes schauen wir uns\n",
    "an, wie der Parameter `C` gesetzt wird.  \n",
    "\n",
    "Die Option zum Setzen des Parameters C lautet schlicht und einfach `C=`. Dabei\n",
    "muss C immer positiv sein.\n",
    "\n",
    "Damit aber besser sichtbar wird, wie sich C auswirkt, vermischen wir die\n",
    "künstlichen Daten stärker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3a76aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate artificial data\n",
    "X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.80)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# plot artificial data\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(x = X[:,0], y = X[:,1],  color=y, color_continuous_scale=['#3b4cc0', '#b40426'],\n",
    "                 title='Künstliche Daten',\n",
    "                 labels={'x': 'Feature 1', 'y': 'Feature 2'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6223429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wahl des Modells mit linearem Kern und großem C\n",
    "svm_modell = svm.SVC(kernel='linear', C=1000000)\n",
    "\n",
    "# Training und Bewertung\n",
    "svm_modell.fit(X_train, y_train);\n",
    "svm_modell.score(X_test, y_test)\n",
    "\n",
    "# Visualisierung\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('SVM mit Soft Margin');\n",
    "plot_svc_grenze(svm_modell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97415ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wahl des Modells mit linearem Kern und kleinem C\n",
    "svm_modell = svm.SVC(kernel='linear', C=1)\n",
    "\n",
    "# Training und Bewertung\n",
    "svm_modell.fit(X_train, y_train);\n",
    "svm_modell.score(X_test, y_test)\n",
    "\n",
    "# Visualisierung\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('SVM mit Soft Margin');\n",
    "plot_svc_grenze(svm_modell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f3b98",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "Verwenden wir den SVC-Klassifikator aus dem Modul SVM von Scikit-Learn, können\n",
    "wir mittels der Option `kernel='linear'` eine binäre Klassifikation durchführen,\n",
    "bei der die Trennungsgerade den größtmöglichen Abstand zwischen den\n",
    "Punkteclustern erzeugt, also einen möglichst großen Margin. Sind die Daten nicht\n",
    "linear trennbar, so können wir mit der Option `C= ` steuern, wie viele Ausnahmen\n",
    "erlaubt werden sollen. Mit Ausnahmen sind Punkte innerhalb des Margins gemeint.\n",
    "Im nächsten Abschnitt betrachten wir nichtlineare Trennungsgrenzen."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.15.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "source_map": [
   13,
   22,
   52,
   55,
   59,
   73,
   77,
   80,
   84,
   86,
   91,
   93,
   102,
   126,
   135,
   151,
   165,
   183,
   198
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}