{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b589d65b",
   "metadata": {},
   "source": [
    "# Übung\n",
    "\n",
    "```{admonition} Aufgabe \n",
    ":class: tip\n",
    "\n",
    "Das Schiff Titanic galt bei seiner Fertigstellung als unsinkbar. 1912\n",
    "kollidierte die Titanic mit einem Eisberg und sank. Bei dem Unglück kamen 1514\n",
    "von 2220 Personen ums Leben, so dass der titanic-Untergan zu den größten\n",
    "Unglücken der Schifffahrt zählt. Mehr Informationen zu der Titanic finden Sie\n",
    "bei Wikipedia https://de.wikipedia.org/wiki/Titanic_(Schiff).\n",
    "\n",
    "In der folgenden Übung werden Passagierlisten der Titanic benutzt, um die\n",
    "Überlebenswahrscheinlichkeit zu prognostizieren (0 = gestorben, 1 = überlebt),\n",
    "deren Quelle hier ist: https://www.kaggle.com/c/titanic\n",
    "\n",
    "Laden sie den Datensatz 'titanic_train_DE.csv'. Führen Sie dann eine explorative\n",
    "Datenanalyse (EDA) durch. Trainieren Sie zuletzt ein Perzeptron, ein\n",
    "logistisches Regressionsmodell und ein SVM. Welches Modell kann am besten die\n",
    "Überlebenswahrscheinlichkeit prognostizieren? Sie können Ihr Modell mit den\n",
    "Testdaten validieren.\n",
    "```\n",
    "\n",
    "````{admonition} Lösung \n",
    ":class: tip, toggle\n",
    "\n",
    "```python\n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv('titanic_train_DE.csv', index_col=0)\n",
    "```\n",
    "\n",
    "```python\n",
    "data.info()\n",
    "```\n",
    "\n",
    "```python\n",
    "data.head()\n",
    "```\n",
    "\n",
    "Der Datensatz 'titanic_train_DE.csv' enthält 891 Einträge. Es gibt 11 Eigenschaften, wobei die Kategorie ueberlebt, die Klasse, die Anzahl_Geschwister_Partner und Anzahl_Eltern_Kinder durch Integers repräsentiert werden, das Alter und der Ticketpreis sind Floats und die Eigenschaften Name, Geschlecht, Ticket, Kabine und Einstiegshafen sind Objekte.\n",
    "\n",
    "Die Eigenschaften Alter, Kabine und Einstiegshafen sind unvollständig.\n",
    "\n",
    "```python\n",
    "data.describe()\n",
    "```\n",
    "\n",
    "Die Eigenschaften ueberlebt und Klasse werden zwar durch Integers repräsentiert, aber ein Blick in die Daten zeigt, dass es sich um kategoriale Daten handelt.\n",
    "\n",
    "```python\n",
    "data['ueberlebt'].unique()\n",
    "```\n",
    "\n",
    "Es gibt nur zwei Kategorien für die Eigenschaft 'ueberlebt', nämlich 0 (gestorben) und 1 (überlebt).\n",
    "\n",
    "```python\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(data['ueberlebt'].value_counts(), title='Trainingsdaten Titanic',\n",
    "             labels={'index': 'Überlebensvariable', 'value': 'Anzahl Personen'})\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "Bei dem Titanic-Unglück sind mehr Personen gestorben (549) als überlebt haben (342).\n",
    "\n",
    "```python\n",
    "data['Klasse'].unique()\n",
    "```\n",
    "\n",
    "Es gibt nur drei Kategorien bei der Eigenschaft Klasse, nämlich Klasse 1, 2 und 3.\n",
    "\n",
    "```python\n",
    "fig = px.bar(data['Klasse'].value_counts(),\n",
    "             title='Trainingsdaten Titanic',\n",
    "             labels={'index': 'Klasse', 'value': 'Anzahl Personen'})\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "In der 2. Klasse gab es am wenigsten Passagiere (184). 216 Personen fuhren in der 1. Klasse mit und am meisten (491) Personen reisten in der 3. Klasse. \n",
    "\n",
    "```python\n",
    "fig = px.box(data['Alter'],\n",
    "             title='Trainingsdaten Titanic',\n",
    "             labels={'variable': '', 'value': 'Alter [Jahre]'})\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "Die Passagiere sind zwischen 0 und 80 Jahre alt. Die Hälfte aller Passigiere ist im Alter von 20 bis 38 bei einem Median von 28 Jahren. Der Median stimmt auch gut mit dem Mittelwert von 29.7 Jahren überein. Es gibt allerdings auch Ausreißer nach oben.\n",
    "\n",
    "```python\n",
    "fig = px.bar(data['Anzahl_Geschwister_Partner'].value_counts(),\n",
    "             title='Trainingsdaten Titanic',\n",
    "             labels={'index': 'Anzahl Geschwister/Partner', 'value': 'Anzahl Personen'})\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "Die meisten Passagiere (608) reisten ohne Geschwister oder Partner. 209 Passagiere gaben an, mit einem Geschwister oder Partner zu reisen. Die Anzahl von Passagieren, die mit zwei oder mehr Geschwistern/Partnern reisten, ist klein (zusammen 70 Personen).\n",
    "\n",
    "```python\n",
    "fig = px.bar(data['Anzahl_Eltern_Kinder'].value_counts(),\n",
    "             title='Trainingsdaten Titanic',\n",
    "             labels={'index': 'Anzahl der mitreisenden Eltern oder Kindern', 'value': 'Anzahl Personen'})\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "Auch sind die meisten Personen (687) ohne Eltern oder Kinder gereist. \n",
    "\n",
    "```python\n",
    "fig = px.box(data['Ticketpreis'],\n",
    "             title='Trainingsdaten Titanic',\n",
    "             labels={'variable': '', 'value': 'Preis'})\n",
    "\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "Bei den Ticketpreisen gibt es einen sehr deutlichen Ausreißer nach oben (512) und andere Ausreißer (zwischen 65 und 263), aber es konnten auch einige Personen kostenlos mitreisen.\n",
    "\n",
    "\n",
    "```python\n",
    "data_by_class = data.groupby('Klasse')\n",
    "\n",
    "fig = px.bar(data_by_class['ueberlebt'].mean(),\n",
    "             title='Trainingsdaten Titanic',\n",
    "             labels={'value': 'Überlwebenswahrscheinlichkeit'})\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "Die Passagiere der 1. Klasse hatten eine deutlich höhere Wahrscheinlichkeit (knapp 63 %), das Unglück zu überleben. Passagiere der 2. Klasse überlebten zu 47 %, wohingegen Passagiere der 3. Klasse nur zu 24 % überlebten.\n",
    "\n",
    "```python\n",
    "data_by_sex = data.groupby('Geschlecht')\n",
    "\n",
    "fig = px.bar(data_by_sex['ueberlebt'].mean(),\n",
    "             title='Trainingsdaten Titanic',\n",
    "             labels={'value': 'Überlebenswahrscheinlichkeit'})\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "Frauen und Kinder zuerst gilt tatsächlich für das Titanic-Unglück. Zumindest überlebten 74 % der weiblichen Passagiere, aber nur knapp 19 % der Männer.\n",
    "\n",
    "Vorbereitung der Daten\n",
    "\n",
    "```python\n",
    "data_cleaned = data.copy()\n",
    "data_cleaned = data.drop(columns=['Name', 'Ticket', 'Kabine', 'Einstiegshafen'], axis=0)\n",
    "\n",
    "data_cleaned['Geschlecht'] = data_cleaned['Geschlecht'].replace('maennlich', 0)\n",
    "data_cleaned['Geschlecht'] = data_cleaned['Geschlecht'].replace('weiblich', 1)\n",
    "\n",
    "data_cleaned = data_cleaned.dropna()\n",
    "data_cleaned.info()\n",
    "```\n",
    "\n",
    "Training der ML-Modelle\n",
    "\n",
    "```python\n",
    "y_train = data_cleaned['ueberlebt']\n",
    "X_train = data_cleaned.loc[:, 'Klasse' : 'Ticketpreis']\n",
    "```\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Perceptron \n",
    "\n",
    "model_perceptron = Perceptron()\n",
    "\n",
    "model_perceptron.fit(X_train, y_train)\n",
    "score_perceptron = model_perceptron.score(X_train, y_train)\n",
    "\n",
    "print(f'Score Perzeptron Trainingsdaten: {score_perceptron :.2f}')\n",
    "```\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_log_reg = LogisticRegression()\n",
    "\n",
    "model_log_reg.fit(X_train, y_train)\n",
    "score_log_reg = model_log_reg.score(X_train, y_train)\n",
    "\n",
    "print(f'Score Logistische Regression Trainingsdaten: {score_log_reg :.2f}')\n",
    "```\n",
    "\n",
    "```python\n",
    "from sklearn import svm\n",
    "\n",
    "model_svm = svm.SVC(kernel='linear')\n",
    "\n",
    "model_svm.fit(X_train, y_train)\n",
    "score_svm = model_svm.score(X_train, y_train)\n",
    "\n",
    "print(f'Score SVM Trainingsdaten: {score_svm :.2f}')\n",
    "```\n",
    "\n",
    "Am besten schneidet die logistische Regression ab, die eine Genauigkeit der Prognose auf den Trainingsdaten von 0.80 erreicht. Am zweitbesten funktioniert -- zumindest auf den Trainignsdaten -- das SVM-Modell mit einem Score von 0.78. Das Perzeptron ist mit einem Score von 0.68 am schlechtesten.\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.15.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "source_map": [
   13
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}