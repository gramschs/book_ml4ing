{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e66e4f",
   "metadata": {},
   "source": [
    "# 6.2 Entscheidungsbäume visualisieren und trainieren\n",
    "\n",
    "Im letzten Kapitel haben wir gelernt, wie mit Scikit-Learn ein Entscheidungsbaum\n",
    "für binäre Klassifikationsaufgaben trainiert wird. In diesem Kapitel werden wir\n",
    "uns damit beschäftigen, den trainierten Entscheidungsbaum von Scikit-Learn\n",
    "visualisieren zu lassen. Darüber hinaus lernen wir, was das\n",
    "Gini-Impurity-Kriterion ist und welche weiteren Einstellmöglichkeiten es für\n",
    "Entscheidungsbäume in Scikit-Learn gibt.\n",
    "\n",
    "\n",
    "## Lernziele\n",
    "\n",
    "```{admonition} Lernziele\n",
    ":class: goals\n",
    "* Sie können einen Entscheidungsbaum mit `plot_tree` visualisieren.\n",
    "* Sie wissen, was die Angaben `samples` und `value` bei der Visualisierung des\n",
    "  Entscheidungsbaumes bedeuten.\n",
    "* Sie wissen, was das **Gini-Impurity-Kriterium** ist.\n",
    "* Sie kennen weitere Parameter für Entscheidungsbäume wie `random_state=` oder\n",
    "  `criterion=`.\n",
    "```\n",
    "\n",
    "\n",
    "## Entscheidungsbäume visualisieren\n",
    "\n",
    "Im letzten Kapitel haben wir den Entscheidungsbaum für das Autohaus mit Hilfe\n",
    "des Moduls Scikit-Learn trainiert. Scikit-Learn bietet in dem Untermodul\n",
    "`sklearn.tree` nicht nur Algorithmen für Entscheidungsbäume an, sondern auch ein\n",
    "dazu passendes Visualisierungswerkzeug. Die Funktion `plot_tree` zeichnet den\n",
    "Entscheidungsbaum. Um diese Funktion auszuprobieren, wird zunächst der Datensatz\n",
    "mit den Autodaten erneut geladen, das Modell Entscheidungsbaum gewählt und\n",
    "anschließend trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ab3328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(random_state=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Sammlung der Daten \n",
    "daten = pd.DataFrame({\n",
    "    'Kilometerstand [km]': [32908, 20328, 13285, 17162, 27449, 13715, 32889,  3111, 15607, 18295],\n",
    "    'Preis [EUR]': [15960, 20495, 17227, 17851, 5428, 22772, 13581, 16793, 23253, 11382],\n",
    "    'verkauft': [False, True, False, True, False, True, False, True, True, False],\n",
    "    },\n",
    "    index=['Auto 1', 'Auto 2', 'Auto 3', 'Auto 4', 'Auto 5', 'Auto 6', 'Auto 7', 'Auto 8', 'Auto 9', 'Auto 10'])\n",
    "daten.head(10)\n",
    "\n",
    "# Auswahl des Modells: Entscheidungsbaum für Klassifikation\n",
    "modell = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Adaption der Daten\n",
    "X = daten[['Kilometerstand [km]', 'Preis [EUR]']]\n",
    "y = daten['verkauft']\n",
    "\n",
    "# Training des Modells\n",
    "modell.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba9fcc",
   "metadata": {},
   "source": [
    "Nun können wir die Funktion `plot_tree` importieren und das trainierte Modell\n",
    "visualisieren lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d8adc86",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_tree\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplot_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/python312/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/python312/lib/python3.12/site-packages/sklearn/tree/_export.py:211\u001b[0m, in \u001b[0;36mplot_tree\u001b[0;34m(decision_tree, max_depth, feature_names, class_names, label, filled, impurity, node_ids, proportion, rounded, precision, ax, fontsize)\u001b[0m\n\u001b[1;32m    196\u001b[0m check_is_fitted(decision_tree)\n\u001b[1;32m    198\u001b[0m exporter \u001b[38;5;241m=\u001b[39m _MPLTreeExporter(\n\u001b[1;32m    199\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39mmax_depth,\n\u001b[1;32m    200\u001b[0m     feature_names\u001b[38;5;241m=\u001b[39mfeature_names,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m     fontsize\u001b[38;5;241m=\u001b[39mfontsize,\n\u001b[1;32m    210\u001b[0m )\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexporter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecision_tree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/python312/lib/python3.12/site-packages/sklearn/tree/_export.py:636\u001b[0m, in \u001b[0;36m_MPLTreeExporter.export\u001b[0;34m(self, decision_tree, ax)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport\u001b[39m(\u001b[38;5;28mself\u001b[39m, decision_tree, ax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 636\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Annotation\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plot_tree(modell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ceb806",
   "metadata": {},
   "source": [
    "`plot_tree` produziert eine Textausgabe und ein Diagramm. Die Textausgabe kann\n",
    "unterdrückt werden, indem hinter den Funktionsaufruf `plot_tree(modell)` ein\n",
    "Semikolon `;` gesetzt wird. Das Diagramm zeichnet wie erwartet die Baumstruktur\n",
    "vom Wurzelknoten über die Knoten und Zweige bis hin zu den Blättern. Die\n",
    "Entscheidungsfragen stehen in der erste Zeile der Knoten. Danach folgen weitere\n",
    "Angaben wie `gini`, `samples` und `value`. Um diese Angaben zu erklären,\n",
    "ergänzen wir zunächst weitere Angaben. Mit der Option `feature_names=` wird eine\n",
    "Liste mit den Eigenschaften ergänzt, die Option `class_names=` ergänzt die\n",
    "Klassenbezeichnugnen. So erhalten wir folgendes Diagramm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4232fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(modell, \n",
    "    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n",
    "    class_names=['nicht verkauft', 'verkauft']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d85dd2",
   "metadata": {},
   "source": [
    "Was `gini` bedeuten könnte, erschließt sich so immer noch nicht, aber die\n",
    "Angaben `samples` und `values` können so leichter von ihrer Bedeutung her\n",
    "eingeordnet werden. `samples` gibt die Anzahl der Datenobjekte an, die sich in\n",
    "diesem Knoten befinden. `values` listet auf, wie viele Datenobjekte die\n",
    "Zielgröße `nicht verkauft` (= False bzw. 0) haben und wie viele zu der Klasse\n",
    "`verkauft` (= True bzw. 1) gehören. \n",
    "\n",
    "Weitere Details zu den Optionen der `plot_tree`-Funktion finden Sie in der\n",
    "[Dokumentation Scikit-Learn →\n",
    "plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html).\n",
    "\n",
    "Als nächstes widmen wir uns der Bedeutung von `gini`.\n",
    "\n",
    "\n",
    "## Was ist das Gini-Impurity-Kriterium?\n",
    "\n",
    "Das Gini-Impurity-Kriterium ist ein Maß für die Unreinheit eines Datensatzes.\n",
    "Beim Beispiel mit dem Autohaus sind im Wurzelknoten fünf Autos, die nicht\n",
    "verkauft wurden, und fünf verkaufte Autos. Bei zwei Klassen ist das die maximale\n",
    "Unreinheit, die auftreten kann. Der Anteil der verkauften Autos ist genau 50 %.\n",
    "Diesem prozentualen Anteil wird das Gini-Impurity-Kriterium von 0.5 zugeordnet.\n",
    "Es gibt zwei weitere Extremfälle. Entweder sind nur verkaufte Autos im Datensatz\n",
    "(100 % verkaufte Autos) oder gar keine verkaufte Autos (0 % verkaufte Autos). In\n",
    "beiden Fällen ist der Datensatz rein, das Gini-Impurity-Kriterium ist 0. In\n",
    "allen anderen Fällen liegt das Gini-Impurity-Kriterium zwischen 0 und 0.5. Die\n",
    "Formel zur Berechnung des genauen Wertes des Gini-Impurity-Kriteriums lautet\n",
    "\n",
    "$$\\text{GI} = 1 - p^2 - (1-p)^2,$$\n",
    "\n",
    "wenn $p$ der prozentuale Anteil der verkauften Autos ist (das gilt natürlich\n",
    "allgemein für binäre Klassifikationsaufgaben und nicht nur das\n",
    "Autohaus-Beispiel).\n",
    " \n",
    "Die folgende Abbildung zeigt die konkreten Werte des Gini-Impurity-Kriteriums\n",
    "für den prozentualen Anteil an verkauften Autos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52f769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linspace\n",
    "\n",
    "p = linspace(0,1)\n",
    "gini = 1 - p**2 - (1-p)**2\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(x = p, y = gini,\n",
    "        title='Gini-Impurity-Kriterium',\n",
    "        labels={'x': 'prozentualer Anteil', 'y': 'Wert des Gini-Impurity-Kriteriums'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e3ebe9",
   "metadata": {},
   "source": [
    "Im Diagramm können wir direkt ablesen, dass bei einem nicht verkauften Auto und\n",
    "fünf verkauften Autos ($p = 0.8\\bar{3}$) das Gini-Impurity-Kriterium den Wert\n",
    "$0.27\\bar{7} \\approx 0.278$ hat.\n",
    "\n",
    "Das Gini-Impurity-Kriterium ist sehr wichtig für das Training eines\n",
    "Entscheidungsbaumes. Der Algorithmus probiert im Hintergrund verschiedene\n",
    "Möglichkeiten durch, mit Hilfe der Entscheidungsfragen den Datensatz zu\n",
    "splitten. Zu jedem Split werden dann die zugehörigen Werte des\n",
    "Gini-Impurity-Kriteriums berechnet. Dann wählt der Algorithmus den Split aus,\n",
    "der die höchste Reinheit hat (also den niedrigsten Gini-Impurity-Wert). Gilt das\n",
    "für mehrere Splits, dann wird zufällig ein Split ausgewählt.  \n",
    "\n",
    "Neben dem Gini-Impurity-Kriterium gibt es noch weitere Bewertungsmaße, um einen\n",
    "Entscheidungsbaum zu trainieren. In Scikit-Learn sind die beiden Alternativen\n",
    "`log_less` und `entropy` für den **Shannonschen Informationsgewinn** verfügbar.\n",
    "Wir schauen uns im Folgenden an, wie diese ausgewählt werden können. Wer zuvor\n",
    "sich noch ein wenig mehr mit den Details von Entscheidungsbäumen beschäftigen\n",
    "möchte, kann sich die folgenden Videos ansehen.\n",
    "\n",
    "```{dropdown} Optionales Video \"Entscheidungsbäume #2 - Der ID3-Algorithmus\" von The Morpheus Tutorials\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/SYyyuHG9qBs?si=MgACjs1hSdFTPu5s\" \n",
    "title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; \n",
    "encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n",
    "```\n",
    "\n",
    "```{dropdown} Optionales Video \"Entscheidungsbäume #3 - Entropie und Informationsgewinn\" von The Morpheus Tutorials\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/lg1pb0YaAjI?si=K66tahVdLcI_sEex\" \n",
    "title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; \n",
    "encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n",
    "```\n",
    "\n",
    "```{dropdown} Optionales Video \"ID3 Entscheidungsbaum\" von 42 Entwickler\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/FAeVafU7qd8?si=JrDW6mu3v9SVOPAz\" \n",
    "title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; \n",
    "encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n",
    "```\n",
    "\n",
    "\n",
    "## Entscheidungsbäume trainieren\n",
    "\n",
    "Der Entscheidungsbaum-Klassifikationsalgorithmus von Scikit-Learn bietet noch\n",
    "weitere Optionen an, wie die Hilfe verrät\n",
    "\n",
    "```python\n",
    "help(DecisionTreeClassifier())\n",
    "```\n",
    "\n",
    "oder in der [Dokumentation Scikit-Learn → DecisionTreeClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) nachgelesen werden kann.\n",
    "\n",
    "Sowohl bei der Initalisierung des Entscheidungsbaumes können Parameter gesetzt\n",
    "werden, als auch beim Verwenden der verschiedenen Methoden. Tatsächlich haben\n",
    "wir bereits weiter oben aus didaktischen Gründen den Parameter `random_state=0`\n",
    "bei der Initialisierung gesetzt, damit immer der gleiche Entscheidungsbaum\n",
    "entsteht. In einem echten Projekt würde dieser Parameter nie verwendet werden.\n",
    "\n",
    "Probieren Sie andere Werte für den Start des Zufallszahlengenerators aus und\n",
    "testen Sie, was sich verändert, wenn Sie andere Kriterien für das Splitting\n",
    "verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea6bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell = DecisionTreeClassifier(criterion='entropy', random_state=3)\n",
    "modell.fit(X,y)\n",
    "\n",
    "plot_tree(modell, \n",
    "    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n",
    "    class_names=['nicht verkauft', 'verkauft']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fa6d5c",
   "metadata": {},
   "source": [
    "## Zusammenfassung und Ausblick\n",
    "\n",
    "In diesem Kapitel haben wir das Training von Entscheidungsbäumen mit Hilfe der\n",
    "Bibliothek Scikit-Learn vertieft. Im nächsten Kapitel widmen wir uns den Vor-,\n",
    "aber auch den Nachteilen von Entscheidungsbäumen."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.15.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "source_map": [
   13,
   48,
   70,
   75,
   79,
   91,
   95,
   133,
   145,
   206,
   213
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}