{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4879e8f9",
   "metadata": {},
   "source": [
    "# Polynomiale Regression oder multiple Regression???\n",
    "\n",
    "\n",
    "## Lernziele\n",
    "\n",
    "```{admonition} Lernziele\n",
    ":class: hint\n",
    "* TODO\n",
    "```\n",
    "\n",
    "## Polynomiale Regression\n",
    "\n",
    "Wenn wir uns das folgende Beispiel betrachten, werden wir feststellen, dass die\n",
    "lineare Regression die Messdaten nicht besonders gut annähert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a006edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "def erzeuge_kuenstliche_messdaten(koeffizienten, anzahl_daten=50):\n",
    "    zufallszahlen_generator = default_rng(seed=42)\n",
    "    xmin = - 5.0\n",
    "    xmax = + 5.0\n",
    "    x = zufallszahlen_generator.uniform(xmin, xmax, anzahl_daten)\n",
    "\n",
    "    error = 3.0 * zufallszahlen_generator.standard_normal(anzahl_daten)\n",
    "    y = error\n",
    "    \n",
    "    for i in range(len(koeffizienten)):\n",
    "        y += koeffizienten[i] * x**i\n",
    "    return x.reshape(-1,1), y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c23268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# erzeuge künstliche Daten\n",
    "X,y = erzeuge_kuenstliche_messdaten([-3, 7, 2, -2], 30)\n",
    "\n",
    "# Split in Trainings- und Testdaten\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "# Auswahl des Modells \n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "\n",
    "# Training \n",
    "model.fit(X_train, y_train)\n",
    "print('k0 = {}'.format(model.intercept_))\n",
    "print('k1 = {}'.format(model.coef_))\n",
    "\n",
    "# Validierung\n",
    "from sklearn.metrics import r2_score\n",
    "r2_train = r2_score(y_train, model.predict(X_train))\n",
    "r2_test  = r2_score(y_test,  model.predict(X_test))\n",
    "print('R2-Score der Trainingsdaten: {:.4}'.format(r2_train))\n",
    "print('R2-Score der Testdaten: {:.4}'.format(r2_test))\n",
    "\n",
    "# Visualisierung\n",
    "X_vis = np.linspace(-5,5,100).reshape(-1,1)\n",
    "y_vis = model.predict(X_vis)\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "fig, ax = plt.subplots(figsize=(12,9))\n",
    "ax.scatter(X_train, y_train, label='train')\n",
    "ax.scatter(X_test, y_test, label='test')\n",
    "ax.plot(X_vis, y_vis, '--k')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84525fa3",
   "metadata": {},
   "source": [
    "Der R2-Score ist sowohl bei den Trainingsdaten (0.7) als auch bei den Testdaten\n",
    "(0.6) nicht gut. Ein Regressionspolynom 2. oder 3. Grades könnte vielleicht\n",
    "besser passen. Wählen wir beispielsweise ein Polynom 3. Grades, so lautet das\n",
    "polynomiale Regressionsproblem wie folgt: Bestimme die Polynomkoeffizienten\n",
    "$k_0, k_1, k_2$ und $k_3$ so, dass $$y_i = k_0 + k_1\\cdot x_i + k_2\\cdot x_i^2 +\n",
    "k_3 \\cdot x_i^3 + \\varepsilon_i.$$ Wenn Sie in der Dokumentation von\n",
    "Scikit-Learn nun nach einer Funktion zur polynomialen Regression suchen, werden\n",
    "Sie nicht fündig werden. Tatsächlich brauchen wir auch keine eigenständige\n",
    "Funktion, sondern können uns mit einem Trick weiterhelfen. Wir erzeugen einfach\n",
    "eine zweite Spalte mit $x_i^2$ und eine dritte Spalte mit $x_i^3$ in den $N$\n",
    "Zeilen von $i=1, \\ldots, N$. \n",
    "\n",
    "Dieser Trick wird auch bei anderen ML-Verfahren angewandt. Aus einem Input, aus\n",
    "einer Eigenschaft werden jetzt drei neue Eigenschaften gemacht. Aus einem\n",
    "eindimensionalen Input wird ein dreidimensionaler Input. Mathematisch gesehen\n",
    "haben wir die Input-Daten in einen höherdimensionalen Bereich projiziert. Diese\n",
    "Methode nennt man **Kernel-Trick**. Es ist auch möglich, andere Funktionen zu\n",
    "benutzen, um die Daten in einen höherdimensionalen Raum zu projizieren, z.B.\n",
    "radiale Gaußsche Basisfunktionen. Das nennt man dann **Kernel-Methoden**.  \n",
    "\n",
    "In dieser Vorlesung bleiben wir aber bei den Polynomen als Basisfunktion.\n",
    "Scikit-Learn stellt auch hier passende Methoden bereit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4432c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# erzeuge eine Matrix mit den Zahlen 1 bis 10 in der 1. Spalte\n",
    "X = np.arange(1,11).reshape(-1,1)\n",
    "print('Original X:\\n', X)\n",
    "\n",
    "# lade die Polynom-Transformator \n",
    "polynom_transformator = PolynomialFeatures(degree = 3)\n",
    "\n",
    "# transformiere X\n",
    "X_transformiert =  polynom_transformator.fit_transform(X)\n",
    "print('transformiertes X:\\n', X_transformiert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba50a17",
   "metadata": {},
   "source": [
    "Damit können wir nun verschiedene Regressionspolynome ausprobieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab526a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# erzeuge künstliche Daten\n",
    "X,y = erzeuge_kuenstliche_messdaten([-3, 7, 2, -2], 30)\n",
    "\n",
    "# setze Polynomgrad\n",
    "grad = 3\n",
    "print('\\nGrad: {}'.format(grad))\n",
    "\n",
    "# Kernel-Trick, Split in Trainings- und Testdaten\n",
    "polynom_transformator = PolynomialFeatures(degree = grad)\n",
    "X = polynom_transformator.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "# Auswahl des Modells \n",
    "model = LinearRegression()\n",
    "\n",
    "# Training \n",
    "model.fit(X_train, y_train)\n",
    "#print('k0 = {}'.format(model.intercept_))\n",
    "#print('k1 = {}'.format(model.coef_))\n",
    "\n",
    "# Validierung\n",
    "r2_train = r2_score(y_train, model.predict(X_train))\n",
    "r2_test  = r2_score(y_test,  model.predict(X_test))\n",
    "print('R2-Score der Trainingsdaten: {:.4}'.format(r2_train))\n",
    "print('R2-Score der Testdaten: {:.4}'.format(r2_test))\n",
    "\n",
    "# Visualisierung\n",
    "X_vis = polynom_transformator.fit_transform( np.linspace(-5,5,100).reshape(-1,1) )\n",
    "y_vis = model.predict(X_vis)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,9))\n",
    "ax.scatter(X_train[:,1], y_train, label='train')\n",
    "ax.scatter(X_test[:,1],   y_test, label='test')\n",
    "ax.plot(X_vis[:,1], y_vis, '--k')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be411d7",
   "metadata": {},
   "source": [
    "Das Transformieren der Daten in eine höhere Dimension machen den Code schwerer\n",
    "lesbar. Deswegen definieren wir nun hiier eine Funktion, die erst transformiert\n",
    "und dann das lineare Regressionsmodell anwendet. Der Grad des Polynoms wird\n",
    "dabei als Argument übergeben. Damit diese Funktion Transformation und lineare\n",
    "Regression hintereinander automatisch ausführen kann, benötigen wir von\n",
    "Scikit-Learn die sogenannte Pipieline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaded5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02292388",
   "metadata": {},
   "source": [
    "Damit kann der obige Code etwas kürzer geschrieben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# erzeuge künstliche Daten\n",
    "X,y = erzeuge_kuenstliche_messdaten([-3, 7, 2, -2], 30)\n",
    "\n",
    "# setze Polynomgrad\n",
    "grad = 3\n",
    "print('\\nGrad: {}'.format(grad))\n",
    "\n",
    "# Split in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "# Auswahl des Modells \n",
    "model = PolynomialRegression(degree = 3)\n",
    "\n",
    "# Training \n",
    "model.fit(X_train, y_train)\n",
    "#print('k0 = {}'.format(model.intercept_))\n",
    "#print('k1 = {}'.format(model.coef_))\n",
    "\n",
    "# Validierung\n",
    "r2_train = r2_score(y_train, model.predict(X_train))\n",
    "r2_test  = r2_score(y_test,  model.predict(X_test))\n",
    "print('R2-Score der Trainingsdaten: {:.4}'.format(r2_train))\n",
    "print('R2-Score der Testdaten: {:.4}'.format(r2_test))\n",
    "\n",
    "# Visualisierung\n",
    "X_vis = np.linspace(-5,5,100).reshape(-1,1) \n",
    "y_vis = model.predict(X_vis)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,9))\n",
    "ax.scatter(X_train, y_train, label='train')\n",
    "ax.scatter(X_test, y_test, label='test')\n",
    "ax.plot(X_vis, y_vis, '--k')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b443e0",
   "metadata": {},
   "source": [
    "Als nächstes beschäftigen wir uns erneut mit der Frage, welches Modell am besten\n",
    "zu unseren Daten passt und ob Underfitting oder Overfitting vorliegt. Dazu\n",
    "kopieren wir den Code aus der obigen Code-Zelle und oacken ihn in eine\n",
    "for-Schleife:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30394ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# erzeuge künstliche Daten\n",
    "X,y = erzeuge_kuenstliche_messdaten([-3, 7, 2, -2], 30)\n",
    "\n",
    "# Split in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "# FOR-Schleife\n",
    "for grad in range(1,15):\n",
    "    print('\\nGrad: {}'.format(grad))\n",
    "\n",
    "\n",
    "    # Auswahl des Modells \n",
    "    model = PolynomialRegression(degree = grad)\n",
    "\n",
    "    # Training \n",
    "    model.fit(X_train, y_train)\n",
    "    #print('k0 = {}'.format(model.intercept_))\n",
    "    #print('k1 = {}'.format(model.coef_))\n",
    "\n",
    "    # Validierung\n",
    "    r2_train = r2_score(y_train, model.predict(X_train))\n",
    "    r2_test  = r2_score(y_test,  model.predict(X_test))\n",
    "    print('R2-Score der Trainingsdaten: {:.4}'.format(r2_train))\n",
    "    print('R2-Score der Testdaten: {:.4}'.format(r2_test))\n",
    "\n",
    "    # Visualisierung\n",
    "    X_vis = np.linspace(-5,5,100).reshape(-1,1) \n",
    "    y_vis = model.predict(X_vis)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    ax.scatter(X_train, y_train, label='train')\n",
    "    ax.scatter(X_test, y_test, label='test')\n",
    "    ax.plot(X_vis, y_vis, '--k')\n",
    "    ax.set_title('Grad: {}'.format(grad))\n",
    "    ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d3bb84",
   "metadata": {},
   "source": [
    "Am besten notieren wir die verschiedenen R2-Scores mit, um zu entscheiden, ob\n",
    "Underfitting oder Overfitting vorliegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a703c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# erzeuge künstliche Daten\n",
    "X,y = erzeuge_kuenstliche_messdaten([-3, 7, 2, -2], 30)\n",
    "\n",
    "# Split in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "r2_train_liste = []\n",
    "r2_test_liste = []\n",
    "\n",
    "# FOR-Schleife\n",
    "for grad in range(1,15):\n",
    "    # Auswahl des Modells \n",
    "    model = PolynomialRegression(degree = grad)\n",
    "\n",
    "    # Training \n",
    "    model.fit(X_train, y_train)\n",
    "   \n",
    "    # Validierung\n",
    "    r2_train = r2_score(y_train, model.predict(X_train))\n",
    "    r2_test  = r2_score(y_test,  model.predict(X_test))\n",
    "   \n",
    "    r2_train_liste.append(r2_train)\n",
    "    r2_test_liste.append(r2_test)\n",
    "\n",
    "print(r2_train_liste)\n",
    "print(r2_test_liste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e78a9e",
   "metadata": {},
   "source": [
    "Ein Plot der R2-Scores hilft bei der Einschätzung Over-/Underfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dfc75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(1,15), r2_train_liste, label='train')\n",
    "ax.plot(range(1,15), r2_test_liste, label='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da593aa7",
   "metadata": {},
   "source": [
    "Offensichtlich sind wir nach Grad 12 so schlecht, dass wir besser uns nur den\n",
    "Anfang angucken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1646f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(range(1,12), r2_train_liste[0:11], label='train')\n",
    "ax.plot(range(1,12), r2_test_liste[0:11], label='test')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be431f3b",
   "metadata": {},
   "source": [
    "Zwischen Grad 3 und Grad 8 ist der R2-Score für die Trainingsdaten praktisch\n",
    "gleich dem R2-Score der Testdaten. Diese Modelle können also gewählt werden. Für\n",
    "Grad 1 und 2 ist der R2-Score sowohl für Trainings- als auch Testdaten schlecht,\n",
    "es liegt Underfitting vor. Ab Grad 9 ist der R2-Score für die Trainingsdaten\n",
    "super, aber er fällt für die Testdaten ab. Wir sind im Bereich des Overfittings.\n",
    "\n",
    "Fazit: es kommen die polynomialen Regressionsmodelle für Grad 3 bis 8 infrage.\n",
    "Wenn man die Wahl hat, wählt man das einfachste Modell, also hier das mit Grad\n",
    "3."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('manim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "091ca22c969dd98be658b157336eff31c3bcbe46988231193628bcdd1fa42070"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
